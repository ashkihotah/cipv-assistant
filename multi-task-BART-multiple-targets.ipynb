{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe897e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ../input/requirements/requirements.txt\n",
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02653c6f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41f6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import (\n",
    "    BartTokenizer,\n",
    "    BartForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import Dataset\n",
    "import evaluate as ev\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import traceback\n",
    "import shutil\n",
    "import optuna\n",
    "import gc\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e01be26",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52948c60",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d5a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartWithRegressionCriterion(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom abstract loss function interface that combines the generation loss from a BART model\n",
    "    with a regression loss for polarity prediction.\n",
    "    \n",
    "    This class is designed to be used with a BART model that outputs both\n",
    "    generated text and regression predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, model_outputs, true_polarities):\n",
    "        \"\"\"\n",
    "        Computes the combined loss.\n",
    "\n",
    "        Args:\n",
    "            model_outputs (dict): Contains 'generation_loss' and 'polarities'.\n",
    "            true_polarities (torch.Tensor): Ground truth polarities.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The computed loss.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses should implement this method.\")\n",
    "\n",
    "class UncertaintyLoss(BartWithRegressionCriterion):\n",
    "    \"\"\"\n",
    "    Implements the Uncertainty-based loss function from Kendall et al. (2018)\n",
    "    for multi-task learning.\n",
    "\n",
    "    This loss function learns to balance multiple task losses by weighting them\n",
    "    based on their homoscedastic uncertainty. It does this by introducing\n",
    "    trainable parameters (log_vars) for each task.\n",
    "\n",
    "    The total loss is calculated as:\n",
    "    L_total = Σ [ (1 / (2*σ_i²)) * L_i + log(σ_i) ]\n",
    "    where σ_i is the uncertainty for task i, and L_i is its raw loss.\n",
    "\n",
    "    To maintain numerical stability, we work with log(σ_i²) instead of σ_i.\n",
    "    Let s_i = log(σ_i²), then σ_i² = exp(s_i). The formula becomes:\n",
    "    L_total = Σ [ exp(-s_i) * L_i + 0.5 * s_i ]\n",
    "    \n",
    "    We have two tasks: \n",
    "    - Task 1: Polarity prediction (Regression)\n",
    "    - Task 2: Explanation generation (Classification-like via Cross-Entropy)\n",
    "    \"\"\"\n",
    "    def __init__(self, regression_loss_fn=nn.SmoothL1Loss()):\n",
    "        super(UncertaintyLoss, self).__init__()\n",
    "        # Initialize two trainable log-variance parameters, one for each task.\n",
    "        # We initialize them to 0.0, which means the initial variance σ² is exp(0) = 1.\n",
    "        # This gives both tasks equal initial weighting.\n",
    "        # We name them to make their purpose clear.\n",
    "        self.log_var_polarity = nn.Parameter(torch.zeros(1))\n",
    "        self.log_var_explanation = nn.Parameter(torch.zeros(1))\n",
    "        self.regression_loss_fn = regression_loss_fn  # Loss function for regression task\n",
    "\n",
    "    def forward(self, model_outputs, true_polarities):\n",
    "        \"\"\"\n",
    "        Calculates the combined, uncertainty-weighted loss.\n",
    "\n",
    "        Args:\n",
    "            model_outputs (dict): Contains the model's outputs:\n",
    "                - 'polarities': Predicted polarities (regression output).\n",
    "                - 'generation_loss': Pre-computed generation loss (e.g., from BART).\n",
    "            true_polarities (torch.Tensor): Ground truth polarities for regression.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The final, combined loss to be backpropagated.\n",
    "        \"\"\"\n",
    "        predicted_polarities = model_outputs['polarities']\n",
    "        loss_explanation = model_outputs['generation_loss']  # BART's pre-computed generation loss\n",
    "\n",
    "        # Compute the Smooth L1 Loss\n",
    "        loss_polarities = self.regression_loss_fn(predicted_polarities, true_polarities)\n",
    "\n",
    "        # Ensure the parameters are on the same device as the input losses\n",
    "        self.log_var_polarity.data = self.log_var_polarity.data.to(loss_polarities.device)\n",
    "        self.log_var_explanation.data = self.log_var_explanation.data.to(loss_explanation.device)\n",
    "\n",
    "        # Calculate the precision terms (1 / (2*σ²))\n",
    "        # The factor of 0.5 for the regression precision term comes from the\n",
    "        # Gaussian likelihood derivation in the paper.\n",
    "        precision_polarity = torch.exp(-self.log_var_polarity)\n",
    "        \n",
    "        # The precision term for classification-like losses is (1 / σ²)\n",
    "        precision_explanation = torch.exp(-self.log_var_explanation)\n",
    "\n",
    "        # The final loss for each task component\n",
    "        # Regression term: (1/(2σ²))*L_reg + log(σ) = (1/(2σ²))*L_reg + 0.5*log(σ²)\n",
    "        term_polarity = 0.5 * precision_polarity * loss_polarities + 0.5 * self.log_var_polarity\n",
    "        \n",
    "        # Explanation term: For simplicity and stability, many implementations use the same\n",
    "        # form for all tasks. This is a robust choice.\n",
    "        # Here we use the general form: (1/σ²)*L_gen + log(σ) = (1/σ²)*L_gen + 0.5*log(σ²)\n",
    "        # term_explanation = precision_explanation * loss_explanation + 0.5 * self.log_var_explanation\n",
    "        \n",
    "        # A more common and stable reparameterization for all tasks is:\n",
    "        # L_i_final = exp(-s_i) * L_i + s_i\n",
    "        # Let's use this for the explanation part for robustness.\n",
    "        term_explanation = precision_explanation * loss_explanation + self.log_var_explanation\n",
    "\n",
    "        # The total loss is the sum of the individual task losses\n",
    "        total_loss = term_polarity + term_explanation\n",
    "\n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'reg_loss': loss_polarities,\n",
    "            'gen_loss': loss_explanation\n",
    "        }\n",
    "\n",
    "class StaticWeightedLoss(BartWithRegressionCriterion):\n",
    "    \"\"\"\n",
    "    A dedicated class to compute the combined loss for the hybrid model.\n",
    "    This separates the loss logic from the model's forward pass.\n",
    "    \"\"\"\n",
    "    def __init__(self, regression_loss_fn=nn.MSELoss(), alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Weight for regression loss\n",
    "        self.regression_loss_fn = regression_loss_fn\n",
    "\n",
    "    def forward(self, model_outputs, true_polarities):\n",
    "        # Unpack model outputs\n",
    "        predicted_polarities = model_outputs['polarities']\n",
    "        gen_loss = model_outputs['generation_loss']  # BART's pre-computed generation loss\n",
    "\n",
    "        # Calculate Regression Loss (MSE) for each sample in the batch\n",
    "        reg_loss = self.regression_loss_fn(predicted_polarities, true_polarities)\n",
    "\n",
    "        total_loss = (self.alpha * reg_loss) + ((1 - self.alpha) * gen_loss)\n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'reg_loss': reg_loss,\n",
    "            'gen_loss': gen_loss\n",
    "        }\n",
    "\n",
    "class EuclideanLoss(BartWithRegressionCriterion):\n",
    "    \"\"\"\n",
    "    A dedicated class to compute the combined loss for the hybrid model.\n",
    "    This separates the loss logic from the model's forward pass.\n",
    "    \"\"\"\n",
    "    def __init__(self, regression_loss_fn=nn.MSELoss()):\n",
    "        super().__init__()\n",
    "        self.regression_loss_fn = regression_loss_fn\n",
    "\n",
    "    def forward(self, model_outputs, true_polarities):\n",
    "        # Unpack model outputs\n",
    "        predicted_polarities = model_outputs['polarities']\n",
    "        gen_loss = model_outputs['generation_loss']  # BART's pre-computed generation loss\n",
    "\n",
    "        # Calculate Regression Loss (MSE) for each sample in the batch\n",
    "        reg_loss = self.regression_loss_fn(predicted_polarities, true_polarities)\n",
    "\n",
    "        # Compute the Euclidean norm of the vector [reg_loss, gen_loss] for each sample\n",
    "        total_loss = torch.sqrt(reg_loss * reg_loss + gen_loss * gen_loss)\n",
    "        return {\n",
    "            'total_loss': total_loss,  # Return the mean loss across the batch\n",
    "            'reg_loss': reg_loss,\n",
    "            'gen_loss': gen_loss\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b99aa32",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9547d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartWithRegression(nn.Module):\n",
    "    \"\"\"\n",
    "    A hybrid model combining BART for explanation generation and a custom\n",
    "    regression head for polarity prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_CHECKPOINT = \"morenolq/bart-it\"\n",
    "    SEP_TOKEN = \"[SEP]\"\n",
    "    USR0_TOKEN = \"[USR0]\"\n",
    "    USR1_TOKEN = \"[USR1]\"\n",
    "\n",
    "    def __initialize_architecture(self, regression_dropout: float):\n",
    "        \"\"\"\n",
    "        Initializes the architecture of the model, including the BART model\n",
    "        and the regression head.\n",
    "        \"\"\"\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Dropout(regression_dropout),\n",
    "            nn.Linear(self.bart.config.hidden_size, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            load_path: str=None,\n",
    "            single_sep_token: bool=False,\n",
    "            init_sep_tokens: str = None,\n",
    "            regression_dropout: float=0.1,\n",
    "            verbose: bool=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.single_sep_token = single_sep_token\n",
    "        self.tokenizer = BartWithRegression.get_tokenizer(single_sep_token)\n",
    "        if load_path != None and os.path.exists(load_path):\n",
    "            self.bart = BartForConditionalGeneration.from_pretrained(load_path)\n",
    "            self.__initialize_architecture(regression_dropout)\n",
    "            if \"regression_head.pt\" in os.listdir(load_path):\n",
    "                self.regression_head.load_state_dict(\n",
    "                    torch.load(\n",
    "                        os.path.join(load_path, \"regression_head.pt\"),\n",
    "                        map_location=torch.device('cpu')\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                raise FileNotFoundError(\n",
    "                    f\"'regression_head.pt' not found in {load_path}.\"\n",
    "                )\n",
    "        else:\n",
    "            self.bart = BartForConditionalGeneration.from_pretrained(self.DEFAULT_CHECKPOINT)\n",
    "            \n",
    "            self.bart.resize_token_embeddings(len(self.tokenizer))\n",
    "            if self.single_sep_token:\n",
    "                self.bart.config.sep_token_id = self.tokenizer.convert_tokens_to_ids(self.SEP_TOKEN)\n",
    "\n",
    "                if init_sep_tokens == \"mean\":\n",
    "                    sep_id = self.bart.config.sep_token_id\n",
    "                    sep_open_id = self.tokenizer.convert_tokens_to_ids(\"<s>\")\n",
    "                    sep_close_id = self.tokenizer.convert_tokens_to_ids(\"</s>\")\n",
    "                    with torch.no_grad():\n",
    "                        emb = self.bart.model.shared\n",
    "                        mean_emb = (emb.weight[sep_open_id] + emb.weight[sep_close_id]) / 2\n",
    "                        emb.weight[sep_id] = mean_emb\n",
    "                elif init_sep_tokens == \"</s>\":\n",
    "                    sep_id = self.bart.config.sep_token_id\n",
    "                    sep_open_id = self.tokenizer.convert_tokens_to_ids(\"</s>\")\n",
    "                    with torch.no_grad():\n",
    "                        emb = self.bart.model.shared\n",
    "                        mean_emb = (emb.weight[sep_open_id] + emb.weight[sep_close_id]) / 2\n",
    "                        emb.weight[sep_id] = mean_emb\n",
    "            else:\n",
    "                self.bart.config.sep_token_id = self.tokenizer.convert_tokens_to_ids(self.USR0_TOKEN)\n",
    "                self.bart.config.sep_token_id2 = self.tokenizer.convert_tokens_to_ids(self.USR1_TOKEN)\n",
    "\n",
    "            self.__initialize_architecture(regression_dropout)\n",
    "            if verbose:\n",
    "                print(\"[WARNING]: Local model checkpoint not found.\")\n",
    "                print(\"It will be initialized with default configurations!\")\n",
    "\n",
    "    def __predict_polarities(\n",
    "            self,\n",
    "            encoder_last_hidden_state: torch.Tensor,\n",
    "            sep_token_matrix_mask: torch.Tensor\n",
    "        ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predicts the polarities for specific token indices in the encoder's\n",
    "        last hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = encoder_last_hidden_state.size(0)\n",
    "        # print(f\"encoder_last_hidden_state.shape: {encoder_last_hidden_state.shape}\")\n",
    "        # print(f\"sep_token_matrix_mask.shape: {sep_token_matrix_mask.shape}\")\n",
    "        flattened_hidden_states = encoder_last_hidden_state.view(-1, encoder_last_hidden_state.size(-1))\n",
    "        flattened_sep_mask = sep_token_matrix_mask.view(-1)\n",
    "\n",
    "        # print(f\"flattened_hidden_states.shape: {flattened_hidden_states.shape}\")\n",
    "        # print(f\"flattened_sep_mask.shape: {flattened_sep_mask.shape}\")\n",
    "\n",
    "        predicted_polarities = self.regression_head(flattened_hidden_states[flattened_sep_mask]).squeeze(-1)\n",
    "\n",
    "        return predicted_polarities\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        labels\n",
    "    ):\n",
    "        outputs = self.bart(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            output_hidden_states=True,\n",
    "            # output_attentions=False,\n",
    "        )\n",
    "\n",
    "        encoder_last_hidden_state = outputs.encoder_last_hidden_state\n",
    "        sep_token_matrix_mask = (\n",
    "            input_ids == self.bart.config.sep_token_id\n",
    "        )\n",
    "        if not self.single_sep_token:\n",
    "            sep_token_matrix_mask = sep_token_matrix_mask | (\n",
    "                (input_ids == self.bart.config.sep_token_id2)\n",
    "            )\n",
    "        predicted_polarities = self.__predict_polarities(encoder_last_hidden_state, sep_token_matrix_mask)\n",
    "\n",
    "        return {\n",
    "            'polarities': predicted_polarities,\n",
    "            'explanation_logits': outputs.logits,\n",
    "            'generation_loss': outputs.loss  # BART's pre-computed generation loss\n",
    "        }\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        **gen_kwargs\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Generates both explanations and polarities from the input data.\n",
    "        The explanations are generated using the BART model, and the polarities\n",
    "        are predicted using the regression head.\n",
    "        \"\"\"\n",
    "        # Get encoder outputs\n",
    "        encoder_outputs = self.bart.model.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        encoder_last_hidden_state = encoder_outputs.last_hidden_state\n",
    "\n",
    "        # Generate explanations using precomputed encoder outputs\n",
    "        generated_sequences = self.bart.generate(\n",
    "            input_ids=input_ids, attention_mask=attention_mask,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            **gen_kwargs\n",
    "        )\n",
    "\n",
    "        # Find target token index for polarity prediction\n",
    "        sep_token_matrix_mask = (\n",
    "            input_ids == self.bart.config.sep_token_id\n",
    "        )\n",
    "        if not self.single_sep_token:\n",
    "            sep_token_matrix_mask = sep_token_matrix_mask | (\n",
    "                (input_ids == self.bart.config.sep_token_id2)\n",
    "            )\n",
    "        predicted_polarities = self.__predict_polarities(encoder_last_hidden_state, sep_token_matrix_mask)\n",
    "\n",
    "        return {\n",
    "            'explanations': generated_sequences,\n",
    "            'polarities': predicted_polarities\n",
    "        }\n",
    "\n",
    "    def save_model(self, save_directory: str):\n",
    "        \"\"\"\n",
    "        Saves the fine-tuned BART model, the regression head, and the tokenizer\n",
    "        to a specified directory.\n",
    "        \"\"\"\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        self.bart.save_pretrained(save_directory)\n",
    "        torch.save(self.regression_head.state_dict(), os.path.join(save_directory, \"regression_head.pt\"))\n",
    "\n",
    "    @classmethod\n",
    "    def get_tokenizer(cls, single_sep_token) -> BartTokenizer:\n",
    "        tokenizer = BartTokenizer.from_pretrained(cls.DEFAULT_CHECKPOINT)\n",
    "        if single_sep_token:\n",
    "            tokenizer.add_special_tokens({\n",
    "                \"additional_special_tokens\": [cls.SEP_TOKEN]\n",
    "            })\n",
    "        else:\n",
    "            tokenizer.add_special_tokens({\n",
    "                \"additional_special_tokens\": [cls.USR0_TOKEN, cls.USR1_TOKEN]\n",
    "            })\n",
    "        return tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6324541",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9dfc029",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MultiTaskBartDataCollator(DataCollatorForSeq2Seq):\n",
    "    \"\"\"\n",
    "    Extends DataCollatorForSeq2Seq to also handle custom 'regression_labels'.\n",
    "    This class pads the input_ids, attention_mask, and labels as usual,\n",
    "    but also pads the 'regression_labels' to the maximum length in the batch.\n",
    "    \"\"\"\n",
    "    def __call__(\n",
    "            self,\n",
    "            features: List[Dict[str, Any]],\n",
    "            return_tensors: Optional[str]=None\n",
    "        ) -> Dict[str, torch.Tensor]:\n",
    "        # Extract polarities from features and create a 1-D tensor\n",
    "        # containing all polarities concatenated\n",
    "        polarities = [polarity for feature in features for polarity in feature.pop('polarities')]\n",
    "\n",
    "        # Let the parent class handle the standard seq2seq padding and\n",
    "        # creation of decoder_input_ids.\n",
    "        # This will correctly pad input_ids, attention_mask, and labels.\n",
    "        batch = super().__call__(features, return_tensors)\n",
    "        \n",
    "        # Convert polarities to a tensor\n",
    "        batch['polarities'] = torch.tensor(polarities, dtype=torch.float32)\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6617e6d",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17154396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingArguments:\n",
    "    \"\"\"\n",
    "    A simple class to hold training arguments for the hybrid model.\n",
    "    This is useful for passing around training configurations without\n",
    "    needing to use a dictionary.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, criterion: BartWithRegressionCriterion,\n",
    "            num_epochs: int=1, gradient_accumulation_steps: int=1,\n",
    "            get_scheduler_fn=get_linear_schedule_with_warmup,\n",
    "            warmup_percentage: float=0.1,\n",
    "            body_lr: float=3e-5, head_lr: float=1.5e-4,\n",
    "            weight_decay: float=0.01,\n",
    "            early_stopping_patience: int=None,\n",
    "            logging: bool=True, save_path: str=None,\n",
    "            load_best_model_at_end: bool=False\n",
    "        ):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.logging = logging\n",
    "        self.save_path = save_path\n",
    "        self.body_lr = body_lr\n",
    "        self.head_lr = head_lr\n",
    "        self.warmup_percentage = warmup_percentage\n",
    "        self.weight_decay = weight_decay\n",
    "        self.get_scheduler_fn = get_scheduler_fn\n",
    "        self.criterion = criterion\n",
    "        self.load_best_model_at_end = load_best_model_at_end\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    A simple trainer class to handle the training loop for the hybrid model.\n",
    "    It abstracts away the training logic, allowing for easy integration with\n",
    "    different datasets and models.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, model: BartWithRegression, device,\n",
    "            args: TrainingArguments=None,\n",
    "            train_dataloader=None, eval_dataloader=None, test_dataloader=None,\n",
    "            eval_sts_model='sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "        ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.eval_dataloader = eval_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.args = args\n",
    "\n",
    "        if self.args is not None:\n",
    "            # Define parameter groups for the optimizer\n",
    "            # A common starting point is a 5x to 10x difference in LR\n",
    "            bart_params = self.model.bart.parameters()\n",
    "            regression_head_params = self.model.regression_head.parameters()\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\"params\": bart_params, \"lr\": self.args.body_lr},\n",
    "                {\"params\": regression_head_params, \"lr\": self.args.head_lr}\n",
    "            ]\n",
    "\n",
    "            # Add UncertaintyLoss parameters if applicable\n",
    "            if isinstance(self.args.criterion, UncertaintyLoss):\n",
    "                uncertainty_loss_params = self.args.criterion.parameters()\n",
    "                optimizer_grouped_parameters.append(\n",
    "                    {\"params\": uncertainty_loss_params, \"lr\": self.args.head_lr}  # Use head_lr or a custom LR\n",
    "                )\n",
    "\n",
    "            # The AdamW optimizer handles the groups seamlessly\n",
    "            self.optimizer = AdamW(\n",
    "                optimizer_grouped_parameters,\n",
    "                # It discourages the model from developing very large weights\n",
    "                # forcing it to use all of its weights to a small extent\n",
    "                # rather than relying heavily on a few\n",
    "                weight_decay=self.args.weight_decay\n",
    "            )\n",
    "\n",
    "            num_training_steps = len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_epochs\n",
    "            # 10 : 100 = num_warmup_steps : num_training_steps\n",
    "            num_warmup_steps = int(num_training_steps * self.args.warmup_percentage)\n",
    "            # The learning rate scheduler will also correctly apply its schedule to each group's base LR\n",
    "            self.lr_scheduler = self.args.get_scheduler_fn(\n",
    "                self.optimizer, num_warmup_steps=num_warmup_steps,\n",
    "                num_training_steps=num_training_steps\n",
    "            )\n",
    "\n",
    "            # print(f\"Number of training steps: {num_training_steps}\")\n",
    "            # print(f\"Number of warmup steps: {num_warmup_steps}\")\n",
    "\n",
    "            self.log_history = {\n",
    "                'epochs': [],\n",
    "                'total_train_losses': [],\n",
    "                'reg_train_losses': [],\n",
    "                'gen_train_losses': [],\n",
    "                'total_eval_losses': [],\n",
    "                'reg_eval_losses': [],\n",
    "                'gen_eval_losses': [],\n",
    "                'reg_loss_weight': [],\n",
    "                'gen_loss_weight': []\n",
    "            }\n",
    "\n",
    "        # print(f\"Loading Evaluation STS model: {eval_sts_model}\")\n",
    "        self.sts_model = SentenceTransformer(eval_sts_model, device=self.device)\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        Validates the model on the provided dataloader using the specified\n",
    "        criterion. Returns the average loss over the validation set.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        reg_loss = 0.0\n",
    "        gen_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            eval_progress_bar = tqdm(self.eval_dataloader, desc=\"Validation\")\n",
    "            for batch in eval_progress_bar:\n",
    "                true_polarities = batch.pop('polarities').detach().clone().to(self.device)\n",
    "                for k in batch: batch[k] = batch[k].to(self.device)\n",
    "\n",
    "                with autocast(device_type=self.device.type):\n",
    "                    model_outputs = self.model(**batch)\n",
    "                    loss = self.args.criterion(model_outputs, true_polarities)\n",
    "\n",
    "                total_loss += loss['total_loss']\n",
    "                reg_loss += loss['reg_loss']\n",
    "                gen_loss += loss['gen_loss']\n",
    "\n",
    "                avg_total_loss = total_loss / len(self.eval_dataloader)\n",
    "                avg_reg_loss = reg_loss / len(self.eval_dataloader)\n",
    "                avg_gen_loss = gen_loss / len(self.eval_dataloader)\n",
    "\n",
    "                eval_progress_bar.set_postfix({\n",
    "                    'avg_total_loss': avg_total_loss.item(),\n",
    "                    'avg_reg_loss': avg_reg_loss.item(),\n",
    "                    'avg_gen_loss': avg_gen_loss.item()\n",
    "                })\n",
    "        return {\n",
    "            'total_loss': avg_total_loss,\n",
    "            'reg_loss': avg_reg_loss,\n",
    "            'gen_loss': avg_gen_loss\n",
    "        }\n",
    "\n",
    "    def train(\n",
    "            self,\n",
    "            trial: optuna.Trial = None\n",
    "        ):\n",
    "\n",
    "        if self.args is None:\n",
    "            raise ValueError(\"Training arguments must be provided!\")\n",
    "    \n",
    "        if self.train_dataloader is None:\n",
    "            raise ValueError(\"Training dataloader must be provided!\")\n",
    "\n",
    "        if not isinstance(self.args.criterion, BartWithRegressionCriterion):\n",
    "            raise TypeError(\"Criterion must be an instance of BartWithRegressionCriterion!\")\n",
    "\n",
    "        # Fundamental if using mixed precision training\n",
    "        # See pytorch docs: https://docs.pytorch.org/docs/stable/amp.html#gradient-scaling\n",
    "        scaler = GradScaler()\n",
    "        best_eval_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(self.args.num_epochs):\n",
    "            self.model.train()\n",
    "            total_train_loss = 0.0\n",
    "            train_progress_bar = tqdm(self.train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "            for batch_idx, batch in enumerate(train_progress_bar):\n",
    "\n",
    "                # Pop polarity and explanation labels from batch\n",
    "                true_polarities = batch.pop('polarities').detach().clone().to(self.device)\n",
    "                # true_explanation_ids = torch.tensor(batch.pop('labels')).to(device)\n",
    "\n",
    "                # Move all batch tensors to device\n",
    "                for k in batch: batch[k] = batch[k].to(self.device)\n",
    "\n",
    "                # Use autocast for mixed precision training\n",
    "                with autocast(device_type=self.device.type):\n",
    "                    # The model forward can now accept the whole batch, as the DataCollator has prepared it perfectly\n",
    "                    model_outputs = self.model(**batch)\n",
    "                    \n",
    "                    # The criterion now only needs the true polarity, as gen_loss is in the model_outputs\n",
    "                    losses_dict = self.args.criterion(\n",
    "                        model_outputs=model_outputs,\n",
    "                        true_polarities=true_polarities\n",
    "                    )\n",
    "                    if not self.model.freeze_bart:\n",
    "                        loss = losses_dict['total_loss']\n",
    "                    else:\n",
    "                        loss = losses_dict['reg_loss']\n",
    "\n",
    "                # Reduce loss to a scalar\n",
    "                total_train_loss += loss.item()\n",
    "\n",
    "                # Scale the loss and call backward\n",
    "                normalized_loss = loss / self.args.gradient_accumulation_steps # Normalize loss\n",
    "                scaler.scale(normalized_loss).backward()\n",
    "\n",
    "                if (batch_idx + 1) % self.args.gradient_accumulation_steps == 0:\n",
    "                    # Unscale gradients and call optimizer.step()\n",
    "                    scaler.step(self.optimizer)\n",
    "                    # Update the scale for next iteration\n",
    "                    scaler.update()\n",
    "                    if self.lr_scheduler is not None:\n",
    "                        self.lr_scheduler.step() # Update learning rate\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "                # Update progress bar\n",
    "                train_progress_bar.set_postfix({\n",
    "                    'avg loss': total_train_loss / (batch_idx + 1),\n",
    "                    'reg_loss': losses_dict['reg_loss'].item(),\n",
    "                    'gen_loss': losses_dict['gen_loss'].item(),\n",
    "                    'lr_body': self.optimizer.param_groups[0]['lr'],\n",
    "                    'lr_head': self.optimizer.param_groups[1]['lr'],\n",
    "                    # 'reg_weight': self.args.criterion.log_var_polarity.item(),\n",
    "                    # 'gen_weight': self.args.criterion.log_var_explanation.item()\n",
    "                })\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(self.train_dataloader)\n",
    "\n",
    "            if self.eval_dataloader is not None:\n",
    "                eval_losses_dict = self.validate()\n",
    "                eval_loss = eval_losses_dict['total_loss'].item()\n",
    "                if eval_loss < best_eval_loss:\n",
    "                    best_eval_loss = eval_loss\n",
    "                    patience_counter = 0\n",
    "                    if self.args.save_path is not None:\n",
    "                        self.model.save_model(os.path.join(self.args.save_path))\n",
    "                else:\n",
    "                    if self.args.early_stopping_patience is not None:\n",
    "                        patience_counter += 1\n",
    "                        if patience_counter >= self.args.early_stopping_patience:\n",
    "                            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                            return\n",
    "\n",
    "            if self.args.logging:\n",
    "                self.log_history['epochs'].append(epoch + 1)\n",
    "                self.log_history['total_train_losses'].append(avg_train_loss)\n",
    "                self.log_history['reg_train_losses'].append(losses_dict['reg_loss'].item())\n",
    "                self.log_history['gen_train_losses'].append(losses_dict['gen_loss'].item())\n",
    "                if isinstance(self.args.criterion, UncertaintyLoss):\n",
    "                    self.log_history['reg_loss_weight'].append(self.args.criterion.log_var_polarity.item())\n",
    "                    self.log_history['gen_loss_weight'].append(self.args.criterion.log_var_explanation.item())\n",
    "                if self.eval_dataloader is not None:\n",
    "                    self.log_history['total_eval_losses'].append(eval_losses_dict['total_loss'].item())\n",
    "                    self.log_history['reg_eval_losses'].append(eval_losses_dict['reg_loss'].item())\n",
    "                    self.log_history['gen_eval_losses'].append(eval_losses_dict['gen_loss'].item())\n",
    "            \n",
    "            # --- NEW: Optuna Pruning Integration ---\n",
    "            if trial is not None:\n",
    "                # Report the intermediate evaluation loss to Optuna\n",
    "                trial.report(eval_loss, epoch)\n",
    "                # Check if the trial should be pruned\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "            # --- END NEW ---\n",
    "\n",
    "        if self.args.load_best_model_at_end and self.args.save_path is not None:\n",
    "            del self.model\n",
    "            gc.collect()\n",
    "\n",
    "            self.model = BartWithRegression.load_model(\n",
    "                self.args.save_path,\n",
    "                single_sep_token=self.model.single_sep_token,\n",
    "                regression_dropout=self.model.regression_head[0].p\n",
    "            )\n",
    "\n",
    "    def evaluate(self, leave=True, **gen_kwargs):\n",
    "        self.model.eval()\n",
    "\n",
    "        rouge_metric = ev.load('rouge')\n",
    "        bleu_metric = ev.load('bleu')\n",
    "        bertscore_metric = ev.load('bertscore')\n",
    "        all_polarities_pred = []\n",
    "        all_polarities_true = []\n",
    "        all_true_explanations = []\n",
    "        all_predicted_explanations = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.test_dataloader, desc=\"Evaluating\", leave=leave):\n",
    "                true_polarities = batch.pop('polarities').detach().clone().to(self.device)\n",
    "                true_explanation_ids = batch.pop('labels').detach().clone().to(self.device)\n",
    "                for k in batch: batch[k] = batch[k].to(self.device)\n",
    "\n",
    "                model_outputs = self.model.generate(**batch, **gen_kwargs)\n",
    "\n",
    "                all_polarities_pred.extend(model_outputs['polarities'].tolist())\n",
    "                all_polarities_true.extend(true_polarities)\n",
    "\n",
    "                predicted_explanations = self.model.tokenizer.batch_decode(\n",
    "                    model_outputs['explanations'],\n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                true_explanation_ids = torch.where(\n",
    "                    true_explanation_ids == -100,\n",
    "                    torch.tensor(self.model.tokenizer.pad_token_id)\n",
    "                        .to(true_explanation_ids.device),\n",
    "                    true_explanation_ids\n",
    "                )\n",
    "                true_explanations = self.model.tokenizer.batch_decode(\n",
    "                    true_explanation_ids,\n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                \n",
    "                all_true_explanations.extend(true_explanations)\n",
    "                all_predicted_explanations.extend(predicted_explanations)\n",
    "\n",
    "                rouge_metric.add_batch(\n",
    "                    predictions=predicted_explanations,\n",
    "                    references=true_explanations\n",
    "                )\n",
    "                bleu_metric.add_batch(\n",
    "                    predictions=predicted_explanations,\n",
    "                    references=[[ref] for ref in true_explanations]\n",
    "                )\n",
    "\n",
    "        reference_embeddings = self.sts_model.encode(all_true_explanations, convert_to_tensor=True)\n",
    "        generated_embeddings = self.sts_model.encode(all_predicted_explanations, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(generated_embeddings, reference_embeddings)\n",
    "        sbert_similarity = torch.diag(cosine_scores).mean().item()\n",
    "\n",
    "        bertscore_results = bertscore_metric.compute(\n",
    "            predictions=all_predicted_explanations, \n",
    "            references=all_true_explanations, \n",
    "            lang=\"it\",\n",
    "            device=self.device\n",
    "        )\n",
    "        bertscore_f1 = sum(bertscore_results['f1']) / len(bertscore_results['f1'])\n",
    "\n",
    "        rouge_score = rouge_metric.compute()\n",
    "        bleu_score = bleu_metric.compute()\n",
    "        mse = F.mse_loss(\n",
    "            torch.tensor(all_polarities_pred),\n",
    "            torch.tensor(all_polarities_true)\n",
    "        ).item()\n",
    "\n",
    "        return {\n",
    "            'rouge1': rouge_score['rouge1'],\n",
    "            'rouge2': rouge_score['rouge2'],\n",
    "            'rougeL': rouge_score['rougeL'],\n",
    "            'bleu': bleu_score['bleu'],\n",
    "            'mse': mse,\n",
    "            'sbert_similarity': sbert_similarity,\n",
    "            'bertscore_f1': bertscore_f1\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4792a16",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78e0c2b",
   "metadata": {},
   "source": [
    "## Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60769c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"Using device: {torch.cuda.get_device_name(0)}\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "SINGLE_SEP_TOKEN = False  # Set to True if using a single separator token, False for two separate tokens\n",
    "\n",
    "losses = {\n",
    "        \"euclidean-mse\": EuclideanLoss(\n",
    "            regression_loss_fn=torch.nn.MSELoss(),\n",
    "        ).to(DEVICE),\n",
    "        \"euclidean-smoothl1\": EuclideanLoss(\n",
    "            regression_loss_fn=torch.nn.SmoothL1Loss(),\n",
    "        ).to(DEVICE),\n",
    "        \"uncertainty-mse\": UncertaintyLoss(\n",
    "            regression_loss_fn=torch.nn.MSELoss(),\n",
    "        ).to(DEVICE),\n",
    "        \"uncertainty-smoothl1\": UncertaintyLoss(\n",
    "            regression_loss_fn=torch.nn.SmoothL1Loss(),\n",
    "        ).to(DEVICE),\n",
    "    }\n",
    "\n",
    "criterion = losses[\"euclidean-mse\"]\n",
    "\n",
    "# criterion = StaticWeightedLoss(\n",
    "#     # regression_loss_fn=torch.nn.MSELoss(),\n",
    "#     regression_loss_fn=torch.nn.SmoothL1Loss(),\n",
    "#     alpha=0.5\n",
    "# ).to(DEVICE)\n",
    "\n",
    "TRAIN_MODE = \"train_no_optuna\"\n",
    "# TRAIN_MODE = \"train_with_optuna\"\n",
    "TEST_MODE = \"test_no_optuna\"\n",
    "# TEST_MODE = \"test_with_optuna\"\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "WARMUP_PERCENTAGE = 0.1\n",
    "WEIGHT_DECAY = 0.01 # int the 0 to 0.1 range\n",
    "BODY_LR = 3e-5\n",
    "HEAD_LR = 1.5e-4\n",
    "\n",
    "OPTUNA_TRAIN_TRIALS=2\n",
    "OPTUNA_TEST_TRIALS=2\n",
    "PRUNER_WARMUP_STEPS=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e01c06",
   "metadata": {},
   "source": [
    "## Paths Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a97a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# ==== LOCAL SETTINGS ====\n",
    "PATH = os.path.join(\".\", \"out\", \"datasets\", \"cipv-chats-sentiment\") # , \"gen2\", \"chats\"\n",
    "OUT_DIR = os.path.join(\".\", \"out\", \"models\", \"Multi-Task BART\", timestamp)\n",
    "\n",
    "# ==== KAGGLE SETTINGS ====\n",
    "# PATH = os.path.join(os.sep, \"kaggle\", \"input\", \"cipv-chats-sentiment\")\n",
    "# OUT_DIR = os.path.join(os.sep, \"kaggle\", \"working\", \"out\")\n",
    "\n",
    "\n",
    "if TRAIN_MODE == \"train_with_optuna\":\n",
    "    temp_save_path = OUT_DIR + \"-temp\"\n",
    "    save_path = OUT_DIR\n",
    "    results_path = os.path.join(save_path, \"results\")\n",
    "    best_log_history = None\n",
    "elif TRAIN_MODE == \"train_no_optuna\":\n",
    "    suffix = f\"-{criterion.__class__.__name__}-{criterion.regression_loss_fn.__class__.__name__}\"\n",
    "    suffix += f\"-single_sep_{SINGLE_SEP_TOKEN}\"\n",
    "    save_path = OUT_DIR + suffix\n",
    "    results_path = os.path.join(save_path, \"results\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "os.makedirs(results_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe8dc3",
   "metadata": {},
   "source": [
    "## Kaggle Specific Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e163c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir(os.path.join(os.sep, \"kaggle\", \"working\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b3bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip_file_path = \"/kaggle/working/out-EuclideanLoss-MSELoss-single_sep_False\"\n",
    "# shutil.make_archive(zip_file_path, 'zip', zip_file_path)\n",
    "# shutil.rmtree(zip_file_path)\n",
    "# os.remove(zip_file_path + '.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f7f39",
   "metadata": {},
   "source": [
    "## Plots Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_general_learning_curve(log_history, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    train_losses = log_history['total_train_losses']\n",
    "    eval_losses = log_history['total_eval_losses']\n",
    "    epochs = log_history['epochs']\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(epochs, train_losses, 'b-o', label='Training Losses')\n",
    "    plt.plot(epochs, eval_losses, 'r-o', label='Validation Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.title('Learning Curve Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(out_dir, \"general_learning_curve.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_reg_learning_curve(log_history, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    reg_train_losses = log_history['reg_train_losses']\n",
    "    reg_eval_losses = log_history['reg_eval_losses']\n",
    "    epochs = log_history['epochs']\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(epochs, reg_train_losses, 'g-o', label='Regression Training Losses')\n",
    "    plt.plot(epochs, reg_eval_losses, 'm-o', label='Regression Validation Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.title('Regression Learning Curve Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(out_dir, \"reg_learning_curve.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_gen_learning_curve(log_history, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    gen_train_losses = log_history['gen_train_losses']\n",
    "    gen_eval_losses = log_history['gen_eval_losses']\n",
    "    epochs = log_history['epochs']\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(epochs, gen_train_losses, 'c-o', label='Generation Training Losses')\n",
    "    plt.plot(epochs, gen_eval_losses, 'y-o', label='Generation Validation Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.title('Generation Learning Curve Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(out_dir, \"gen_learning_curve.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_loss_weights(log_history, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    reg_loss_weights = log_history.get('reg_loss_weight', [])\n",
    "    gen_loss_weights = log_history.get('gen_loss_weight', [])\n",
    "    epochs = log_history['epochs']\n",
    "    \n",
    "    if not reg_loss_weights or not gen_loss_weights:\n",
    "        print(\"No loss weights to plot.\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(epochs, reg_loss_weights, 'g-o', label='Regression Loss Weight')\n",
    "    plt.plot(epochs, gen_loss_weights, 'c-o', label='Generation Loss Weight')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss Weights')\n",
    "    plt.title('Loss Weights Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(out_dir, \"loss_weights.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_all_learning_curves(log_history, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    epochs = log_history['epochs']\n",
    "    \n",
    "    # Plot total train losses\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(epochs, log_history['total_train_losses'], 'b-o', label='Total Train Losses')\n",
    "    plt.plot(epochs, log_history['total_eval_losses'], 'r-o', label='Total Eval Losses')\n",
    "    plt.plot(epochs, log_history['reg_train_losses'], 'g-o', label='Regression Train Losses')\n",
    "    plt.plot(epochs, log_history['reg_eval_losses'], 'm-o', label='Regression Eval Losses')\n",
    "    plt.plot(epochs, log_history['gen_train_losses'], 'c-o', label='Generation Train Losses')\n",
    "    plt.plot(epochs, log_history['gen_eval_losses'], 'y-o', label='Generation Eval Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.title('Total Train Losses Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(out_dir, \"all_learning_curves.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62adebe2",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c237e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_add(dataset, messages, couple_dir):\n",
    "    if SINGLE_SEP_TOKEN:\n",
    "        all_messages = [msg.group(\"name_content\") + \"[SEP]\" for idx, msg in enumerate(messages)]\n",
    "    else:\n",
    "        all_messages = [msg.group(\"name_content\") + f\"[USR{idx%2}]\" for idx, msg in enumerate(messages)]\n",
    "\n",
    "    input_chat = \"\\n\".join(all_messages)\n",
    "\n",
    "    dataset['chats'].append(input_chat)\n",
    "    dataset['polarities'].append([float(msg.group(\"polarity\")) for msg in messages])\n",
    "    # use the directory as user_id\n",
    "    dataset['user_ids'].append(couple_dir)\n",
    "    dataset['msgs_lengths'].append(len(messages))\n",
    "\n",
    "def load_dataset(path):\n",
    "    msgs_regex = re.compile(r\"(?P<message>\\(?(?P<timestamp>\\d\\d\\d\\d-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d)\\)? ?\\|? ?(?P<name_content>(?P<name>.+):\\n?\\s?(?P<content>.+))\\n?\\s?Polarity: (?P<polarity>(?:-?|\\+?)\\d\\.?\\d?\\d?))\")\n",
    "    explanation_regex = re.compile(r\"Spiegazione:\\n(?P<explanation>(?:.|\\n)+)\")\n",
    "    dataset = {\n",
    "        \"chats\": [],\n",
    "        \"explanations\": [],\n",
    "        \"polarities\": [],\n",
    "        \"user_ids\": [],\n",
    "        \"msgs_lengths\": []\n",
    "    }\n",
    "    skipped = 0\n",
    "    model_dirs = os.listdir(path)\n",
    "    for model_dir in tqdm(model_dirs, desc=\"📂 Loading Dataset\"):\n",
    "        model_dir_path = os.path.join(path, model_dir)\n",
    "        couple_dirs = os.listdir(model_dir_path)\n",
    "        for couple_dir in couple_dirs: # tqdm(couple_dirs, desc=f\"📂 Loading Directory: {model_dir_path}\")\n",
    "            couple_dir_path = os.path.join(model_dir_path, couple_dir)\n",
    "            files = os.listdir(couple_dir_path)\n",
    "            for file in files:\n",
    "                with open(os.path.join(couple_dir_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    chat = f.read()\n",
    "                    messages = list(msgs_regex.finditer(chat))\n",
    "                    if len(messages) > 0: # checks if there are matched messages\n",
    "                        match = explanation_regex.search(chat)\n",
    "                        if match:\n",
    "                            dataset['explanations'].append(match.group(\"explanation\"))\n",
    "                        else:\n",
    "                            print(f\"No explanation found in file: {os.path.join(couple_dir_path, file)}\")\n",
    "                            skipped += 1\n",
    "                            continue\n",
    "                        simple_add(dataset, messages, couple_dir)\n",
    "                    else:\n",
    "                        skipped += 1\n",
    "                        print(f\"No messages found in file: {os.path.join(couple_dir_path, file)}\")\n",
    "                        \n",
    "    return dataset, skipped\n",
    "\n",
    "dataset, skipped = load_dataset(PATH)\n",
    "dataset = Dataset.from_dict(dataset)\n",
    "print(f\"Skipped: {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_info(dataset):\n",
    "    print(dataset)\n",
    "    # For each field, print the first entry\n",
    "    for field in dataset.features:\n",
    "        print(f\"{field}: {dataset[0][field]}\\n\")\n",
    "\n",
    "print_dataset_info(dataset)\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3505aa33",
   "metadata": {},
   "source": [
    "# Pre-Processing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02488f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartWithRegression.get_tokenizer(SINGLE_SEP_TOKEN)\n",
    "\n",
    "def preprocess(examples):\n",
    "    tokenized_chats = tokenizer(\n",
    "        examples['chats'],\n",
    "        # padding='max_length',\n",
    "        # truncation=True,\n",
    "        # max_length=1024,\n",
    "        # return_tensors='pt',\n",
    "        text_target=examples['explanations']\n",
    "    )\n",
    "    tokenized_chats[\"polarities\"] = examples[\"polarities\"]\n",
    "    tokenized_chats[\"user_ids\"] = examples[\"user_ids\"]\n",
    "    tokenized_chats[\"msgs_lengths\"] = examples[\"msgs_lengths\"]\n",
    "    return tokenized_chats\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    # batch_size=1000,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "# print_dataset_info(tokenized_dataset)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490277ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(tokenized_dataset)\n",
    "\n",
    "more_than_1024_input_mask = df['input_ids'].apply(lambda x: len(x) > 1024)\n",
    "more_than_512_input_mask = df['input_ids'].apply(lambda x: len(x) > 512)\n",
    "more_than_1024_labels_mask = df['labels'].apply(lambda x: len(x) > 1024)\n",
    "more_than_512_labels_mask = df['labels'].apply(lambda x: len(x) > 512)\n",
    "\n",
    "# print(f\"Tokenized dataset:\\n{df.head()}\")\n",
    "print(\"input_ids token length statistics:\")\n",
    "print(f\"Number of samples with more than 1024 tokens: {len(df[more_than_1024_input_mask])}\")\n",
    "print(f\"Number of samples with more than 512 tokens: {len(df[more_than_512_input_mask])}\")\n",
    "\n",
    "print(\"\\nlabels token length statistics:\")\n",
    "print(f\"Number of samples with more than 1024 tokens: {len(df[more_than_1024_labels_mask])}\")\n",
    "print(f\"Number of samples with more than 512 tokens: {len(df[more_than_512_labels_mask])}\")\n",
    "\n",
    "# plots histograms for input_ids and labels with different colors in a single plot\n",
    "# with semi-transparent bars in order to visualize overlaps\n",
    "# with 1024 + 1 bins where the last bin is for samples with more than 1024 tokens\n",
    "\n",
    "input_ds_token_lengths = df['input_ids'].apply(lambda x: len(x))\n",
    "labels_ds_token_lengths = df['labels'].apply(lambda x: len(x))\n",
    "\n",
    "input_ds_token_lengths.hist(bins=input_ds_token_lengths.max(), edgecolor='blue', alpha=0.5, label='Input Chats')\n",
    "labels_ds_token_lengths.hist(bins=labels_ds_token_lengths.max(), edgecolor='orange', alpha=0.5, label='Output Explanations')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Token Length Distribution')\n",
    "plt.legend()\n",
    "# plt.savefig(os.path.join(OUT_DIR, \"token_and_message_length_distribution.png\"))\n",
    "plt.show()\n",
    "\n",
    "# min_msgs = df['msgs_lengths'].min()\n",
    "max_msgs = df['msgs_lengths'].max()\n",
    "df['msgs_lengths'].hist(\n",
    "    bins=range(max_msgs + 1),  # +2 so last bin includes max\n",
    "    edgecolor='black',\n",
    "    label='Messages Lengths',\n",
    "    align='left'\n",
    ")\n",
    "plt.xlabel('Number of Messages')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Messages Length Distribution')\n",
    "plt.legend()\n",
    "# plt.savefig(os.path.join(OUT_DIR, \"messages_length_distribution.png\"))\n",
    "plt.yscale('log')\n",
    "plt.xticks(range(max_msgs + 1))\n",
    "plt.show()\n",
    "\n",
    "# Remove all samples with more than 1024 tokens in input_ids and labels\n",
    "df = df[~more_than_1024_input_mask & ~more_than_1024_labels_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c0867",
   "metadata": {},
   "source": [
    "# Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d9c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Users in dataset: {df['user_ids'].nunique()}\")\n",
    "print(f\"Dataset size: {len(df)}\\n\")\n",
    "\n",
    "df.drop(columns=['msgs_lengths'], inplace=True)\n",
    "\n",
    "# Split the dataset into train, test, and eval sets using user_ids grouped examples\n",
    "grouped = df.groupby('user_ids')#.size().reset_index(name='counts')\n",
    "user_ids = list(grouped.groups.keys())#[:10]\n",
    "# df = df[df['user_ids'].isin(user_ids)]\n",
    "\n",
    "train_ids, test_ids = train_test_split(user_ids, test_size=TEST_SIZE, random_state=42)\n",
    "train_ids, eval_ids = train_test_split(train_ids, test_size=TEST_SIZE, random_state=42)\n",
    "tokenized_train_set = df[df['user_ids'].isin(train_ids)]\n",
    "tokenized_test_set = df[df['user_ids'].isin(test_ids)]\n",
    "tokenized_eval_set = df[df['user_ids'].isin(eval_ids)]\n",
    "\n",
    "# Prints how many users are in each set\n",
    "print(f\"Users in train set: {tokenized_train_set['user_ids'].nunique()}\")\n",
    "print(f\"Users in test set: {tokenized_test_set['user_ids'].nunique()}\")\n",
    "print(f\"Users in eval set: {tokenized_eval_set['user_ids'].nunique()}\\n\")\n",
    "\n",
    "# Remove the 'user_ids' column from the train, test, and eval sets\n",
    "tokenized_train_set = tokenized_train_set.drop(columns=['user_ids'])\n",
    "tokenized_test_set = tokenized_test_set.drop(columns=['user_ids'])\n",
    "tokenized_eval_set = tokenized_eval_set.drop(columns=['user_ids'])\n",
    "\n",
    "tokenized_train_set = tokenized_train_set.reset_index(drop=True)\n",
    "tokenized_test_set = tokenized_test_set.reset_index(drop=True)\n",
    "tokenized_eval_set = tokenized_eval_set.reset_index(drop=True)\n",
    "\n",
    "tokenized_train_set = Dataset.from_pandas(tokenized_train_set)\n",
    "tokenized_test_set = Dataset.from_pandas(tokenized_test_set)\n",
    "tokenized_eval_set = Dataset.from_pandas(tokenized_eval_set)\n",
    "\n",
    "print(f\"Train set size: {len(tokenized_train_set)}\")\n",
    "print(f\"Test set size: {len(tokenized_test_set)}\")\n",
    "print(f\"Eval set size: {len(tokenized_eval_set)}\\n\")\n",
    "\n",
    "# Set the format to PyTorch tensors\n",
    "tokenized_train_set.set_format(\"torch\")\n",
    "tokenized_test_set.set_format(\"torch\")\n",
    "tokenized_eval_set.set_format(\"torch\")\n",
    "\n",
    "print_dataset_info(tokenized_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7829e681",
   "metadata": {},
   "source": [
    "# Preparing Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbf46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = MultiTaskBartDataCollator(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_train_set,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    # Parallelize data loading so the GPU\n",
    "    # doesn't have to wait for the CPU\n",
    "    # to prepare the next batch.\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_eval_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb38819",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62afcbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec3c6ad",
   "metadata": {},
   "source": [
    "## Training without Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b85b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODE == \"train_no_optuna\":\n",
    "\n",
    "    model = BartWithRegression(\n",
    "        single_sep_token=SINGLE_SEP_TOKEN, \n",
    "        freeze_bart=True,  # Freeze BART parameters\n",
    "    ).to(DEVICE)\n",
    "    # model = torch.compile(model)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        criterion=criterion,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        # get_scheduler_fn=get_linear_schedule_with_warmup,\n",
    "        warmup_percentage=WARMUP_PERCENTAGE,\n",
    "        body_lr=BODY_LR, head_lr=HEAD_LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        logging=True,\n",
    "        save_path=save_path,\n",
    "        load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, args=args, device=DEVICE,\n",
    "        train_dataloader=train_dataloader,\n",
    "        eval_dataloader=eval_dataloader\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    plot_general_learning_curve(\n",
    "        trainer.log_history,\n",
    "        results_path\n",
    "    )\n",
    "    plot_reg_learning_curve(\n",
    "        trainer.log_history,\n",
    "        results_path\n",
    "    )\n",
    "    plot_gen_learning_curve(\n",
    "        trainer.log_history,\n",
    "        results_path\n",
    "    )\n",
    "\n",
    "    plot_loss_weights(\n",
    "        trainer.log_history,\n",
    "        results_path\n",
    "    )\n",
    "\n",
    "    del model, trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe4a3cd",
   "metadata": {},
   "source": [
    "## Train with Optuna Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the Objective Function for Optuna ---\n",
    "def objective(trial: optuna.Trial):\n",
    "    \"\"\"\n",
    "    An objective function to be maximized or minimized by Optuna.\n",
    "    This function takes a `trial` object, sets up a model with hyperparameters suggested\n",
    "    by the trial, trains it, and returns the performance metric to optimize (eval_loss).\n",
    "    \"\"\"\n",
    "\n",
    "    # BODY_LR = 3e-5\n",
    "    # HEAD_LR = 1.5e-4\n",
    "    criterion = trial.suggest_categorical(\"criterion\", list(losses.keys()))\n",
    "    head_lr = trial.suggest_float(\"head_lr\", 1e-5, 1e-3, log=True)\n",
    "    body_lr = trial.suggest_float(\"body_lr\", 1e-5, 1e-4, log=True)\n",
    "    # batch_size = trial.suggest_categorical(\"batch_size\", [4, 8])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-3, 0.1, log=True)\n",
    "    # label_smoothing = trial.suggest_float(\"label_smoothing\", 0.0, 0.2)\n",
    "    \n",
    "    # print(f\"Trial {trial.number} - Hyperparameters:\")\n",
    "    # print(f\"  Criterion: {criterion}\")\n",
    "    # print(f\"  Head LR: {head_lr:.6f}\")\n",
    "    # print(f\"  Body LR: {body_lr:.6f}\")\n",
    "    # print(f\"  Weight Decay: {weight_decay:.6f}\")\n",
    "\n",
    "    criterion = losses[criterion]\n",
    "\n",
    "    try:\n",
    "        model = BartWithRegression().to(DEVICE)\n",
    "        # model = torch.compile(model)\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            criterion=criterion,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            # get_scheduler_fn=get_linear_schedule_with_warmup,\n",
    "            warmup_percentage=WARMUP_PERCENTAGE,\n",
    "            body_lr=body_lr, head_lr=head_lr,\n",
    "            weight_decay=weight_decay,\n",
    "            early_stopping_patience=None,\n",
    "            logging=True,\n",
    "            save_path=temp_save_path,\n",
    "            load_best_model_at_end=True\n",
    "        )\n",
    "\n",
    "        current_trainer = Trainer(\n",
    "            model=model, args=args, device=DEVICE,\n",
    "            train_dataloader=train_dataloader,\n",
    "            eval_dataloader=eval_dataloader\n",
    "        )\n",
    "        current_trainer.train()\n",
    "\n",
    "        best_eval_loss = min(current_trainer.log_history['total_eval_losses'])\n",
    "\n",
    "        # Check if this is the best trial so far\n",
    "        if trial.number == 0 or best_eval_loss > trial.study.best_value:\n",
    "            shutil.rmtree(save_path)\n",
    "            os.rename(temp_save_path, save_path)\n",
    "            plot_general_learning_curve(\n",
    "                current_trainer.log_history,\n",
    "                results_path\n",
    "            )\n",
    "            plot_reg_learning_curve(\n",
    "                current_trainer.log_history,\n",
    "                results_path\n",
    "            )\n",
    "            plot_gen_learning_curve(\n",
    "                current_trainer.log_history,\n",
    "                results_path\n",
    "            )\n",
    "            plot_loss_weights(\n",
    "                trainer.log_history,\n",
    "                results_path\n",
    "            )\n",
    "            trainer = current_trainer\n",
    "            # print(f\"New best model saved with score: {best_eval_loss:.4f}\")\n",
    "        \n",
    "        # --- Clean up GPU memory before the next trial ---\n",
    "        del model, current_trainer, args\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Return the best validation loss achieved during this trial\n",
    "        return best_eval_loss\n",
    "\n",
    "    except optuna.exceptions.TrialPruned as e:\n",
    "        # --- Handle pruned trials ---\n",
    "        # Clean up memory for pruned trials as well\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        # --- Handle other errors like CUDA OOM ---\n",
    "        print(\"Trial failed with error:\")\n",
    "        traceback.print_exc()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        # Return a very high loss value so Optuna knows this trial was bad\n",
    "        return float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac20b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODE == \"train_with_optuna\":\n",
    "    # Create a study object. `direction=\"minimize\"` means Optuna will try to minimize the return value of `objective`.\n",
    "    # The TPE sampler is the algorithm for Bayesian Optimization.\n",
    "    # The MedianPruner is an aggressive early stopping algorithm.\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(),\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=PRUNER_WARMUP_STEPS) # Prune after the 1st epoch\n",
    "    )\n",
    "\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=OPTUNA_TRAIN_TRIALS,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(f\"  Number of finished trials: {len(study.trials)}\")\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"  Value (min eval loss): {best_trial.value}\")\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    df = study.trials_dataframe()\n",
    "    path = os.path.join(results_path, \"optuna_train_hyperparams_results.csv\")\n",
    "    df.to_csv(path)\n",
    "    print(f\"\\nStudy results saved to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e103a",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf1c46",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = BartWithRegression(\n",
    "#     \".\\\\out\\\\models\\\\Multi-Task BART\\\\Euclidea_2Sep_20_epoche\",\n",
    "#     single_sep_token=SINGLE_SEP_TOKEN,\n",
    "#     verbose=True\n",
    "# ).to(DEVICE)\n",
    "# trainer = Trainer(\n",
    "#     model=loaded_model, args=None, device=DEVICE,\n",
    "#     train_dataloader=None,\n",
    "#     eval_dataloader=eval_dataloader,\n",
    "#     test_dataloader=test_dataloader\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd439bc",
   "metadata": {},
   "source": [
    "## Generation Hyperparameters Tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f361e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_generation(trial: optuna.Trial):\n",
    "    decoding_strategy = trial.suggest_categorical(\"strategy\", [\"beam_search\", \"sampling\"])\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": 1024,\n",
    "        \"repetition_penalty\": trial.suggest_float(\"repetition_penalty\", 1.0, 1.3)\n",
    "    }\n",
    "\n",
    "    if decoding_strategy == \"beam_search\":\n",
    "        gen_kwargs[\"num_beams\"] = trial.suggest_int(\"num_beams\", 2, 8)\n",
    "        gen_kwargs[\"early_stopping\"] = True # Usually a good idea with beam search\n",
    "    \n",
    "    elif decoding_strategy == \"sampling\":\n",
    "        gen_kwargs[\"do_sample\"] = True\n",
    "        gen_kwargs[\"top_p\"] = trial.suggest_float(\"top_p\", 0.85, 0.98)\n",
    "        gen_kwargs[\"top_k\"] = trial.suggest_int(\"top_k\", 20, 100) # Optional, often top_p is enough\n",
    "        gen_kwargs[\"temperature\"] = trial.suggest_float(\"temperature\", 0.7, 1.0)\n",
    "\n",
    "    print(f\"--- Trial {trial.number}: Testing with {gen_kwargs} ---\")\n",
    "\n",
    "    # This is fast because it's only inference\n",
    "    results = trainer.evaluate(eval_dataloader, leave=False, **gen_kwargs)\n",
    "\n",
    "    # We want to maximize a semantic score. Let's choose BERTScore F1.\n",
    "    metric_to_optimize = 'sbert_similarity'\n",
    "    score = results[metric_to_optimize]\n",
    "    print(f\"Trial {trial.number} Result -> {metric_to_optimize}: {score:.4f}\")\n",
    "    \n",
    "    # Clean up memory just in case, although less critical for inference\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return metric_to_optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b9e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE == \"test_with_optuna\":\n",
    "    # For this search, we want to MAXIMIZE the score.\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=optuna.samplers.TPESampler(),\n",
    "        # Pruning is not needed here because each trial is just one full evaluation,\n",
    "        # there are no intermediate steps to prune.\n",
    "    )\n",
    "\n",
    "    # Run the optimization. 20-30 trials should be plenty for this.\n",
    "    study.optimize(\n",
    "        objective_generation,\n",
    "        n_trials=OPTUNA_TEST_TRIALS,\n",
    "        show_progress_bar=True,\n",
    "        # n_jobs=-1\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\n--- Generation Hyperparameter Search Complete ---\")\n",
    "    print(f\"  Number of finished trials: {len(study.trials)}\")\n",
    "\n",
    "    print(\"Best trial for generation:\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"    Best Score: {best_trial.value:.4f}\")\n",
    "\n",
    "    print(\"  Best Generation Params: \")\n",
    "    best_hyperparams = best_trial.params\n",
    "    for key, value in best_hyperparams.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "    df = study.trials_dataframe()\n",
    "    path = os.path.join(results_path, \"optuna_gen_hyperparams_results.csv\")\n",
    "    df.to_csv(path)\n",
    "    print(f\"\\nGeneration study results saved to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce2d91",
   "metadata": {},
   "source": [
    "## Default Generation Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE == \"test_no_optuna\":\n",
    "    best_hyperparams = {\n",
    "        # \"num_beams\": 1,\n",
    "        # \"repetition_penalty\": 1.2,\n",
    "        # \"early_stopping\": True,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.95,\n",
    "        # Top-k sampling is a simple generalization of greedy decoding. Instead of choosing\n",
    "        # the single most probable word to generate, we first truncate the distribution to the\n",
    "        # top k most likely words, renormalize to produce a legitimate probability distribution,\n",
    "        # and then randomly sample from within these k words according to their renormalized\n",
    "        # probabilities.\n",
    "        \"top_k\": 20,\n",
    "        \"temperature\": 0.6 # 0.6\n",
    "    }\n",
    "    print(f\"Using default hyperparameters: {best_hyperparams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70ab6fa",
   "metadata": {},
   "source": [
    "## Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541d6739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inference(model, input_text):\n",
    "#     \"\"\"\n",
    "#     Run inference on a single input text and print the results.\n",
    "#     \"\"\"\n",
    "#     tokenized = tokenizer(\n",
    "#         input_text, \n",
    "#         return_tensors='pt', \n",
    "#         truncation=True, \n",
    "#         max_length=1024\n",
    "#     ).to(DEVICE)\n",
    "#     # input_len = tokenized['input_ids'].shape[1]\n",
    "#     # max_length = min(int(input_len * 3.95), 1024)  # 20% longer than input, capped at 1024\n",
    "#     max_length = 1024  # Use a fixed max length for generation\n",
    "\n",
    "#     output = model.generate(\n",
    "#         input_ids=tokenized['input_ids'][0].unsqueeze(0),\n",
    "#         attention_mask=tokenized['attention_mask'][0].unsqueeze(0),\n",
    "#         max_length=max_length,\n",
    "#         **best_hyperparams\n",
    "#     )\n",
    "\n",
    "#     decoded_output = tokenizer.decode(\n",
    "#         output['explanations'][0], \n",
    "#         skip_special_tokens=True\n",
    "#     )\n",
    "\n",
    "#     return {\n",
    "#         \"polarities\": output['polarities'],\n",
    "#         \"explanations\": decoded_output\n",
    "#     }\n",
    "\n",
    "\n",
    "# print(best_hyperparams)\n",
    "# chat = '''\n",
    "# Topolina:\n",
    "# Ciao[USR0]\n",
    "# '''\n",
    "# output = inference(trainer.model, chat)\n",
    "# print(f\"Output Polarities: {output['polarities']}\")\n",
    "# print(f\"Output Explanations: {output['explanations']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in tokenized_train_set.select(range(1)):\n",
    "    decoded_chat = tokenizer.decode(example['input_ids']) # , skip_special_tokens=True\n",
    "    decoded_true_explanation = tokenizer.decode(example['labels'], skip_special_tokens=True)\n",
    "\n",
    "    output = inference(trainer.model, decoded_chat)\n",
    "\n",
    "    print(f\"Message:\\n{decoded_chat}\")\n",
    "    print(f\"True Polarity:{example['polarities']}\")\n",
    "    print(f\"True Explanation:\\n{decoded_true_explanation}\\n\")\n",
    "    print(f\"Generated Polarity:{output['polarities']}\")\n",
    "    print(f\"Generated Explanation:\\n{output['explanations']}\\n\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d49cdcf",
   "metadata": {},
   "source": [
    "## Evaluation of the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fd87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(**best_hyperparams)\n",
    "with open(os.path.join(results_path, \"test_results.txt\"), \"w\") as f:\n",
    "    for key, value in results.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "        f.write(f\"{key}: {value:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
