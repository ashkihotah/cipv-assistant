{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab09196",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb8478",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_regex = re.compile(r\"(?P<stage>Stage: .+)\\nChat Polarity Mean: (?:-?|\\+?)\\d\\.?\\d?\\d?\\nChat Polarity Variance: \\d\\.?\\d?\\d?\\n(?P<event>Event: .+)\\n\\n(?P<chat>(?:.+|\\n+)+)\")\n",
    "msgs_regex = re.compile(r\"(?P<message>(?P<timestamp>\\d\\d\\d\\d-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d) \\| (?P<name>.+):\\n(?P<content>.+)\\nPolarity: (?P<polarity>(?:-?|\\+?)\\d\\.?\\d?\\d?)\\n\\[(?P<tag_explanation>Tag: (?P<tag>.+)\\nSpiegazione: (?P<explanation>.+))\\])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e747c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer_bart = AutoTokenizer.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../rsc/gemini-2.5-flash-dataset_2025-07-07-10-45-16/chats\"\n",
    "dirs = os.listdir(path)\n",
    "bert_lengths = []\n",
    "bart_lengths = []\n",
    "polarities = []\n",
    "for directory in dirs:\n",
    "    files = os.listdir(os.path.join(path, directory))\n",
    "    for file in files:\n",
    "        bert_count = 0\n",
    "        bart_count = 0\n",
    "        polarity_sum = 0\n",
    "        with open(os.path.join(path, directory, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            match = top_regex.match(content)\n",
    "            if match:\n",
    "                chat = match.group(\"chat\")\n",
    "                messages = msgs_regex.finditer(chat)\n",
    "                total = 0\n",
    "                for message in messages:\n",
    "                    content = message.group(\"content\")\n",
    "                    if content:\n",
    "                        bert_tokens = tokenizer_bert.encode(content, add_special_tokens=True)\n",
    "                        bart_tokens = tokenizer_bart.encode(content, add_special_tokens=True)\n",
    "                        bert_count += len(bert_tokens)\n",
    "                        bart_count += len(bart_tokens)\n",
    "                    polarity_sum += float(message.group(\"polarity\"))\n",
    "                    total += 1\n",
    "                # try:\n",
    "                #     polarities.append(polarity_sum / total)\n",
    "                # except ZeroDivisionError:\n",
    "                #     print(f\"ZeroDivisionError in file: {file}\")\n",
    "                bert_lengths.append(bert_count)\n",
    "                bart_lengths.append(bart_count)\n",
    "            else:\n",
    "                print(f\"No match found in file: {os.path.join(path, directory, file)}\")\n",
    "            \n",
    "\n",
    "bert_v = np.array(bert_lengths)\n",
    "bart_v = np.array(bart_lengths)\n",
    "polarities_v = np.array(polarities)\n",
    "\n",
    "print(f\"BERT Max Token Count: {np.max(bert_v)}, Min Token Count: {np.min(bert_v)}\")\n",
    "print(f\"BERT Mean Token Count: {np.mean(bert_v):.2f}\")\n",
    "print(f\"BERT Variance Token Count: {np.std(bert_v):.2f}\")\n",
    "print(f\"BERT Over 512 Tokens: {len(bert_v[bert_v > 512])} / {bert_v.shape[0]}\\n\")\n",
    "\n",
    "print(f\"BART Max Token Count: {np.max(bart_v)}, Min Token Count: {np.min(bart_v)}\")\n",
    "print(f\"BART Mean Token Count: {np.mean(bart_v):.2f}\")\n",
    "print(f\"BART Variance Token Count: {np.std(bart_v):.2f}\")\n",
    "print(f\"BART Over 1024 Tokens: {len(bart_v[bart_v > 1024])} / {bart_v.shape[0]}\\n\")\n",
    "\n",
    "print(f\"Polarities Mean: {np.mean(polarities_v):.2f}\")\n",
    "print(f\"Polarities Variance: {np.std(polarities_v):.2f}\")\n",
    "print(f\"Polarities Min: {np.min(polarities_v):.2f}\")\n",
    "print(f\"Polarities Max: {np.max(polarities_v):.2f}\")\n",
    "print(f\"Polarities Over 0: {len(polarities_v[polarities_v > 0])} / {polarities_v.shape[0]}\")\n",
    "print(f\"Polarities Under 0: {len(polarities_v[polarities_v < 0])} / {polarities_v.shape[0]}\")\n",
    "print(f\"Polarities Around 0: {len(polarities_v[np.logical_and(polarities_v < 0.5, polarities_v > -0.5)])} / {polarities_v.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3fb4f6",
   "metadata": {},
   "source": [
    "# BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b4d1b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicol\\Desktop\\cipv-assistant\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    BartPreTrainedModel,\n",
    "    BartModel,\n",
    ")\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union, List, Dict, Any\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "import evaluate # Hugging Face's library for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189c0b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0. Setup Dependencies ---\n",
    "# Ensure you have the necessary libraries installed:\n",
    "# pip install \"evaluate>=0.4.0\" \"rouge_score>=0.1.2\" \"nltk>=3.8.1\"\n",
    "\n",
    "# --- 1. Setup & Dummy Chat Dataset ---\n",
    "# Each item in the list is a full conversation.\n",
    "raw_chats = [\n",
    "    {\n",
    "        \"messages\": [\"Sei un incompetente!\", \"Davvero, non capisci niente.\"],\n",
    "        \"polarities\": [-0.9, -1.0],\n",
    "        \"explanations\": [\"Il messaggio contiene un insulto.\", \"Il messaggio rafforza l'attacco personale.\"]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\"Grazie mille per l'aiuto.\", \"Sei stato gentilissimo.\", \"Apprezzo molto il tuo tempo.\"],\n",
    "        \"polarities\": [1.0, 0.9, 0.9],\n",
    "        \"explanations\": [\"Esprime gratitudine esplicita.\", \"Contiene un complimento diretto.\", \"Mostra apprezzamento per lo sforzo altrui.\"]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\"Non sono d'accordo con questa decisione.\"],\n",
    "        \"polarities\": [0.1],\n",
    "        \"explanations\": [\"Esprime disaccordo in modo neutrale e rispettoso.\"]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\"Fai schifo.\", \"Spero che ti licenzino.\"],\n",
    "        \"polarities\": [-1.0, -1.0],\n",
    "        \"explanations\": [\"Contiene un insulto grave.\", \"Contiene un augurio negativo e minaccioso.\"]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\"Il prodotto Ã¨ arrivato rotto.\", \"Il servizio clienti non risponde.\", \"Sono molto insoddisfatto.\"],\n",
    "        \"polarities\": [-0.8, -0.7, -0.9],\n",
    "        \"explanations\": [\"Descrive un problema con il prodotto.\", \"Lamenta una mancanza di supporto.\", \"Esprime insoddisfazione generale.\"]\n",
    "    }\n",
    "]\n",
    "# Convert to a Hugging Face Dataset\n",
    "dataset = Dataset.from_list(raw_chats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0205906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special token ID for '<sep>': 52000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 13125, 329, 7635, 40980, 73, 5, 52000, 35375, 16, 384, 43722, 3900, 18, 52000, 2], [0, 4876, 4881, 300, 264, 11, 7560, 18, 52000, 13125, 781, 42167, 18, 52000, 8326, 14431, 765, 311, 1163, 876, 18, 52000, 2], [0, 1876, 458, 322, 11, 5151, 225, 71, 83, 82, 782, 4543, 18, 52000, 2], [0, 24501, 28141, 18, 52000, 19765, 312, 710, 527, 1099, 45297, 18, 52000, 2], [0, 714, 1882, 345, 5904, 20121, 18, 52000, 714, 1533, 2603, 384, 6452, 18, 52000, 3327, 765, 25849, 15643, 18, 52000, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[0, 714, 4789, 5988, 329, 7483, 2755, 18, 52000, 714, 4789, 6805, 264, 11, 10242, 2116, 18, 52000, 2], [0, 41, 14195, 272, 25644, 28711, 18, 52000, 2669, 5759, 329, 50757, 4611, 18, 52000, 26017, 27068, 300, 511, 9090, 14550, 18, 52000, 2], [0, 41, 14195, 272, 48967, 225, 77, 82, 927, 49559, 266, 34177, 18, 52000, 2], [0, 2669, 5759, 329, 7483, 2755, 5437, 18, 52000, 2669, 5759, 329, 35951, 9156, 266, 39746, 281, 18, 52000, 2], [0, 5317, 19123, 329, 2092, 225, 71, 83, 82, 311, 1882, 18, 52000, 729, 928, 368, 5588, 322, 77, 3795, 18, 52000, 41, 14195, 272, 25849, 15977, 2536, 18, 52000, 2]], 'regression_labels': [[-0.9, -1.0], [1.0, 0.9, 0.9], [0.1], [-1.0, -1.0], [-0.8, -0.7, -0.9]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model and special token\n",
    "MODEL_CHECKPOINT = \"morenolq/bart-it\"\n",
    "SEP_TOKEN = \"<sep>\"\n",
    "\n",
    "# --- 2. Tokenizer Setup ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "tokenizer.add_special_tokens({'sep_token': SEP_TOKEN})\n",
    "sep_token_id = tokenizer.sep_token_id\n",
    "print(f\"Special token ID for '{SEP_TOKEN}': {sep_token_id}\")\n",
    "\n",
    "# --- 4. Preprocessing ---\n",
    "def preprocess_function(chats):\n",
    "    inputs = [SEP_TOKEN.join(chat['messages']) + SEP_TOKEN for chat in chats]\n",
    "    targets = [SEP_TOKEN.join(chat['explanations']) + SEP_TOKEN for chat in chats]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=512, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    model_inputs[\"regression_labels\"] = [chat[\"polarities\"] for chat in chats]\n",
    "    return model_inputs\n",
    "\n",
    "preprocess_function(raw_chats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02049e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Custom Data Collator ---\n",
    "@dataclass\n",
    "class DataCollatorForChat:\n",
    "    tokenizer: AutoTokenizer\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    label_pad_token_id: int = -100\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # Separate regression labels before padding the rest\n",
    "        regression_labels = [f.pop(\"regression_labels\") for f in features]\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        max_messages = max(len(p) for p in regression_labels)\n",
    "        padded_polarities = []\n",
    "        for p in regression_labels:\n",
    "            padded_list = p + [self.label_pad_token_id] * (max_messages - len(p))\n",
    "            padded_polarities.append(padded_list)\n",
    "        \n",
    "        batch[\"regression_labels\"] = torch.tensor(padded_polarities, dtype=torch.float)\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorForChat(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ba72ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Custom Model Definition ---\n",
    "@dataclass\n",
    "class ChatMultiTaskOutput(Seq2SeqLMOutput):\n",
    "    regression_loss: Optional[torch.FloatTensor] = None\n",
    "    regression_logits: Optional[torch.FloatTensor] = None\n",
    "\n",
    "class BartForChatRegressionAndGeneration(BartPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = BartModel(config)\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.d_model, 1)\n",
    "        )\n",
    "        self.post_init() # Weights initialization and other post-initialization tasks\n",
    "        if not hasattr(config, 'sep_token_id'):\n",
    "            raise ValueError(\"sep_token_id must be set in the model config.\")\n",
    "        self.sep_token_id = config.sep_token_id\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        # indicates which tokens are padding (0) or not (1)\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        regression_labels: Optional[torch.FloatTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Union[Tuple, ChatMultiTaskOutput]:\n",
    "        \n",
    "        encoder_outputs = self.model.encoder(\n",
    "            input_ids=input_ids,\n",
    "            # ignore padding tokens in the attention mask\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        encoder_last_hidden_state = encoder_outputs.last_hidden_state\n",
    "\n",
    "        # retrieve the hidden states corresponding to the special separator token <sep>\n",
    "        sep_token_mask = (input_ids == self.sep_token_id)\n",
    "        sep_hidden_states = encoder_last_hidden_state[sep_token_mask]\n",
    "        \n",
    "        # If there are no <sep> tokens, we return an empty tensor for regression logits\n",
    "        regression_logits = torch.tensor([], device=input_ids.device)\n",
    "        if sep_hidden_states.shape[0] > 0:\n",
    "            regression_logits = self.regression_head(sep_hidden_states).squeeze(-1)\n",
    "            regression_logits = torch.tanh(regression_logits)\n",
    "\n",
    "        # Calculate regression loss if regression labels are provided\n",
    "        regression_loss = None\n",
    "        if regression_labels is not None:\n",
    "            # We flatten the regression labels to match the shape of regression_logits\n",
    "            valid_regression_labels = regression_labels.view(-1)\n",
    "            # Create a mask to ignore padding tokens in regression labels\n",
    "            valid_mask = valid_regression_labels != -100\n",
    "            if regression_logits.shape[0] == valid_regression_labels[valid_mask].shape[0]:\n",
    "                loss_fct = MSELoss()\n",
    "                regression_loss = loss_fct(regression_logits, valid_regression_labels[valid_mask])\n",
    "            else:\n",
    "                regression_loss = torch.tensor(0.0, device=input_ids.device)\n",
    "        \n",
    "        # aggiunta da me\n",
    "        decoder_attention_mask = (decoder_input_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "        # We need to pass decoder_input_ids for the generation task during training\n",
    "        decoder_input_ids = kwargs.get('decoder_input_ids')\n",
    "        decoder_outputs = self.model.decoder(\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            encoder_hidden_states=encoder_last_hidden_state,\n",
    "            # used in the self-attention to mask out padding tokens in the target text\n",
    "            decoder_attention_mask=decoder_attention_mask, # aggiunta da me\n",
    "            # used in the cross-attention layer to mask out embeddings of padding tokens\n",
    "            encoder_attention_mask=attention_mask,\n",
    "        )\n",
    "        sequence_output = decoder_outputs.last_hidden_state\n",
    "        lm_logits = self.lm_head(sequence_output)\n",
    "\n",
    "        generation_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "            generation_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        total_loss = None\n",
    "        if generation_loss is not None and regression_loss is not None:\n",
    "            regression_weight = 1.0\n",
    "            total_loss = generation_loss + (regression_weight * regression_loss)\n",
    "\n",
    "        return ChatMultiTaskOutput(\n",
    "            loss=total_loss,\n",
    "            logits=lm_logits,\n",
    "            regression_loss=regression_loss,\n",
    "            regression_logits=regression_logits,\n",
    "            encoder_last_hidden_state=encoder_last_hidden_state,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset before tokenizing\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=test_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b3866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Metrics Computation ---\n",
    "nltk.download('punkt', quiet=True)\n",
    "rouge_metric = evaluate.load('rouge')\n",
    "bleu_metric = evaluate.load('bleu')\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # Decode generated summaries, replacing -100 padding with pad_token\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Decode reference summaries, replacing -100 padding with pad_token\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Simple text cleaning\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds_rouge = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in decoded_preds]\n",
    "    decoded_labels_rouge = [\"\\n\".join(nltk.sent_tokenize(label)) for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    rouge_result = rouge_metric.compute(predictions=decoded_preds_rouge, references=decoded_labels_rouge)\n",
    "    \n",
    "    # Compute BLEU scores\n",
    "    decoded_labels_bleu = [[label] for label in decoded_labels] # BLEU expects a list of references\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels_bleu)\n",
    "\n",
    "    result = {\n",
    "        \"rouge1\": rouge_result[\"rouge1\"],\n",
    "        \"rouge2\": rouge_result[\"rouge2\"],\n",
    "        \"rougeL\": rouge_result[\"rougeL\"],\n",
    "        \"bleu\": bleu_result[\"bleu\"]\n",
    "    }\n",
    "\n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "# --- 7. Training ---\n",
    "model = BartForChatRegressionAndGeneration.from_pretrained(MODEL_CHECKPOINT)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.sep_token_id = sep_token_id\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./bart-it-chat-multitask\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=30,\n",
    "    predict_with_generate=True, # Crucial for generation metrics\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=5,\n",
    ")\n",
    "\n",
    "class CustomChatTrainer(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = CustomChatTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics, # Add metrics computation\n",
    ")\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "trainer.save_model(\"./bart-it-chat-multitask-final\")\n",
    "tokenizer.save_pretrained(\"./bart-it-chat-multitask-final\")\n",
    "\n",
    "# --- 8. Final Evaluation on Test Set ---\n",
    "print(\"\\n--- Evaluating on Test Set ---\")\n",
    "eval_results = trainer.evaluate(eval_dataset=tokenized_test_dataset)\n",
    "\n",
    "# Calculate and add perplexity\n",
    "perplexity = math.exp(eval_results['eval_loss'])\n",
    "eval_results['perplexity'] = round(perplexity, 4)\n",
    "\n",
    "print(\"\\n--- Final Evaluation Metrics ---\")\n",
    "for key, value in sorted(eval_results.items()):\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "# --- 9. Inference ---\n",
    "print(\"\\n--- Running Inference ---\")\n",
    "# ... (Inference code remains the same)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
