{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d14544a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.4\n",
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d3990",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a41f6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import traceback\n",
    "import itertools\n",
    "import shutil\n",
    "import optuna\n",
    "import gc\n",
    "import re\n",
    "import os\n",
    "\n",
    "from multitask_bart.single_target import (\n",
    "    BartWithRegression,\n",
    "    MultiTaskBartDataCollator,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from multitask_bart.losses import (\n",
    "    EuclideanLoss,\n",
    "    StaticWeightedLoss,\n",
    "    UncertaintyLoss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4792a16",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60769c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "SEP_TOKEN = BartWithRegression.SEP_TOKEN\n",
    "TARGET_TOKEN = BartWithRegression.TARGET_TOKEN\n",
    "PATH = os.path.join(\".\", \"out\", \"datasets\", \"toxicity\", \"gen1\", \"chats\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using device: {torch.cuda.get_device_name(0)}\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "OUT_DIR = os.path.join(\".\", \"out\", \"models\", timestamp)\n",
    "TRAIN_MODE = \"train_no_optuna\"\n",
    "# TRAIN_MODE = \"train_with_optuna\"\n",
    "TEST_MODE = \"test_no_optuna\"\n",
    "# TEST_MODE = \"test_with_optuna\"\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "NUM_EPOCHS = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "WARMUP_PERCENTAGE = 0.1\n",
    "WEIGHT_DECAY = 0.01 # int the 0 to 0.1 range\n",
    "BODY_LR = 3e-5\n",
    "HEAD_LR = 1.5e-4\n",
    "\n",
    "OPTUNA_TRAIN_TRIALS=2\n",
    "OPTUNA_TEST_TRIALS=2\n",
    "PRUNER_WARMUP_STEPS=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea5b0ff",
   "metadata": {},
   "source": [
    "# Loading and Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62adebe2",
   "metadata": {},
   "source": [
    "## Loading and Pre-Processing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c240486c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2a6e1b67ce4389b6c4bfa7575d7515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üìÇ Loading Dataset:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65420951ec57453eaafc3cee1775a2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üìÇ Loading Directory: .\\out\\datasets\\toxicity\\gen1\\chats\\gemini-2.0-flash-dataset_2025-07-09-15-56-22:   0%|   ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d261dfa66d5436288c8f4aba54bc29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üìÇ Loading Directory: .\\out\\datasets\\toxicity\\gen1\\chats\\gemini-2.5-flash-dataset_2025-07-07-10-45-16:   0%|   ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a524c986bd8641baaefcb1d44f77781c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üìÇ Loading Directory: .\\out\\datasets\\toxicity\\gen1\\chats\\gemini-2.5-flash-dataset_2025-07-08-20-06-22:   0%|   ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45dcc33bd52d427eba9ff3d739bd9df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üìÇ Loading Directory: .\\out\\datasets\\toxicity\\gen1\\chats\\gemini-2.5-flash-dataset_2025-07-16-09-07-33:   0%|   ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce82e6ef71ca46e3b19db1267576eda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üìÇ Loading Directory: .\\out\\datasets\\toxicity\\gen1\\chats\\gemini-2.5-flash-dataset_2025-07-19-14-42-59:   0%|   ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c1e540bda249f28fce23393054f557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üìÇ Loading Directory: .\\out\\datasets\\toxicity\\gen1\\chats\\gemini-2.5-flash-lite-preview-06-17-dataset_2025-07-09‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: 201\n"
     ]
    }
   ],
   "source": [
    "def add_all_consecutive_subsets(dataset, messages, couple_dir):\n",
    "    all_messages = [msg.group(\"name_content\") for msg in messages]\n",
    "    for target_idx, msg in enumerate(messages, start=0):\n",
    "        for pre_context in range(target_idx + 1):\n",
    "            for post_context in range(target_idx + 1, len(messages) + 1):\n",
    "                # Re-assemble the context for each example\n",
    "                context_parts = all_messages[target_idx-pre_context: target_idx] + \\\n",
    "                                [all_messages[target_idx] + TARGET_TOKEN] + \\\n",
    "                                all_messages[target_idx + 1: post_context]\n",
    "                input_chat = (SEP_TOKEN + \"\\n\").join(context_parts) + SEP_TOKEN\n",
    "\n",
    "                dataset['chats'].append(input_chat)\n",
    "                dataset['polarities'].append(float(msg.group(\"polarity\")))\n",
    "                dataset['explanations'].append(msg.group(\"explanation\"))\n",
    "                # use the directory as user_id\n",
    "                dataset['user_ids'].append(couple_dir)\n",
    "\n",
    "# def add_(dataset, messages, couple_dir):\n",
    "    # all_messages = [msg.group(\"name_content\") for msg in messages]\n",
    "    # for target_idx, msg in enumerate(messages, start=0):\n",
    "    #     for idx in range(target_idx, len(messages)):\n",
    "    #         context_parts = all_messages[:target_idx] + \\\n",
    "    #                         [all_messages[target_idx] + TARGET_TOKEN] + \\\n",
    "    #                         all_messages[target_idx + 1 : idx + 1]\n",
    "    #         input_chat = (SEP_TOKEN + \"\\n\").join(context_parts) + SEP_TOKEN\n",
    "            \n",
    "    #         dataset['chats'].append(input_chat)\n",
    "    #         dataset['polarities'].append(float(msg.group(\"polarity\")))\n",
    "    #         dataset['explanations'].append(msg.group(\"explanation\"))\n",
    "    #         dataset['user_ids'].append(couple_dir)\n",
    "\n",
    "def simple_add(dataset, messages, couple_dir):\n",
    "    all_messages = [msg.group(\"name_content\") for msg in messages]\n",
    "    for target_idx, msg in enumerate(messages, start=0):\n",
    "        context_parts = all_messages[:target_idx] + \\\n",
    "                        [all_messages[target_idx] + TARGET_TOKEN] + \\\n",
    "                        all_messages[target_idx + 1:]\n",
    "        input_chat = (SEP_TOKEN + \"\\n\").join(context_parts) + SEP_TOKEN\n",
    "        \n",
    "        dataset['chats'].append(input_chat)\n",
    "        dataset['polarities'].append(float(msg.group(\"polarity\")))\n",
    "        dataset['explanations'].append(msg.group(\"explanation\"))\n",
    "        # use the directory as user_id\n",
    "        dataset['user_ids'].append(couple_dir)\n",
    "\n",
    "def load_dataset(path):\n",
    "    msgs_regex = re.compile(r\"(?P<message>(?P<timestamp>\\d\\d\\d\\d-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d) \\|? ?(?P<name_content>(?P<name>.+):\\n(?P<content>.+))\\n+Polarity: (?P<polarity>[-+]?\\d\\.?\\d?\\d?)\\n\\[(?P<tag_explanation>(?P<tag>Tag: .+)\\n?Spiegazione: (?P<explanation>.+))\\])\")\n",
    "    dirs = os.listdir(path)\n",
    "    dataset = {\n",
    "        \"chats\": [],\n",
    "        \"explanations\": [],\n",
    "        \"polarities\": [],\n",
    "        \"user_ids\": []\n",
    "    }\n",
    "    skipped = 0\n",
    "    model_dirs = os.listdir(path)\n",
    "    for model_dir in tqdm(model_dirs, desc=\"üìÇ Loading Dataset\"):\n",
    "        model_dir_path = os.path.join(path, model_dir)\n",
    "        couple_dirs = os.listdir(model_dir_path)\n",
    "        for couple_dir in tqdm(couple_dirs, desc=f\"üìÇ Loading Directory: {model_dir_path}\"):\n",
    "            couple_dir_path = os.path.join(model_dir_path, couple_dir)\n",
    "            files = os.listdir(couple_dir_path)\n",
    "            for file in files:\n",
    "                with open(os.path.join(couple_dir_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    chat = f.read()\n",
    "                    messages = list(msgs_regex.finditer(chat))\n",
    "                    if len(messages) > 0: # checks if there are matched messages\n",
    "                        simple_add(dataset, messages, couple_dir)\n",
    "                    else:\n",
    "                        skipped += 1\n",
    "                        # print(f\"No messages found in file: {os.path.join(path, directory, file)}\")\n",
    "    return dataset, skipped\n",
    "\n",
    "dataset, skipped = load_dataset(PATH)\n",
    "dataset = Dataset.from_dict(dataset)\n",
    "print(f\"Skipped: {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "85e1e206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['chats', 'explanations', 'polarities', 'user_ids'],\n",
      "    num_rows: 154770\n",
      "})\n",
      "chats: Sofia:\n",
      "Ciao Marco, come stai?[TARGET][SEP]\n",
      "\n",
      "explanations: Inizio cortese, ma con una certa formalit√†.\n",
      "\n",
      "polarities: 0.8\n",
      "\n",
      "user_ids: 2025-07-09-15-56-39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_dataset_info(dataset):\n",
    "    print(dataset)\n",
    "    # For each field, print the first entry\n",
    "    for field in dataset.features:\n",
    "        print(f\"{field}: {dataset[0][field]}\\n\")\n",
    "\n",
    "print_dataset_info(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c02488f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartWithRegression.get_tokenizer()\n",
    "def preprocess(examples):\n",
    "    tokenized_chats = tokenizer(\n",
    "        examples['chats'],\n",
    "        # padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        # return_tensors='pt',\n",
    "        text_target=examples['explanations']\n",
    "    )\n",
    "    tokenized_chats[\"polarities\"] = examples[\"polarities\"]\n",
    "    tokenized_chats[\"user_ids\"] = examples[\"user_ids\"]\n",
    "    return tokenized_chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490277ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e441ec506a13410fb011ea0f6082261f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    # batch_size=1000,\n",
    "    remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c0867",
   "metadata": {},
   "source": [
    "## Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86d9c0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users in dataset: 10\n",
      "Dataset size: 2224\n",
      "\n",
      "Users in train set: 6\n",
      "Users in test set: 2\n",
      "Users in eval set: 2\n",
      "\n",
      "Train set size: 1340\n",
      "Test set size: 402\n",
      "Eval set size: 482\n",
      "\n",
      "Dataset({\n",
      "    features: ['polarities', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1340\n",
      "})\n",
      "polarities: 0.5\n",
      "\n",
      "input_ids: tensor([    0, 25522, 12963,    30,   203, 27273,    86,    69,    16,  5136,\n",
      "        13546,   637,   368,  1190,  1777,    18,  2576,  2485,  1296,   458,\n",
      "         1526,  6081,  5937,   300,   311,  1233,  1705,   339,   889,   287,\n",
      "         9724,    18, 49613,   301,  1798,  2948, 10329,   266,  5754,   384,\n",
      "          429,   437,  8559,   765,  2230,  9745,   300,  1494,    18, 52001,\n",
      "        52000, 27273,    86,    69, 23562,    30,   203, 33011,    16, 14795,\n",
      "           16,  3046,    18,  3806,    81,  1955,   377,  2276,  1004,    16,\n",
      "        26044, 28501, 20336,    18,  1474, 16359,  1812,   312,   473,   676,\n",
      "          975,  1004,  7905,  4687,    18,   488,   384,   710, 26151,   300,\n",
      "          400,    16,  2045,    18,   581,  1848, 13706,   266,   301,  1848,\n",
      "        10329,   681,  1005,   458,   301,  1190,   467,  1777,  4448,    18,\n",
      "        44872,   823,   511,  2230,   322,    77,   638,  2340,  2237,    18,\n",
      "         2554,  1386,   300,   386,    18, 11130,   256,   102, 50825, 52000,\n",
      "        25522, 12963,    30,   203,  8326, 14431,   765,   301,  1848, 10254,\n",
      "           16, 34406,    18,  1282,   345,   368,  3523,  2116,    16,   346,\n",
      "          368,  3246, 17705,   300,  5092,   225,    69,    80,  1751,   782,\n",
      "         2829,  7083,    18,  1474,   411,  4987,  3180,   312,  2248,   322,\n",
      "           77,  3795,    18, 52000, 27273,    86,    69, 23562,    30,   203,\n",
      "        40100,  4900,    16,  3046,    18,   389,    11,  1242,  6691,  8748,\n",
      "           18,   581,  1848, 18981,   225,    69,    80,  1005,   345,   368,\n",
      "          496,  2098,   312, 18140,   275,   322,    77,   467,   225,    77,\n",
      "           82,   386,    18,   703,  2340,  2237,   322,    77,  2357,   322,\n",
      "           77, 10166,    16,   493,   722,   312,   710, 13233,   287,  5161,\n",
      "          332,   710, 11329,   225,    71,    83,    82,  1438,  7740,   300,\n",
      "        11876, 10196,   876,    16,   411, 11356,  3180,    18, 28088,    16,\n",
      "          225,    71,    83,    82,    88,    69,   339,   322,    77,   400,\n",
      "            5, 31981,   237,  1282,  2248,   722,   225,    77,    82,   579,\n",
      "           18, 52000, 25522, 12963,    30,   203,  4876,    16, 34406,    18,\n",
      "         1875, 34535, 24132,    18,  3957,  1077,   521, 16728,   965,   301,\n",
      "         2421,   330, 37403,  2600,    18, 52000,     2])\n",
      "\n",
      "attention_mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "labels: tensor([    0, 25349,    30, 19433,   322,    77, 31432,   612,   322,    77,\n",
      "        18699,   203,    55,  1688,  2810,    30,  6578,    16,   225,    77,\n",
      "           82,   927, 44108,  4611,   346, 34177,    16, 13546,   311,   670,\n",
      "         2237,   322,    77,  2230,   225,    69,  2042,   287,  2218,  7528,\n",
      "        14951,    18,   581, 20636,   598,   345,  9019,   958,   301,  3305,\n",
      "          345,  8500,   266,   384,  3855,  1156,   743,    16, 10967,   301,\n",
      "         1676,   287,  4789, 11879,   462,   368,  2765,   322,    77,  3660,\n",
      "           18,     2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 0.2\n",
    "\n",
    "tokenized_dataset = DataFrame(tokenized_dataset.to_dict())\n",
    "print(f\"Users in dataset: {tokenized_dataset['user_ids'].nunique()}\")\n",
    "print(f\"Dataset size: {len(tokenized_dataset)}\\n\")\n",
    "\n",
    "# Split the dataset into train, test, and eval sets using user_ids grouped examples\n",
    "grouped = tokenized_dataset.groupby('user_ids')#.size().reset_index(name='counts')\n",
    "user_ids = list(grouped.groups.keys())\n",
    "user_ids = list(grouped.groups.keys())[:10]\n",
    "tokenized_dataset = tokenized_dataset[tokenized_dataset['user_ids'].isin(user_ids)]\n",
    "\n",
    "train_ids, test_ids = train_test_split(user_ids, test_size=TEST_SIZE, random_state=42)\n",
    "train_ids, eval_ids = train_test_split(train_ids, test_size=TEST_SIZE, random_state=42)\n",
    "tokenized_train_set = tokenized_dataset[tokenized_dataset['user_ids'].isin(train_ids)]\n",
    "tokenized_test_set = tokenized_dataset[tokenized_dataset['user_ids'].isin(test_ids)]\n",
    "tokenized_eval_set = tokenized_dataset[tokenized_dataset['user_ids'].isin(eval_ids)]\n",
    "\n",
    "# Prints how many users are in each set\n",
    "print(f\"Users in train set: {tokenized_train_set['user_ids'].nunique()}\")\n",
    "print(f\"Users in test set: {tokenized_test_set['user_ids'].nunique()}\")\n",
    "print(f\"Users in eval set: {tokenized_eval_set['user_ids'].nunique()}\\n\")\n",
    "\n",
    "# Remove the 'user_ids' column from the train, test, and eval sets\n",
    "tokenized_train_set = tokenized_train_set.drop(columns=['user_ids'])\n",
    "tokenized_test_set = tokenized_test_set.drop(columns=['user_ids'])\n",
    "tokenized_eval_set = tokenized_eval_set.drop(columns=['user_ids'])\n",
    "\n",
    "tokenized_train_set = tokenized_train_set.reset_index(drop=True)\n",
    "tokenized_test_set = tokenized_test_set.reset_index(drop=True)\n",
    "tokenized_eval_set = tokenized_eval_set.reset_index(drop=True)\n",
    "\n",
    "tokenized_train_set = Dataset.from_pandas(tokenized_train_set)\n",
    "tokenized_test_set = Dataset.from_pandas(tokenized_test_set)\n",
    "tokenized_eval_set = Dataset.from_pandas(tokenized_eval_set)\n",
    "\n",
    "print(f\"Train set size: {len(tokenized_train_set)}\")\n",
    "print(f\"Test set size: {len(tokenized_test_set)}\")\n",
    "print(f\"Eval set size: {len(tokenized_eval_set)}\\n\")\n",
    "\n",
    "# Set the format to PyTorch tensors\n",
    "tokenized_train_set.set_format(\"torch\")\n",
    "tokenized_test_set.set_format(\"torch\")\n",
    "tokenized_eval_set.set_format(\"torch\")\n",
    "\n",
    "print_dataset_info(tokenized_train_set)\n",
    "\n",
    "data_collator = MultiTaskBartDataCollator(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_train_set,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    # Parallelize data loading so the GPU\n",
    "    # doesn't have to wait for the CPU\n",
    "    # to prepare the next batch.\n",
    "    num_workers=4\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_eval_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb38819",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc16aa",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a40bdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_general_learning_curve(log_history, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    train_losses = log_history['total_train_losses']\n",
    "    eval_losses = log_history['total_eval_losses']\n",
    "    epochs = log_history['epochs']\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(epochs, train_losses, 'b-o', label='Training Losses')\n",
    "    plt.plot(epochs, eval_losses, 'r-o', label='Validation Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.title('Learning Curve Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(out_dir, \"general_learning_curve.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_reg_learning_curve(log_history, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    reg_train_losses = log_history['reg_train_losses']\n",
    "    reg_eval_losses = log_history['reg_eval_losses']\n",
    "    epochs = log_history['epochs']\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(epochs, reg_train_losses, 'g-o', label='Regression Training Losses')\n",
    "    plt.plot(epochs, reg_eval_losses, 'm-o', label='Regression Validation Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.title('Regression Learning Curve Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(out_dir, \"reg_learning_curve.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_gen_learning_curve(log_history, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    gen_train_losses = log_history['gen_train_losses']\n",
    "    gen_eval_losses = log_history['gen_eval_losses']\n",
    "    epochs = log_history['epochs']\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(epochs, gen_train_losses, 'c-o', label='Generation Training Losses')\n",
    "    plt.plot(epochs, gen_eval_losses, 'y-o', label='Generation Validation Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.title('Generation Learning Curve Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(out_dir, \"gen_learning_curve.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_all_learning_curves(log_history, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    epochs = log_history['epochs']\n",
    "    \n",
    "    # Plot total train losses\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(epochs, log_history['total_train_losses'], 'b-o', label='Total Train Losses')\n",
    "    plt.plot(epochs, log_history['total_eval_losses'], 'r-o', label='Total Eval Losses')\n",
    "    plt.plot(epochs, log_history['reg_train_losses'], 'g-o', label='Regression Train Losses')\n",
    "    plt.plot(epochs, log_history['reg_eval_losses'], 'm-o', label='Regression Eval Losses')\n",
    "    plt.plot(epochs, log_history['gen_train_losses'], 'c-o', label='Generation Train Losses')\n",
    "    plt.plot(epochs, log_history['gen_eval_losses'], 'y-o', label='Generation Eval Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.title('Total Train Losses Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(out_dir, \"all_learning_curves.png\"))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec3c6ad",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b85b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODE == \"train_no_optuna\":\n",
    "    criterion = EuclideanLoss(\n",
    "        regression_loss_fn=torch.nn.MSELoss(),\n",
    "        # regression_loss_fn=torch.nn.SmoothL1Loss(),\n",
    "    ).to(DEVICE)\n",
    "    # criterion = StaticWeightedLoss(\n",
    "    #     # regression_loss_fn=torch.nn.MSELoss(),\n",
    "    #     regression_loss_fn=torch.nn.SmoothL1Loss(),\n",
    "    #     alpha=0.5\n",
    "    # ).to(DEVICE)\n",
    "    # criterion = UncertaintyLoss(\n",
    "    #     # regression_loss_fn=torch.nn.MSELoss(),\n",
    "    #     regression_loss_fn=torch.nn.SmoothL1Loss()\n",
    "    # ).to(DEVICE)\n",
    "\n",
    "    model = BartWithRegression().to(DEVICE)\n",
    "    # model = torch.compile(model)\n",
    "\n",
    "    suffix = f\"-{criterion.__class__.__name__}-{criterion.regression_loss_fn.__class__.__name__}\"\n",
    "    save_path = OUT_DIR + suffix\n",
    "    results_path = os.path.join(save_path, \"results\")\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        criterion=criterion,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        # get_scheduler_fn=get_linear_schedule_with_warmup,\n",
    "        warmup_percentage=WARMUP_PERCENTAGE,\n",
    "        body_lr=BODY_LR, head_lr=HEAD_LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "        logging=True,\n",
    "        save_path=save_path\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, args=args, device=DEVICE,\n",
    "        train_dataloader=train_dataloader,\n",
    "        eval_dataloader=eval_dataloader\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    plot_general_learning_curve(\n",
    "        trainer.log_history,\n",
    "        results_path\n",
    "    )\n",
    "    plot_reg_learning_curve(\n",
    "        trainer.log_history,\n",
    "        results_path\n",
    "    )\n",
    "    plot_gen_learning_curve(\n",
    "        trainer.log_history,\n",
    "        results_path\n",
    "    )\n",
    "\n",
    "    del model, trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe4a3cd",
   "metadata": {},
   "source": [
    "## Train with Optuna Hyperparameters Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODE == \"train_with_optuna\":\n",
    "    temp_save_path = OUT_DIR + \"-temp\"\n",
    "    save_path = OUT_DIR\n",
    "    results_path = os.path.join(save_path, \"results\")\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "    best_log_history = None\n",
    "\n",
    "    losses = {\n",
    "        \"euclidean-mse\": EuclideanLoss(\n",
    "            regression_loss_fn=torch.nn.MSELoss(),\n",
    "        ).to(DEVICE),\n",
    "        \"euclidean-smoothl1\": EuclideanLoss(\n",
    "            regression_loss_fn=torch.nn.SmoothL1Loss(),\n",
    "        ).to(DEVICE),\n",
    "        \"uncertainty-mse\": UncertaintyLoss(\n",
    "            regression_loss_fn=torch.nn.MSELoss(),\n",
    "        ).to(DEVICE),\n",
    "        \"uncertainty-smoothl1\": UncertaintyLoss(\n",
    "            regression_loss_fn=torch.nn.SmoothL1Loss(),\n",
    "        ).to(DEVICE),\n",
    "    }\n",
    "\n",
    "# --- Define the Objective Function for Optuna ---\n",
    "def objective(trial: optuna.Trial):\n",
    "    \"\"\"\n",
    "    An objective function to be maximized or minimized by Optuna.\n",
    "    This function takes a `trial` object, sets up a model with hyperparameters suggested\n",
    "    by the trial, trains it, and returns the performance metric to optimize (eval_loss).\n",
    "    \"\"\"\n",
    "\n",
    "    # BODY_LR = 3e-5\n",
    "    # HEAD_LR = 1.5e-4\n",
    "    criterion = trial.suggest_categorical(\"criterion\", list(losses.keys()))\n",
    "    head_lr = trial.suggest_float(\"head_lr\", 1e-5, 1e-3, log=True)\n",
    "    body_lr = trial.suggest_float(\"body_lr\", 1e-5, 1e-4, log=True)\n",
    "    # batch_size = trial.suggest_categorical(\"batch_size\", [4, 8])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-3, 0.1, log=True)\n",
    "    # label_smoothing = trial.suggest_float(\"label_smoothing\", 0.0, 0.2)\n",
    "    \n",
    "    # print(f\"Trial {trial.number} - Hyperparameters:\")\n",
    "    # print(f\"  Criterion: {criterion}\")\n",
    "    # print(f\"  Head LR: {head_lr:.6f}\")\n",
    "    # print(f\"  Body LR: {body_lr:.6f}\")\n",
    "    # print(f\"  Weight Decay: {weight_decay:.6f}\")\n",
    "\n",
    "    criterion = losses[criterion]\n",
    "\n",
    "    try:\n",
    "        model = BartWithRegression().to(DEVICE)\n",
    "        # model = torch.compile(model)\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            criterion=criterion,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            # get_scheduler_fn=get_linear_schedule_with_warmup,\n",
    "            warmup_percentage=WARMUP_PERCENTAGE,\n",
    "            body_lr=body_lr, head_lr=head_lr,\n",
    "            weight_decay=weight_decay,\n",
    "            early_stopping_patience=None,\n",
    "            logging=True,\n",
    "            save_path=temp_save_path\n",
    "        )\n",
    "\n",
    "        current_trainer = Trainer(\n",
    "            model=model, args=args, device=DEVICE,\n",
    "            train_dataloader=train_dataloader,\n",
    "            eval_dataloader=eval_dataloader\n",
    "        )\n",
    "        current_trainer.train()\n",
    "\n",
    "        best_eval_loss = min(current_trainer.log_history['total_eval_losses'])\n",
    "\n",
    "        # Check if this is the best trial so far\n",
    "        if trial.number == 0 or best_eval_loss > trial.study.best_value:\n",
    "            shutil.rmtree(save_path)\n",
    "            os.rename(temp_save_path, save_path)\n",
    "            plot_general_learning_curve(\n",
    "                current_trainer.log_history,\n",
    "                results_path\n",
    "            )\n",
    "            plot_reg_learning_curve(\n",
    "                current_trainer.log_history,\n",
    "                results_path\n",
    "            )\n",
    "            plot_gen_learning_curve(\n",
    "                current_trainer.log_history,\n",
    "                results_path\n",
    "            )\n",
    "            # print(f\"New best model saved with score: {best_eval_loss:.4f}\")\n",
    "        \n",
    "        # --- Clean up GPU memory before the next trial ---\n",
    "        del model, current_trainer, args\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Return the best validation loss achieved during this trial\n",
    "        return best_eval_loss\n",
    "\n",
    "    except optuna.exceptions.TrialPruned as e:\n",
    "        # --- Handle pruned trials ---\n",
    "        # Clean up memory for pruned trials as well\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        # --- Handle other errors like CUDA OOM ---\n",
    "        print(\"Trial failed with error:\")\n",
    "        traceback.print_exc()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        # Return a very high loss value so Optuna knows this trial was bad\n",
    "        return float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac20b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODE == \"train_with_optuna\":\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Create a study object. `direction=\"minimize\"` means Optuna will try to minimize the return value of `objective`.\n",
    "    # The TPE sampler is the algorithm for Bayesian Optimization.\n",
    "    # The MedianPruner is an aggressive early stopping algorithm.\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(),\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=PRUNER_WARMUP_STEPS) # Prune after the 1st epoch\n",
    "    )\n",
    "\n",
    "    # Start the optimization. Optuna will call the `objective` function `n_trials` times.\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=OPTUNA_TRAIN_TRIALS,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    # --- Print the results ---\n",
    "    print(\"Study statistics: \")\n",
    "    print(f\"  Number of finished trials: {len(study.trials)}\")\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"  Value (min eval loss): {best_trial.value}\")\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545aea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODE == \"train_with_optuna\":\n",
    "    results_path = os.path.join(save_path, \"results\")\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "    # You can also save the results to a file\n",
    "    df = study.trials_dataframe()\n",
    "    path = os.path.join(results_path, \"optuna_train_hyperparams_results.csv\")\n",
    "    df.to_csv(path)\n",
    "    print(f\"\\nStudy results saved to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e103a",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26df032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = BartWithRegression(os.path.join(save_path, \"epoch-\" + str(NUM_EPOCHS))).to(DEVICE)\n",
    "loaded_model = BartWithRegression(\n",
    "    \".\\\\out\\\\2025-07-21_09-40-43-UncertaintyLoss-SmoothL1Loss\\\\epoch-9\",\n",
    "    verbose=True\n",
    ").to(DEVICE)\n",
    "trainer = Trainer(\n",
    "    model=loaded_model, args=None, device=DEVICE,\n",
    "    train_dataloader=None,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    test_dataloader=test_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddf21e9",
   "metadata": {},
   "source": [
    "## Generation Hyperparameters Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d5982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_search_grid = {\n",
    "#     'num_beams': [3, 5, 7],\n",
    "#     'repetition_penalty': [1.0, 1.2, 1.5],\n",
    "#     'do_sample': [False] # Fix this to false for beam search\n",
    "# }\n",
    "\n",
    "# sampling_grid = {\n",
    "#     'do_sample': [True], # Fix this to true for sampling\n",
    "#     'top_k': [40, 50],\n",
    "#     'top_p': [0.92, 0.95],\n",
    "#     'temperature': [0.8, 0.85, 0.9, 0.95]\n",
    "# }\n",
    "\n",
    "# METRIC = 'sbert_similarity'\n",
    "\n",
    "# keys, values = zip(*sampling_grid.items())\n",
    "# hyperparameter_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "# best_score = -float('inf')\n",
    "# best_hyperparams = None\n",
    "\n",
    "# search_progress_bar = tqdm(\n",
    "#     hyperparameter_combinations,\n",
    "#     desc=\"Generation Hyperparameters Grid Search\"\n",
    "# )\n",
    "# for params in search_progress_bar:  \n",
    "#     scores = trainer.evaluate(eval_dataloader, leave=False, **params)\n",
    "\n",
    "#     current_score = scores[METRIC]\n",
    "#     if current_score > best_score:\n",
    "#         best_score = current_score\n",
    "#         best_hyperparams = params\n",
    "\n",
    "#     search_progress_bar.set_postfix({\n",
    "#         'current_score': f'{current_score:.4f}',\n",
    "#         'best_score': f'{best_score:.4f}'\n",
    "#     })\n",
    "\n",
    "# print(f\"Best hyperparameters: {best_hyperparams}\")\n",
    "# print(f\"Best score: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd439bc",
   "metadata": {},
   "source": [
    "## Generation Hyperparameters Search with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1f361e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_generation(trial: optuna.Trial):\n",
    "    decoding_strategy = trial.suggest_categorical(\"strategy\", [\"beam_search\", \"sampling\"])\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": 1024,\n",
    "        \"repetition_penalty\": trial.suggest_float(\"repetition_penalty\", 1.0, 1.3)\n",
    "    }\n",
    "\n",
    "    if decoding_strategy == \"beam_search\":\n",
    "        gen_kwargs[\"num_beams\"] = trial.suggest_int(\"num_beams\", 2, 8)\n",
    "        gen_kwargs[\"early_stopping\"] = True # Usually a good idea with beam search\n",
    "    \n",
    "    elif decoding_strategy == \"sampling\":\n",
    "        gen_kwargs[\"do_sample\"] = True\n",
    "        gen_kwargs[\"top_p\"] = trial.suggest_float(\"top_p\", 0.85, 0.98)\n",
    "        gen_kwargs[\"top_k\"] = trial.suggest_int(\"top_k\", 20, 100) # Optional, often top_p is enough\n",
    "        gen_kwargs[\"temperature\"] = trial.suggest_float(\"temperature\", 0.7, 1.0)\n",
    "\n",
    "    print(f\"--- Trial {trial.number}: Testing with {gen_kwargs} ---\")\n",
    "\n",
    "    # --- Run Evaluation ---\n",
    "    # This is fast because it's only inference\n",
    "    results = trainer.evaluate(eval_dataloader, leave=False, **gen_kwargs)\n",
    "\n",
    "    # --- Return the Metric to Maximize ---\n",
    "    # We want to maximize a semantic score. Let's choose BERTScore F1.\n",
    "    metric_to_optimize = 'sbert_similarity'\n",
    "    score = results[metric_to_optimize]\n",
    "    print(f\"Trial {trial.number} Result -> {metric_to_optimize}: {score:.4f}\")\n",
    "    \n",
    "    # Clean up memory just in case, although less critical for inference\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return metric_to_optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b9e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skipping generation hyperparameter search. Using default settings.\n",
      "Using default hyperparameters: {'max_length': 1024, 'do_sample': True, 'top_p': 0.9, 'top_k': 100, 'temperature': 0.8}\n"
     ]
    }
   ],
   "source": [
    "if TEST_MODE == \"test_with_optuna\":\n",
    "    # For this search, we want to MAXIMIZE the score.\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=optuna.samplers.TPESampler(),\n",
    "        # Pruning is not needed here because each trial is just one full evaluation,\n",
    "        # there are no intermediate steps to prune.\n",
    "    )\n",
    "\n",
    "    # Run the optimization. 20-30 trials should be plenty for this.\n",
    "    study.optimize(\n",
    "        objective_generation,\n",
    "        n_trials=OPTUNA_TEST_TRIALS,\n",
    "        show_progress_bar=True,\n",
    "        # n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # --- Print the results ---\n",
    "    print(\"\\n\\n--- Generation Hyperparameter Search Complete ---\")\n",
    "    print(f\"  Number of finished trials: {len(study.trials)}\")\n",
    "\n",
    "    print(\"Best trial for generation:\")\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"    Best Score: {best_trial.value:.4f}\")\n",
    "\n",
    "    print(\"  Best Generation Params: \")\n",
    "    best_hyperparams = best_trial.params\n",
    "    for key, value in best_hyperparams.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "    # Save the results\n",
    "    df = study.trials_dataframe()\n",
    "    path = os.path.join(results_path, \"optuna_gen_hyperparams_results.csv\")\n",
    "    df.to_csv(path)\n",
    "    print(f\"\\nGeneration study results saved to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e54ac",
   "metadata": {},
   "source": [
    "## Default Generation Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76f513c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODE == \"test_no_optuna\":\n",
    "    best_hyperparams = {\n",
    "        \"max_length\": 1024,\n",
    "        # \"num_beams\": 3,\n",
    "        # # \"repetition_penalty\": 1.8,\n",
    "        # \"early_stopping\": True,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.95,\n",
    "        # Top-k sampling is a simple generalization of greedy decoding. Instead of choosing\n",
    "        # the single most probable word to generate, we first truncate the distribution to the\n",
    "        # top k most likely words, renormalize to produce a legitimate probability distribution,\n",
    "        # and then randomly sample from within these k words according to their renormalized\n",
    "        # probabilities.\n",
    "        \"top_k\": 20,\n",
    "        \"temperature\": 0.6 # 0.6\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70ab6fa",
   "metadata": {},
   "source": [
    "## Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b448872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "\n",
      "Topolino:\n",
      "Sei la ragazza pi√π bella del mondo. Spero che staremo per sempre insieme![TARGET][SEP]\n",
      "\n",
      "Polarities: tensor([0.5712], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Explanations:\n",
      "Tag: Accettazione e Annuncio di Chiusura\n",
      "Spiegazione: Topolino conclude la conversazione con una dichiarazione di affetto e speranza, sperando di mantenere un legame.\n"
     ]
    }
   ],
   "source": [
    "def inference_example(input_text):\n",
    "    \"\"\"\n",
    "    Run inference on a single input text and print the results.\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(input_text, return_tensors='pt', truncation=True, max_length=1024)\n",
    "    tokenized = {k: v.to(DEVICE) for k, v in tokenized.items()}\n",
    "\n",
    "    output = loaded_model.generate(\n",
    "        input_ids=tokenized['input_ids'][0].unsqueeze(0),\n",
    "        attention_mask=tokenized['attention_mask'][0].unsqueeze(0),\n",
    "        **best_hyperparams\n",
    "    )\n",
    "\n",
    "    print(f\"Input:\\n{input_text}\")\n",
    "    print(f\"Polarities: {output['polarities']}\")\n",
    "    print(f\"Explanations:\\n{tokenizer.decode(output['explanations'][0], skip_special_tokens=True)}\")\n",
    "\n",
    "chat = '''\n",
    "Topolino:\n",
    "Sei la ragazza pi√π bella del mondo. Spero che staremo per sempre insieme![TARGET][SEP]\n",
    "'''\n",
    "inference_example(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "822a9471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message:\n",
      "<s>Alessandro Conti:\n",
      "Clara, devo comunicarti una cosa importante. Gli ultimi giorni sono stati estremamente intensi per il nuovo progetto su via del Corso. Richiede la mia totale concentrazione e purtroppo non mi sta lasciando molto spazio mentale per altro. [TARGET] [SEP]\n",
      " Clara Neri:\n",
      "Oh, capisco, Ale. Immginavo fosse cos√¨, eri teso ultimamente. Mi dispiace tanto che tu sia sotto cos√¨ tanta pressione. E non ti preoccupare per me, davvero. La tua serenit√† e la tua concentrazione sul lavoro sono la cosa pi√π importante adesso. Prendi tutto lo spazio di cui hai bisogno. Sono qui per te. ‚ù§Ô∏è [SEP]\n",
      " Alessandro Conti:\n",
      "Apprezzo molto la tua comprensione, Clara. Non √® una questione personale, ma una necessit√† logistica per gestire al meglio questa fase critica. Mi fa piacere sapere che sei di supporto. [SEP]\n",
      " Clara Neri:\n",
      "Assolutamente s√¨, Ale. L'ho capito benissimo. La tua dedizione al lavoro √® una delle cose che ammiro di pi√π in te. Se hai bisogno di qualcosa di pratico, anche solo che ti porti del cibo o ti aiuti con qualche commissione per farti guadagnare tempo, fammi sapere. Davvero, conta su di me! üòä Non sei solo in questo. [SEP]\n",
      " Alessandro Conti:\n",
      "Grazie, Clara. √à rassicurante saperlo. Ti terr√≤ aggiornata quando la situazione si allenter√†. [SEP]\n",
      " </s>\n",
      "True Polarity:0.5\n",
      "True Explanation:\n",
      "Tag: Comunicazione di Bisogno di Spazio\n",
      "Spiegazione: Alessandro, in modo inaspettatamente diretto ma rispettoso, comunica il suo bisogno di spazio a causa del forte stress lavorativo. La polarit√† √® positiva perch√© la comunicazione √® chiara e non accusatoria, sebbene la natura del messaggio implichi una richiesta di distanza.\n",
      "Generated Polarity:0.6953279376029968\n",
      "Generated Explanation:\n",
      "Tag: Supporto e Conferma\n",
      "Spiegazione: Alessandro accoglie la richiesta di Clara con gratitudine e conferma il suo supporto, rafforzando il legame e il senso di connessione. Questo messaggio rafforza il legame e rafforza la natura di supporto reciproco, mantenendo la polarit√† positiva.\n",
      "\n",
      "\n",
      "\n",
      "Message:\n",
      "<s>Alessandro Conti:\n",
      "Clara, devo comunicarti una cosa importante. Gli ultimi giorni sono stati estremamente intensi per il nuovo progetto su via del Corso. Richiede la mia totale concentrazione e purtroppo non mi sta lasciando molto spazio mentale per altro. [SEP]\n",
      " Clara Neri:\n",
      "Oh, capisco, Ale. Immginavo fosse cos√¨, eri teso ultimamente. Mi dispiace tanto che tu sia sotto cos√¨ tanta pressione. E non ti preoccupare per me, davvero. La tua serenit√† e la tua concentrazione sul lavoro sono la cosa pi√π importante adesso. Prendi tutto lo spazio di cui hai bisogno. Sono qui per te. ‚ù§Ô∏è [TARGET] [SEP]\n",
      " Alessandro Conti:\n",
      "Apprezzo molto la tua comprensione, Clara. Non √® una questione personale, ma una necessit√† logistica per gestire al meglio questa fase critica. Mi fa piacere sapere che sei di supporto. [SEP]\n",
      " Clara Neri:\n",
      "Assolutamente s√¨, Ale. L'ho capito benissimo. La tua dedizione al lavoro √® una delle cose che ammiro di pi√π in te. Se hai bisogno di qualcosa di pratico, anche solo che ti porti del cibo o ti aiuti con qualche commissione per farti guadagnare tempo, fammi sapere. Davvero, conta su di me! üòä Non sei solo in questo. [SEP]\n",
      " Alessandro Conti:\n",
      "Grazie, Clara. √à rassicurante saperlo. Ti terr√≤ aggiornata quando la situazione si allenter√†. [SEP]\n",
      " </s>\n",
      "True Polarity:0.949999988079071\n",
      "True Explanation:\n",
      "Tag: Empatia e Supporto Incondizionato\n",
      "Spiegazione: Clara risponde con profonda empatia e comprensione, mettendo da parte la sua ansia per mostrare supporto incondizionato. Offre rassicurazione e validazione, rafforzando il legame anche nel momento del bisogno di distanza. L'uso dell'emoji del cuore sottolinea l'affetto e il supporto emotivo.\n",
      "Generated Polarity:0.9072540998458862\n",
      "Generated Explanation:\n",
      "Tag: Supporto e Conferma dell'Empatia\n",
      "Spiegazione: Clara esprime un profondo senso di gratitudine e conferma la sua gratitudine, rafforzando il legame e la fiducia. La sua dichiarazione finale di affetto e gratitudine rafforza la polarit√† positiva, rafforzando il legame e la fiducia reciproca.\n",
      "\n",
      "\n",
      "\n",
      "Message:\n",
      "<s>Alessandro Conti:\n",
      "Clara, devo comunicarti una cosa importante. Gli ultimi giorni sono stati estremamente intensi per il nuovo progetto su via del Corso. Richiede la mia totale concentrazione e purtroppo non mi sta lasciando molto spazio mentale per altro. [SEP]\n",
      " Clara Neri:\n",
      "Oh, capisco, Ale. Immginavo fosse cos√¨, eri teso ultimamente. Mi dispiace tanto che tu sia sotto cos√¨ tanta pressione. E non ti preoccupare per me, davvero. La tua serenit√† e la tua concentrazione sul lavoro sono la cosa pi√π importante adesso. Prendi tutto lo spazio di cui hai bisogno. Sono qui per te. ‚ù§Ô∏è [SEP]\n",
      " Alessandro Conti:\n",
      "Apprezzo molto la tua comprensione, Clara. Non √® una questione personale, ma una necessit√† logistica per gestire al meglio questa fase critica. Mi fa piacere sapere che sei di supporto. [TARGET] [SEP]\n",
      " Clara Neri:\n",
      "Assolutamente s√¨, Ale. L'ho capito benissimo. La tua dedizione al lavoro √® una delle cose che ammiro di pi√π in te. Se hai bisogno di qualcosa di pratico, anche solo che ti porti del cibo o ti aiuti con qualche commissione per farti guadagnare tempo, fammi sapere. Davvero, conta su di me! üòä Non sei solo in questo. [SEP]\n",
      " Alessandro Conti:\n",
      "Grazie, Clara. √à rassicurante saperlo. Ti terr√≤ aggiornata quando la situazione si allenter√†. [SEP]\n",
      " </s>\n",
      "True Polarity:0.75\n",
      "True Explanation:\n",
      "Tag: Riconoscimento e Chiarimento\n",
      "Spiegazione: Alessandro esprime apprezzamento per la comprensione di Clara, chiarendo che il suo bisogno di spazio non √® un rifiuto personale. Questo rinforza la base di rispetto nella conversazione e mantiene un tono positivo, pur sottolineando la sua preferenza per l'approccio logistico ai problemi.\n",
      "Generated Polarity:0.8460918664932251\n",
      "Generated Explanation:\n",
      "Tag: Supporto e Conferma\n",
      "Spiegazione: Alessandro risponde con gratitudine e conferma, mostrando apertura e disponibilit√†. Il tono √® affettuoso e rassicurante, confermando il supporto di Clara e rafforzando il legame.\n",
      "\n",
      "\n",
      "\n",
      "Message:\n",
      "<s>Alessandro Conti:\n",
      "Clara, devo comunicarti una cosa importante. Gli ultimi giorni sono stati estremamente intensi per il nuovo progetto su via del Corso. Richiede la mia totale concentrazione e purtroppo non mi sta lasciando molto spazio mentale per altro. [SEP]\n",
      " Clara Neri:\n",
      "Oh, capisco, Ale. Immginavo fosse cos√¨, eri teso ultimamente. Mi dispiace tanto che tu sia sotto cos√¨ tanta pressione. E non ti preoccupare per me, davvero. La tua serenit√† e la tua concentrazione sul lavoro sono la cosa pi√π importante adesso. Prendi tutto lo spazio di cui hai bisogno. Sono qui per te. ‚ù§Ô∏è [SEP]\n",
      " Alessandro Conti:\n",
      "Apprezzo molto la tua comprensione, Clara. Non √® una questione personale, ma una necessit√† logistica per gestire al meglio questa fase critica. Mi fa piacere sapere che sei di supporto. [SEP]\n",
      " Clara Neri:\n",
      "Assolutamente s√¨, Ale. L'ho capito benissimo. La tua dedizione al lavoro √® una delle cose che ammiro di pi√π in te. Se hai bisogno di qualcosa di pratico, anche solo che ti porti del cibo o ti aiuti con qualche commissione per farti guadagnare tempo, fammi sapere. Davvero, conta su di me! üòä Non sei solo in questo. [TARGET] [SEP]\n",
      " Alessandro Conti:\n",
      "Grazie, Clara. √à rassicurante saperlo. Ti terr√≤ aggiornata quando la situazione si allenter√†. [SEP]\n",
      " </s>\n",
      "True Polarity:1.0\n",
      "True Explanation:\n",
      "Tag: Offerta di Aiuto e Riassicurazione\n",
      "Spiegazione: Clara eleva ulteriormente la polarit√† della conversazione offrendo aiuto concreto e riaffermando il suo sostegno. La sua risposta mostra una profonda generosit√† e la volont√† di essere presente in modi che rispettino il bisogno di spazio di Alessandro, rafforzando il concetto di supporto reciproco anche nella distanza.\n",
      "Generated Polarity:0.8373677730560303\n",
      "Generated Explanation:\n",
      "Tag: Accettazione Riluttante e Affermazione di Spazio\n",
      "Spiegazione: Clara accetta la richiesta di Alessandro con grande sollievo e gratitudine, ma con un tono di rassegnazione e una leggera riserva di spazio per s√©. La polarit√† √® leggermente positiva per il suo supporto e la sua accettazione della situazione, anche se non √® ancora pienamente percepibile.\n",
      "\n",
      "\n",
      "\n",
      "Message:\n",
      "<s>Alessandro Conti:\n",
      "Clara, devo comunicarti una cosa importante. Gli ultimi giorni sono stati estremamente intensi per il nuovo progetto su via del Corso. Richiede la mia totale concentrazione e purtroppo non mi sta lasciando molto spazio mentale per altro. [SEP]\n",
      " Clara Neri:\n",
      "Oh, capisco, Ale. Immginavo fosse cos√¨, eri teso ultimamente. Mi dispiace tanto che tu sia sotto cos√¨ tanta pressione. E non ti preoccupare per me, davvero. La tua serenit√† e la tua concentrazione sul lavoro sono la cosa pi√π importante adesso. Prendi tutto lo spazio di cui hai bisogno. Sono qui per te. ‚ù§Ô∏è [SEP]\n",
      " Alessandro Conti:\n",
      "Apprezzo molto la tua comprensione, Clara. Non √® una questione personale, ma una necessit√† logistica per gestire al meglio questa fase critica. Mi fa piacere sapere che sei di supporto. [SEP]\n",
      " Clara Neri:\n",
      "Assolutamente s√¨, Ale. L'ho capito benissimo. La tua dedizione al lavoro √® una delle cose che ammiro di pi√π in te. Se hai bisogno di qualcosa di pratico, anche solo che ti porti del cibo o ti aiuti con qualche commissione per farti guadagnare tempo, fammi sapere. Davvero, conta su di me! üòä Non sei solo in questo. [SEP]\n",
      " Alessandro Conti:\n",
      "Grazie, Clara. √à rassicurante saperlo. Ti terr√≤ aggiornata quando la situazione si allenter√†. [TARGET] [SEP]\n",
      " </s>\n",
      "True Polarity:0.8999999761581421\n",
      "True Explanation:\n",
      "Tag: Gratitudine e Accettazione del Supporto\n",
      "Spiegazione: Alessandro conclude il breve scambio con un'espressione di gratitudine. La sua disponibilit√† a \"tenere aggiornata\" Clara, seppur breve, indica una rara apertura e apprezzamento per il suo supporto, chiudendo la conversazione su una nota di fiducia e reciproco rispetto, in linea con l'alta polarit√† generale e la sana gestione dei confini.\n",
      "Generated Polarity:0.8572942018508911\n",
      "Generated Explanation:\n",
      "Tag: Supporto e Riconoscimento del Supporto\n",
      "Spiegazione: Alessandro accoglie positivamente l'invito di Clara, mostrando un approccio collaborativo e un apprezzamento per il supporto di Clara. Il messaggio riflette il suo desiderio di mantenere la relazione, ma anche la sua alta gradevolezza e il desiderio di mantenere una connessione.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_model.eval()\n",
    "for example in tokenized_train_set.select(range(5)):\n",
    "    output = loaded_model.generate(\n",
    "        input_ids=example['input_ids'].unsqueeze(0).to(DEVICE),\n",
    "        attention_mask=example['attention_mask'].unsqueeze(0).to(DEVICE),\n",
    "        **best_hyperparams\n",
    "    )\n",
    "    # print(f\"Output: {output['explanations'][0]}\")\n",
    "    decoded_output = tokenizer.decode(output['explanations'][0], skip_special_tokens=True)\n",
    "    decoded_chat = tokenizer.decode(example['input_ids']) # , skip_special_tokens=True\n",
    "    decoded_chat = decoded_chat.replace(SEP_TOKEN, SEP_TOKEN + \"\\n\")\n",
    "    decoded_true_explanation = tokenizer.decode(example['labels'], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Message:\\n{decoded_chat}\")\n",
    "    print(f\"True Polarity:{example['polarities']}\")\n",
    "    print(f\"True Explanation:\\n{decoded_true_explanation}\")\n",
    "    print(f\"Generated Polarity:{output['polarities'].item()}\")\n",
    "    print(f\"Generated Explanation:\\n{decoded_output}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d49cdcf",
   "metadata": {},
   "source": [
    "## Evaluation of the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fd87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(**best_hyperparams)\n",
    "with open(os.path.join(results_path, \"test_results.txt\"), \"w\") as f:\n",
    "    for key, value in results.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "        f.write(f\"{key}: {value:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
