{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02653c6f",
   "metadata": {},
   "source": [
    "# üì¶ Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41f6cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T15:22:08.617410Z",
     "iopub.status.busy": "2025-09-06T15:22:08.616886Z",
     "iopub.status.idle": "2025-09-06T15:22:37.548268Z",
     "shell.execute_reply": "2025-09-06T15:22:37.547721Z",
     "shell.execute_reply.started": "2025-09-06T15:22:08.617364Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import scipy.stats as stats\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import shutil\n",
    "import emoji\n",
    "import re\n",
    "import os\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BertForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4792a16",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a84bca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T15:22:37.550278Z",
     "iopub.status.busy": "2025-09-06T15:22:37.549679Z",
     "iopub.status.idle": "2025-09-06T15:22:37.555102Z",
     "shell.execute_reply": "2025-09-06T15:22:37.554131Z",
     "shell.execute_reply.started": "2025-09-06T15:22:37.550259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"/kaggle\"):\n",
    "    # Clean up the entire /kaggle/working directory\n",
    "    shutil.rmtree(\"/kaggle/working\", ignore_errors=True)\n",
    "    os.makedirs(\"/kaggle/working\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed5d9b",
   "metadata": {},
   "source": [
    "## Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be0afc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T15:22:37.556878Z",
     "iopub.status.busy": "2025-09-06T15:22:37.556521Z",
     "iopub.status.idle": "2025-09-06T15:22:37.577148Z",
     "shell.execute_reply": "2025-09-06T15:22:37.576408Z",
     "shell.execute_reply.started": "2025-09-06T15:22:37.556848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATASET_TYPE = \"toxicity\"\n",
    "\n",
    "MODEL_CHECKPOINT = \"dbmdz/bert-base-italian-cased\"\n",
    "WITH_USER_TYPES = True\n",
    "\n",
    "MULTICLASS_TARGET_NAMES = ['Toxic', 'Neutral', 'Healthy']\n",
    "BINARY_TARGET_NAMES = ['Toxic', 'Healthy']\n",
    "\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "SAVE_TOTAL_LIMIT = 1\n",
    "N_FOLDS = 5\n",
    "TEST_SIZE = 0.2\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "EARLY_STOPPING_PATIENCE = 4\n",
    "# EARLY_STOPPING_THRESHOLD = 0.0005\n",
    "\n",
    "LR_SCHEDULER_KWARGS = {\n",
    "    \"factor\": 0.5,        # Riduce il learning rate del 50% quando non migliora\n",
    "    \"patience\": 2,\n",
    "    # \"threshold\": EARLY_STOPPING_THRESHOLD,\n",
    "    \"mode\": \"max\"\n",
    "}\n",
    "\n",
    "WARMUP_PERCENTAGE = 0.1\n",
    "WEIGHT_DECAY = 0.01 # int the 0 to 0.1 range\n",
    "BODY_LR = 3e-5\n",
    "\n",
    "COST_MAT = np.array([\n",
    "    [0, 8, 16],\n",
    "    [8, 0, 1],\n",
    "    [16, 4, 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4e3c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T15:22:37.578493Z",
     "iopub.status.busy": "2025-09-06T15:22:37.577962Z",
     "iopub.status.idle": "2025-09-06T15:22:37.592899Z",
     "shell.execute_reply": "2025-09-06T15:22:37.592123Z",
     "shell.execute_reply.started": "2025-09-06T15:22:37.578452Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # The following two lines are for deterministic results on CUDA.\n",
    "        # They can have a performance impact.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd1663e",
   "metadata": {},
   "source": [
    "## Paths Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7571b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T15:22:37.595165Z",
     "iopub.status.busy": "2025-09-06T15:22:37.594954Z",
     "iopub.status.idle": "2025-09-06T15:22:37.601076Z",
     "shell.execute_reply": "2025-09-06T15:22:37.600356Z",
     "shell.execute_reply.started": "2025-09-06T15:22:37.595148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "MODEL_NAME = MODEL_CHECKPOINT.replace('/', '-')\n",
    "\n",
    "if os.path.exists(\"/kaggle\"):\n",
    "    # ==== KAGGLE SETTINGS ====\n",
    "    PATH = os.path.join(os.sep, \"kaggle\", \"input\", f\"cipv-chats-{DATASET_TYPE}\", f\"cipv-chats-multiclass-{DATASET_TYPE}.parquet\")\n",
    "    OUT_DIR = os.path.join(os.sep, \"kaggle\", \"working\", f\"{timestamp}-{MODEL_NAME}\")\n",
    "else:\n",
    "    # ==== LOCAL SETTINGS ====\n",
    "    PATH = os.path.join(\".\", \"out\", \"datasets\", f\"cipv-chats-multiclass-{DATASET_TYPE}.parquet\")\n",
    "    OUT_DIR = os.path.join(\".\", 'out', 'models', DATASET_TYPE, 'messages-regression-explanation', f'{timestamp}-{MODEL_NAME}-user_types_{WITH_USER_TYPES}')\n",
    "\n",
    "RESULTS_PATH = os.path.join(OUT_DIR, \"results\")\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3460e1f9",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15e08c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T15:22:37.601902Z",
     "iopub.status.busy": "2025-09-06T15:22:37.601698Z",
     "iopub.status.idle": "2025-09-06T15:22:37.620081Z",
     "shell.execute_reply": "2025-09-06T15:22:37.619432Z",
     "shell.execute_reply.started": "2025-09-06T15:22:37.601886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Use the same color palette as confusion matrix\n",
    "colors = sns.color_palette(\"ch:s=-.2,r=.6\", n_colors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f95671",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T15:22:37.620921Z",
     "iopub.status.busy": "2025-09-06T15:22:37.620716Z",
     "iopub.status.idle": "2025-09-06T15:22:37.634435Z",
     "shell.execute_reply": "2025-09-06T15:22:37.633732Z",
     "shell.execute_reply.started": "2025-09-06T15:22:37.620898Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_aggregated_curves(log_histories, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    train_losses_by_epoch = defaultdict(list)\n",
    "    eval_losses_by_epoch = defaultdict(list)\n",
    "    eval_costs_by_epoch = defaultdict(list)\n",
    "\n",
    "    for history in log_histories:\n",
    "        for log in history:\n",
    "            epoch = log.get('epoch')\n",
    "            if epoch is None:\n",
    "                continue\n",
    "\n",
    "            # Round epoch to handle potential float values like 1.0, 2.0\n",
    "            epoch = int(round(epoch))\n",
    "            \n",
    "            if 'loss' in log:\n",
    "                train_losses_by_epoch[epoch].append(log['loss'])\n",
    "            if 'eval_loss' in log:\n",
    "                eval_losses_by_epoch[epoch].append(log['eval_loss'])\n",
    "            if 'eval_cost' in log:\n",
    "                eval_costs_by_epoch[epoch].append(log['eval_cost'])\n",
    "\n",
    "    # --- Plotting Aggregated Loss Curve ---\n",
    "    epochs = sorted(eval_losses_by_epoch.keys())\n",
    "    \n",
    "    mean_train_loss = [np.mean(train_losses_by_epoch[e]) for e in epochs if e in train_losses_by_epoch]\n",
    "    std_train_loss = [np.std(train_losses_by_epoch[e]) for e in epochs if e in train_losses_by_epoch]\n",
    "    \n",
    "    mean_eval_loss = [np.mean(eval_losses_by_epoch[e]) for e in epochs]\n",
    "    std_eval_loss = [np.std(eval_losses_by_epoch[e]) for e in epochs]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, mean_eval_loss, 'c-o', label='Mean Eval Loss')\n",
    "    plt.fill_between(\n",
    "        epochs,\n",
    "        np.array(mean_eval_loss) - np.array(std_eval_loss),\n",
    "        np.array(mean_eval_loss) + np.array(std_eval_loss),\n",
    "        color='c', alpha=0.2\n",
    "    )\n",
    "\n",
    "    # Ensure train epochs align with eval epochs for plotting\n",
    "    train_epochs_for_plot = [e for e in epochs if e in train_losses_by_epoch]\n",
    "    plt.plot(train_epochs_for_plot, mean_train_loss, 'g-o', label='Mean Train Loss')\n",
    "    plt.fill_between(\n",
    "        train_epochs_for_plot,\n",
    "        np.array(mean_train_loss) - np.array(std_train_loss),\n",
    "        np.array(mean_train_loss) + np.array(std_train_loss),\n",
    "        color='g', alpha=0.2\n",
    "    )\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Aggregated Learning Curve (Mean ¬± Std)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"aggregated_learning_curve.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # --- Plotting Aggregated Cost Curve ---\n",
    "    epochs = sorted(eval_costs_by_epoch.keys())\n",
    "    mean_eval_cost = [np.mean(eval_costs_by_epoch[e]) for e in epochs]\n",
    "    std_eval_cost = [np.std(eval_costs_by_epoch[e]) for e in epochs]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, mean_eval_cost, 'r-o', label='Mean Eval Cost')\n",
    "    plt.fill_between(\n",
    "        epochs,\n",
    "        np.array(mean_eval_cost) - np.array(std_eval_cost),\n",
    "        np.array(mean_eval_cost) + np.array(std_eval_cost),\n",
    "        color='r', alpha=0.2\n",
    "    )\n",
    "\n",
    "    min_cost_epoch_idx = np.argmin(mean_eval_cost)\n",
    "    min_cost_epoch = epochs[min_cost_epoch_idx]\n",
    "    min_cost_value = mean_eval_cost[min_cost_epoch_idx]\n",
    "    \n",
    "    plt.axvline(\n",
    "        x=min_cost_epoch, color='green', linestyle='--', alpha=0.7,\n",
    "        label=f'Min Mean Cost at Epoch {min_cost_epoch}'\n",
    "    )\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Aggregated Evaluation Cost (Mean ¬± Std)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"aggregated_cost_curve.png\"))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62adebe2",
   "metadata": {},
   "source": [
    "# ü™Ñ Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "\n",
    "def get_formatted_msg(msg):\n",
    "    return emoji.demojize(f\"{msg['user']}:\\n{msg['content']}\", language='it')\n",
    "\n",
    "def preprocess(messages):\n",
    "    all_unique_names = list(set(msg['user'] for msg in messages))\n",
    "    samples = []\n",
    "\n",
    "    for target_idx, target_msg in enumerate(messages):\n",
    "        tokenized_input = {\n",
    "            'input_ids': [cls_token_id],\n",
    "            'attention_mask': [1],\n",
    "            'token_type_ids': [1],\n",
    "            'user_type_ids': [all_unique_names.index(messages[target_idx]['user'])],\n",
    "        }\n",
    "        for idx, msg in enumerate(messages):\n",
    "            if idx == target_idx:\n",
    "                tokenized_msg = tokenizer(\n",
    "                    \"[SEP]\" + get_formatted_msg(messages[target_idx]) + \"[SEP]\\n\",\n",
    "                    add_special_tokens=False\n",
    "                )\n",
    "                tokenized_msg['token_type_ids'] = [1] * len(tokenized_msg['input_ids'])\n",
    "            else:\n",
    "                tokenized_msg = tokenizer(\n",
    "                    get_formatted_msg(messages[idx]) + \"\\n\",\n",
    "                    add_special_tokens=False\n",
    "                )\n",
    "                tokenized_msg['token_type_ids'] = [0] * len(tokenized_msg['input_ids'])\n",
    "            tokenized_input['input_ids'] += tokenized_msg['input_ids']\n",
    "            tokenized_input['attention_mask'] += tokenized_msg['attention_mask']\n",
    "            tokenized_input['token_type_ids'] += tokenized_msg['token_type_ids']\n",
    "            if WITH_USER_TYPES:\n",
    "                tokenized_input['user_type_ids'] += [all_unique_names.index(msg['user'])] * len(tokenized_msg['input_ids'])\n",
    "\n",
    "        tokenized_input['value'] = float(target_msg['value'])\n",
    "        tokenized_input['user_id'] = all_unique_names.index(target_msg['user'])\n",
    "        samples.append(tokenized_input)\n",
    "\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb8e7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T15:22:37.662464Z",
     "iopub.status.busy": "2025-09-06T15:22:37.662181Z",
     "iopub.status.idle": "2025-09-06T15:22:39.845453Z",
     "shell.execute_reply": "2025-09-06T15:22:39.844543Z",
     "shell.execute_reply.started": "2025-09-06T15:22:37.662397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(PATH)\n",
    "print(df.info())\n",
    "df['messages'] = df['messages'].apply(lambda x: preprocess(x))\n",
    "df = df.explode('messages')\n",
    "df['input_ids'] = df['messages'].apply(lambda x: x['input_ids'])\n",
    "df['attention_mask'] = df['messages'].apply(lambda x: x['attention_mask'])\n",
    "df['token_type_ids'] = df['messages'].apply(lambda x: x['token_type_ids'])\n",
    "if WITH_USER_TYPES:\n",
    "    df['user_type_ids'] = df['messages'].apply(lambda x: x['user_type_ids'])\n",
    "df['labels'] = df['messages'].apply(lambda x: x['value'])\n",
    "df['user_ids'] = df['messages'].apply(lambda x: x['user_id'])\n",
    "\n",
    "# Drop the original messages column\n",
    "df = df.drop(columns=['messages'])\n",
    "df = df.reset_index() # drop=True\n",
    "df = df.rename(columns={'index': 'chat_ids'})\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1e206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T15:22:39.846954Z",
     "iopub.status.busy": "2025-09-06T15:22:39.846378Z",
     "iopub.status.idle": "2025-09-06T15:22:39.853997Z",
     "shell.execute_reply": "2025-09-06T15:22:39.852902Z",
     "shell.execute_reply.started": "2025-09-06T15:22:39.846931Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_dataset_info(dataset):\n",
    "    print(dataset)\n",
    "    # For each field, print the first entry\n",
    "    for field in dataset.features:\n",
    "        print(f\"{field}: {dataset[1][field]}\\n\")\n",
    "\n",
    "print_dataset_info(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df649e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the dataset to only 2 unique couples\n",
    "# unique_couples = list(set(tokenized_dataset['couple_ids']))\n",
    "# selected_couples = unique_couples[:5]  # Take first 2 couples\n",
    "# print(f\"Selected couples: {selected_couples}\")\n",
    "\n",
    "# # Filter dataset to only include samples from selected couples\n",
    "# filtered_indices = [i for i, couple_id in enumerate(tokenized_dataset['couple_ids']) \n",
    "#                    if couple_id in selected_couples]\n",
    "# tokenized_dataset = tokenized_dataset.select(filtered_indices)\n",
    "\n",
    "# print(f\"Dataset size after filtering to 2 couples: {len(tokenized_dataset)}\")\n",
    "# print_dataset_info(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84dbc79",
   "metadata": {},
   "source": [
    "# ü§ñ Fine-Tuning BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527b4a2d",
   "metadata": {},
   "source": [
    "## üìà‚Äã Cross-Validation Training-Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec904eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(scores, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Computes the confidence interval for a given performance metric.\n",
    "\n",
    "    This function is useful for understanding the reliability of a single model's \n",
    "    mean performance score from cross-validation.\n",
    "\n",
    "    Args:\n",
    "        scores (list or np.ndarray): A list of scores from cross-validation folds.\n",
    "        confidence_level (float): The desired confidence level (e.g., 0.95 for 95%).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the mean score, and the lower and upper bounds \n",
    "               of the confidence interval (mean, lower_bound, upper_bound).\n",
    "    \"\"\"\n",
    "    n = len(scores)\n",
    "    if n <= 1:\n",
    "        # Cannot compute CI for 1 or 0 scores, return mean and NaN for bounds\n",
    "        return (np.mean(scores), np.nan, np.nan)\n",
    "        \n",
    "    mean_score = np.mean(scores)\n",
    "    # Standard Error of the Mean (SEM) = Sn / sqrt(n)\n",
    "    # where Sn is the standard deviation of the scores\n",
    "    std_err = stats.sem(scores)\n",
    "    \n",
    "    # Degrees of freedom\n",
    "    dof = n - 1\n",
    "    \n",
    "    # Get the critical value from the t-distribution\n",
    "    t_critical = stats.t.ppf((1 + confidence_level) / 2., dof)\n",
    "    \n",
    "    margin_of_error = t_critical * std_err\n",
    "    \n",
    "    lower_bound = mean_score - margin_of_error\n",
    "    upper_bound = mean_score + margin_of_error\n",
    "    \n",
    "    return (mean_score, lower_bound, upper_bound)\n",
    "\n",
    "def print_and_save_classification_report_conf_intervals(cv_results, save_path, label_names, confidence=0.95, name=\"classification_report_with_cv.txt\"):\n",
    "    with open(os.path.join(save_path, name), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"=== Cross-Validation Results (Mean ¬± Std [{confidence * 100:.0f}% CI]) ===\\n\\n\")\n",
    "        print(f\"=== Cross-Validation Results (Mean ¬± Std [{confidence * 100:.0f}% CI]) ===\\n\")\n",
    "\n",
    "        # Create the classification report format\n",
    "        report_lines = []\n",
    "        \n",
    "        # Header\n",
    "        header = f\"{'':>14} {'precision':>27} {'recall':>27} {'f1-score':>27}\"\n",
    "        report_lines.append(header)\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        # Per-class metrics\n",
    "        for i, name in enumerate(label_names):\n",
    "            name_lower = name.lower()\n",
    "            \n",
    "            # Calculate confidence intervals for each metric\n",
    "            precision_scores = cv_results[f'test_precision_{name_lower}']\n",
    "            precision_mean, precision_lower, precision_upper = confidence_interval(precision_scores, confidence)\n",
    "            precision_std = np.std(precision_scores)\n",
    "\n",
    "            recall_scores = cv_results[f'test_recall_{name_lower}']\n",
    "            recall_mean, recall_lower, recall_upper = confidence_interval(recall_scores, confidence)\n",
    "            recall_std = np.std(recall_scores)\n",
    "\n",
    "            f1_scores = cv_results[f'test_f1_{name_lower}']\n",
    "            f1_mean, f1_lower, f1_upper = confidence_interval(f1_scores, confidence)\n",
    "            f1_std = np.std(f1_scores)\n",
    "\n",
    "            # Format with confidence intervals\n",
    "            precision_ci = f\"{precision_mean:.2f} ¬± {precision_std:.2f} [{precision_lower:.2f}, {precision_upper:.2f}]\"\n",
    "            recall_ci = f\"{recall_mean:.2f} ¬± {recall_std:.2f} [{recall_lower:.2f}, {recall_upper:.2f}]\"\n",
    "            f1_ci = f\"{f1_mean:.2f} ¬± {f1_std:.2f} [{f1_lower:.2f}, {f1_upper:.2f}]\"\n",
    "\n",
    "            line = f\"{name:>14} {precision_ci:>27} {recall_ci:>27} {f1_ci:>27}\"\n",
    "            report_lines.append(line)\n",
    "        \n",
    "        report_lines.append(\"\")\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy_scores = cv_results['test_accuracy']\n",
    "        accuracy_mean, accuracy_lower, accuracy_upper = confidence_interval(accuracy_scores, confidence)\n",
    "        accuracy_std = np.std(accuracy_scores)\n",
    "        accuracy_ci = f\"{accuracy_mean:.2f} ¬± {accuracy_std:.2f} [{accuracy_lower:.2f}, {accuracy_upper:.2f}]\"\n",
    "        line = f\"{'accuracy':>14} {'':>27} {'':>27} {accuracy_ci:>27}\"\n",
    "        report_lines.append(line)\n",
    "        \n",
    "        # Macro and weighted averages\n",
    "        for avg_type in ['macro', 'weighted']:\n",
    "            precision_scores = cv_results[f'test_precision_{avg_type}']\n",
    "            precision_mean, precision_lower, precision_upper = confidence_interval(precision_scores, confidence)\n",
    "            precision_std = np.std(precision_scores)\n",
    "\n",
    "            recall_scores = cv_results[f'test_recall_{avg_type}']\n",
    "            recall_mean, recall_lower, recall_upper = confidence_interval(recall_scores, confidence)\n",
    "            recall_std = np.std(recall_scores)\n",
    "\n",
    "            f1_scores = cv_results[f'test_f1_{avg_type}']\n",
    "            f1_mean, f1_lower, f1_upper = confidence_interval(f1_scores, confidence)\n",
    "            f1_std = np.std(f1_scores)\n",
    "\n",
    "            # Format with confidence intervals\n",
    "            precision_ci = f\"{precision_mean:.2f} ¬± {precision_std:.2f} [{precision_lower:.2f}, {precision_upper:.2f}]\"\n",
    "            recall_ci = f\"{recall_mean:.2f} ¬± {recall_std:.2f} [{recall_lower:.2f}, {recall_upper:.2f}]\"\n",
    "            f1_ci = f\"{f1_mean:.2f} ¬± {f1_std:.2f} [{f1_lower:.2f}, {f1_upper:.2f}]\"\n",
    "\n",
    "            line = f\"{avg_type + ' avg':>14} {precision_ci:>27} {recall_ci:>27} {f1_ci:>27}\"\n",
    "            report_lines.append(line)\n",
    "        \n",
    "        if 'cost' in cv_results:\n",
    "            cost_scores = cv_results['test_cost']\n",
    "            cost_mean, cost_lower, cost_upper = confidence_interval(cost_scores, confidence)\n",
    "            cost_std = np.std(cost_scores)\n",
    "            cost_ci = f\"{cost_mean:.2f} ¬± {cost_std:.2f} [{cost_lower:.2f}, {cost_upper:.2f}]\"\n",
    "            report_lines.append(\"\")\n",
    "            report_lines.append(f\"Total Cost: {cost_ci}\")\n",
    "        \n",
    "        # Write to file and print\n",
    "        report_text = \"\\n\".join(report_lines)\n",
    "        f.write(report_text)\n",
    "        print(report_text)\n",
    "\n",
    "def plot_confusion_matrices(cms, classes, path=None):\n",
    "    \"\"\"\n",
    "    Plots a single confusion matrix showing mean ¬± standard deviation for each cell.\n",
    "    \n",
    "    Args:\n",
    "        cv_results: Cross-validation results containing confusion_matrix scores\n",
    "        classes: List of class names\n",
    "        path: Path to save the plot\n",
    "    \"\"\"\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    # Calculate mean and std for each cell\n",
    "    cm_mean = np.mean(cms, axis=0)\n",
    "    cm_std = np.std(cms, axis=0)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    \n",
    "    # Create annotations with mean ¬± std format\n",
    "    annotations = np.empty_like(cm_mean, dtype=object)\n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_classes):\n",
    "            annotations[i, j] = f'{cm_mean[i, j]:.1f} ¬± {cm_std[i, j]:.2f}'\n",
    "    \n",
    "    sns.heatmap(\n",
    "        cm_mean, \n",
    "        annot=annotations, \n",
    "        fmt='', \n",
    "        cmap=sns.color_palette(\"ch:s=-.2,r=.6\", as_cmap=True),\n",
    "        xticklabels=classes, \n",
    "        yticklabels=classes,\n",
    "        cbar_kws={'label': 'Mean Count'}\n",
    "    )\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion Matrix (Mean ¬± Std)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if path:\n",
    "        plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a4247",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T15:22:46.963365Z",
     "iopub.status.busy": "2025-09-06T15:22:46.963159Z",
     "iopub.status.idle": "2025-09-06T15:22:46.996417Z",
     "shell.execute_reply": "2025-09-06T15:22:46.995690Z",
     "shell.execute_reply.started": "2025-09-06T15:22:46.963349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_total_cost(y_true, y_pred, cost_mat):\n",
    "    \"\"\"\n",
    "    Calculates the total cost of predictions using a cost matrix.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels  \n",
    "        cost_mat: Cost matrix where cost_mat[i,j] is the cost of \n",
    "                 predicting class j when true class is i\n",
    "    \n",
    "    Returns:\n",
    "        Total cost (scalar)\n",
    "    \"\"\"\n",
    "    # Generate labels that match cost matrix dimensions\n",
    "    # Assumes labels are 0, 1, 2, ..., num_classes-1\n",
    "    num_classes = cost_mat.shape[0]\n",
    "    labels = np.arange(num_classes)\n",
    "    \n",
    "    # Get confusion matrix with all possible labels\n",
    "    conf_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    # Calculate total cost\n",
    "    return np.sum(conf_mat * cost_mat)\n",
    "\n",
    "def multiclass_clf(values):\n",
    "    \"\"\"\n",
    "    Classify regression values into categories:\n",
    "    - toxic: [-1, -0.35]\n",
    "    - neutral: (-0.35, 0.35)\n",
    "    - healthy: [0.35, 1]\n",
    "    \"\"\"\n",
    "    classified = np.where(values <= -0.35, 0,  # toxic\n",
    "                    np.where(values < 0.35, 1,  # neutral\n",
    "                            2))  # healthy\n",
    "    return classified\n",
    "\n",
    "def binary_clf(values):\n",
    "    \"\"\"\n",
    "    Classify regression values into categories:\n",
    "    - toxic: [-1, -0.35)\n",
    "    - healthy: [-0.35, 1]\n",
    "    \"\"\"\n",
    "    classified = np.where(values < -0.35, 0, 1)\n",
    "    return classified\n",
    "\n",
    "def training_compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Flatten predictions and labels if they're multi-dimensional\n",
    "    predictions = predictions.flatten()\n",
    "    labels = labels.flatten()\n",
    "    \n",
    "    # Classify predictions and ground truth\n",
    "    pred_classes = multiclass_clf(predictions)\n",
    "    true_classes = multiclass_clf(labels)\n",
    "\n",
    "    cost = calculate_total_cost(true_classes, pred_classes, COST_MAT)\n",
    "\n",
    "    # Extract metrics for logging\n",
    "    metrics = {\n",
    "        'cost': cost\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65258e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T15:22:46.997301Z",
     "iopub.status.busy": "2025-09-06T15:22:46.997061Z",
     "iopub.status.idle": "2025-09-06T15:22:47.008209Z",
     "shell.execute_reply": "2025-09-06T15:22:47.007414Z",
     "shell.execute_reply.started": "2025-09-06T15:22:46.997261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_trainer(tokenized_train_set, tokenized_eval_set, out_dir):\n",
    "\n",
    "    if WITH_USER_TYPES:\n",
    "        data_collator = DataCollatorWithUserTypePadding(tokenizer=tokenizer)\n",
    "        config = BertConfig.from_pretrained(\n",
    "            MODEL_CHECKPOINT,\n",
    "            num_labels=1, # our task is regression (num_labels=1).\n",
    "        )\n",
    "        config.user_type_vocab_size = 2\n",
    "        model = BertWithUserTypeForSequenceClassification.from_pretrained(\n",
    "            MODEL_CHECKPOINT,\n",
    "            config=config\n",
    "        )\n",
    "    else:\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            MODEL_CHECKPOINT,\n",
    "            num_labels=1,\n",
    "        )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        \n",
    "        eval_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=SAVE_TOTAL_LIMIT,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        metric_for_best_model=\"cost\",\n",
    "        greater_is_better=False,\n",
    "        report_to=\"none\",\n",
    "\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=BODY_LR,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_ratio=WARMUP_PERCENTAGE,\n",
    "\n",
    "        # lr_scheduler_type=\"linear\",\n",
    "        lr_scheduler_type=\"reduce_lr_on_plateau\",\n",
    "        lr_scheduler_kwargs=LR_SCHEDULER_KWARGS,\n",
    "\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=NUM_WORKERS,\n",
    "        # remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_set, # .remove_columns([\"chat_ids\", \"couple_ids\"])\n",
    "        eval_dataset=tokenized_eval_set, # .remove_columns([\"chat_ids\", \"couple_ids\"])\n",
    "        # tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=training_compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(\n",
    "            early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "            # early_stopping_threshold=EARLY_STOPPING_THRESHOLD\n",
    "        )]\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e399a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T15:22:47.009892Z",
     "iopub.status.busy": "2025-09-06T15:22:47.009464Z",
     "iopub.status.idle": "2025-09-06T15:22:47.023419Z",
     "shell.execute_reply": "2025-09-06T15:22:47.022707Z",
     "shell.execute_reply.started": "2025-09-06T15:22:47.009874Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_per_message_report(predictions, labels, clf, target_names, cost_mat=None):\n",
    "    \n",
    "    # Flatten predictions and labels if they're multi-dimensional\n",
    "    # predictions = predictions.flatten()\n",
    "    # labels = labels.flatten()\n",
    "    \n",
    "    pred_classes = clf(predictions)\n",
    "    true_classes = clf(labels)\n",
    "\n",
    "    per_msg_report = classification_report(\n",
    "        true_classes, pred_classes, \n",
    "        target_names=target_names,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    if cost_mat is not None:\n",
    "        per_msg_report['cost'] = calculate_total_cost(true_classes, pred_classes, cost_mat)\n",
    "\n",
    "    conf_matrix = confusion_matrix(true_classes, pred_classes)\n",
    "\n",
    "    return per_msg_report, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38826be1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T15:22:47.024361Z",
     "iopub.status.busy": "2025-09-06T15:22:47.024116Z",
     "iopub.status.idle": "2025-09-06T15:22:47.039031Z",
     "shell.execute_reply": "2025-09-06T15:22:47.038232Z",
     "shell.execute_reply.started": "2025-09-06T15:22:47.024325Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def classify_chat(df, clf):\n",
    "    # Group by chat_ids and user_ids, then calculate the mean value for each group.\n",
    "    user_means = df.groupby(['chat_ids', 'user_ids'])['values'].mean()\n",
    "    # For each chat, find the minimum of the user mean values.\n",
    "    chat_mins = user_means.groupby('chat_ids').min()\n",
    "    # The results are already sorted by chat_ids, so we can convert to a numpy array.\n",
    "    classified = chat_mins.to_numpy()\n",
    "    return clf(classified)\n",
    "\n",
    "def compute_per_chat_report(df, clf, target_names, cost_mat=None):    \n",
    "    # Classify predictions and ground truth\n",
    "    pred_classes = classify_chat(df.drop(columns=['labels']).rename(columns={'predictions': 'values'}), clf)\n",
    "    true_classes = classify_chat(df.drop(columns=['predictions']).rename(columns={'labels': 'values'}), clf)\n",
    "\n",
    "    chats_report = classification_report(\n",
    "        true_classes, \n",
    "        pred_classes, \n",
    "        target_names=target_names,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    if cost_mat is not None:\n",
    "        chats_report['cost'] = calculate_total_cost(true_classes, pred_classes, cost_mat)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(true_classes, pred_classes)\n",
    "\n",
    "    return chats_report, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec1c25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T15:22:47.040167Z",
     "iopub.status.busy": "2025-09-06T15:22:47.039903Z",
     "iopub.status.idle": "2025-09-06T15:22:47.055597Z",
     "shell.execute_reply": "2025-09-06T15:22:47.054945Z",
     "shell.execute_reply.started": "2025-09-06T15:22:47.040145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def report_dict_to_dataframe_dict(report, prefix=\"\"):\n",
    "    df_dict = {}\n",
    "    for key, value in report.items():\n",
    "        if isinstance(value, dict):\n",
    "            key = key.split(' ')[0]\n",
    "            for sub_key, sub_value in value.items():\n",
    "                df_dict[f\"{prefix}{sub_key}_{key}\".lower().replace('-score', '')] = sub_value\n",
    "        else:\n",
    "            df_dict[f\"{prefix}{key}\".lower().replace('-score', '')] = value\n",
    "    return df_dict\n",
    "\n",
    "def compute_clf_metrics(predictions, fold_test_dataset, clf, target_names, prefix=\"\", cost_mat=None):\n",
    "    per_msg_report, per_msg_conf_matrix = compute_per_message_report(\n",
    "        predictions.predictions,\n",
    "        fold_test_dataset['labels'],\n",
    "        clf,\n",
    "        target_names,\n",
    "        cost_mat=cost_mat\n",
    "    )\n",
    "    per_chat_report, per_chat_conf_matrix = compute_per_chat_report(\n",
    "        pd.DataFrame({\n",
    "            'chat_ids': fold_test_dataset['chat_ids'],\n",
    "            'user_ids': fold_test_dataset['user_ids'],\n",
    "            'labels': fold_test_dataset['labels'],\n",
    "            'predictions': predictions.predictions.flatten()\n",
    "        }),\n",
    "        clf,\n",
    "        target_names,\n",
    "        cost_mat=cost_mat\n",
    "    )\n",
    "\n",
    "    metrics = report_dict_to_dataframe_dict(per_msg_report, prefix=f\"{prefix}per_msg_\")\n",
    "    metrics.update(report_dict_to_dataframe_dict(per_chat_report, prefix=f\"{prefix}per_chat_\"))\n",
    "    metrics[f'{prefix}per_msg_conf_matrix'] = per_msg_conf_matrix\n",
    "    metrics[f'{prefix}per_chat_conf_matrix'] = per_chat_conf_matrix\n",
    "    return metrics\n",
    "\n",
    "def evaluate(predictions, fold_test_dataset):\n",
    "\n",
    "    metrics = compute_clf_metrics(\n",
    "        predictions, \n",
    "        fold_test_dataset, \n",
    "        multiclass_clf, \n",
    "        MULTICLASS_TARGET_NAMES, \n",
    "        \"multiclass_\", \n",
    "        cost_mat=COST_MAT\n",
    "    )\n",
    "    metrics.update(compute_clf_metrics(\n",
    "        predictions, \n",
    "        fold_test_dataset, \n",
    "        binary_clf, \n",
    "        BINARY_TARGET_NAMES, \n",
    "        \"binary_\"\n",
    "    ))\n",
    "\n",
    "    labels_np = fold_test_dataset['labels'].cpu().numpy()  # Convert tensor to numpy\n",
    "\n",
    "    # compute mse, mae, rmse and correlation coefficient between predictions and labels\n",
    "    metrics['mse'] = np.mean((predictions.predictions.flatten() - labels_np) ** 2)\n",
    "    metrics['mae'] = np.mean(np.abs(predictions.predictions.flatten() - labels_np))\n",
    "    metrics['rmse'] = np.sqrt(metrics['mse'])\n",
    "    metrics['corr_coef'] = np.corrcoef(predictions.predictions.flatten(), labels_np)[0, 1]\n",
    "\n",
    "    baseline_clf = np.mean(labels_np)\n",
    "\n",
    "    # compute relative squared error (RSE) and relative absolute error (RAE)\n",
    "    metrics['rse'] = np.sum((predictions.predictions.flatten() - labels_np) ** 2) / np.sum((labels_np - baseline_clf) ** 2)\n",
    "    metrics['rae'] = np.sum(np.abs(predictions.predictions.flatten() - labels_np)) / np.sum(np.abs(labels_np - baseline_clf))\n",
    "    metrics['rrmse'] = np.sqrt(metrics['rse'])\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def do_cross_validation(outer_cv, tokenized_dataset):\n",
    "\n",
    "    eval_metrics = []\n",
    "    all_log_histories = []\n",
    "\n",
    "    df = tokenized_dataset.to_pandas()\n",
    "    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(\n",
    "        X=df[['input_ids', 'token_type_ids', 'attention_mask']],\n",
    "        y=df['labels'],\n",
    "        groups=df['couple_ids']\n",
    "    )):\n",
    "        print(f\"Starting fold {fold + 1}/{outer_cv.get_n_splits()}\")\n",
    "\n",
    "        # Subset the datasets for the current fold\n",
    "        fold_train_dataset = tokenized_dataset.select(train_idx)\n",
    "        fold_test_dataset = tokenized_dataset.select(test_idx)\n",
    "\n",
    "        # Further split fold_train_dataset into training and validation sets\n",
    "        gss_val = GroupShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=GLOBAL_SEED)\n",
    "        train_idx, eval_idx = next(gss_val.split(\n",
    "            X=fold_train_dataset,\n",
    "            y=fold_train_dataset['labels'],\n",
    "            groups=fold_train_dataset['couple_ids']\n",
    "        ))\n",
    "        fold_eval_dataset = fold_train_dataset.select(eval_idx)\n",
    "        fold_train_dataset = fold_train_dataset.select(train_idx)\n",
    "\n",
    "        # remove all couple_ids columns\n",
    "        fold_train_dataset = fold_train_dataset.remove_columns(['couple_ids', \"chat_ids\", \"user_ids\"])\n",
    "        fold_eval_dataset = fold_eval_dataset.remove_columns(['couple_ids', \"chat_ids\", \"user_ids\"])\n",
    "        fold_test_dataset = fold_test_dataset.remove_columns(['couple_ids'])\n",
    "\n",
    "        # Set the format to PyTorch tensors\n",
    "        fold_train_dataset.set_format(\"torch\")\n",
    "        fold_eval_dataset.set_format(\"torch\")\n",
    "        fold_test_dataset.set_format(\"torch\")\n",
    "\n",
    "        # Define a unique output directory for this fold\n",
    "        fold_output_dir = os.path.join(OUT_DIR, f\"fold_{fold+1}\")\n",
    "        trainer = get_trainer(fold_train_dataset, fold_eval_dataset, fold_output_dir)\n",
    "        trainer.train()\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        predictions = trainer.predict(fold_test_dataset.remove_columns([\"chat_ids\", \"user_ids\"]))\n",
    "        eval_metrics.append(evaluate(predictions, fold_test_dataset))\n",
    "\n",
    "        all_log_histories.append(trainer.state.log_history)\n",
    "\n",
    "        print(f\"Cleaning up checkpoint directory: {fold_output_dir}\")\n",
    "        shutil.rmtree(fold_output_dir, ignore_errors=True)\n",
    "    \n",
    "    return pd.DataFrame(eval_metrics), all_log_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e2fa01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T15:22:47.056537Z",
     "iopub.status.busy": "2025-09-06T15:22:47.056259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "outer_cv = GroupKFold(n_splits=N_FOLDS) # , shuffle=True, random_state=GLOBAL_SEED\n",
    "cv_results, all_log_histories = do_cross_validation(outer_cv, tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957d6ed",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def render_cv_results(cv_results, target_names, out_dir):\n",
    "    cv_results.drop(columns=['per_msg_conf_matrix', 'per_chat_conf_matrix']).to_csv(os.path.join(out_dir, \"cv_results.csv\"), index=False)\n",
    "    per_msg_cv_results = cv_results.filter(like=\"per_msg_\")\n",
    "    per_msg_cv_results.columns = [re.sub(r'^per_msg_', 'test_', col) for col in per_msg_cv_results.columns]\n",
    "    print_and_save_classification_report_conf_intervals(\n",
    "        per_msg_cv_results.drop(columns=['test_conf_matrix']),\n",
    "        out_dir, \n",
    "        target_names, \n",
    "        name=\"per_message_classification_report_with_cv.txt\"\n",
    "    )\n",
    "\n",
    "    per_chat_cv_results = cv_results.filter(like=\"per_chat_\")\n",
    "    per_chat_cv_results.columns = [re.sub(r'^per_chat_', 'test_', col) for col in per_chat_cv_results.columns]\n",
    "    print_and_save_classification_report_conf_intervals(\n",
    "        per_chat_cv_results.drop(columns=['test_conf_matrix']),\n",
    "        out_dir, \n",
    "        target_names, \n",
    "        name=\"per_chat_classification_report_with_cv.txt\"\n",
    "    )\n",
    "    plot_confusion_matrices(\n",
    "        per_msg_cv_results['test_conf_matrix'].tolist(),\n",
    "        target_names,\n",
    "        path=os.path.join(out_dir, \"per_msg_confusion_matrix_cv.png\")\n",
    "    )\n",
    "    plot_confusion_matrices(\n",
    "        per_chat_cv_results['test_conf_matrix'].tolist(),\n",
    "        target_names,\n",
    "        path=os.path.join(out_dir, \"per_chat_confusion_matrix_cv.png\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f952156",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "multiclass_cv_results = cv_results.filter(like='multiclass')\n",
    "multiclass_cv_results.columns = [re.sub(r'^multiclass_', '', col) for col in multiclass_cv_results.columns]\n",
    "os.makedirs(os.path.join(RESULTS_PATH, 'multiclass'), exist_ok=True)\n",
    "render_cv_results(\n",
    "    multiclass_cv_results,\n",
    "    MULTICLASS_TARGET_NAMES, \n",
    "    os.path.join(RESULTS_PATH, 'multiclass')\n",
    ")\n",
    "\n",
    "binary_cv_results = cv_results.filter(like='binary')\n",
    "binary_cv_results.columns = [re.sub(r'^binary_', '', col) for col in binary_cv_results.columns]\n",
    "os.makedirs(os.path.join(RESULTS_PATH, 'binary'), exist_ok=True)\n",
    "render_cv_results(\n",
    "    binary_cv_results,\n",
    "    BINARY_TARGET_NAMES,\n",
    "    os.path.join(RESULTS_PATH, 'binary')\n",
    ")\n",
    "\n",
    "plot_aggregated_curves(all_log_histories, RESULTS_PATH)\n",
    "\n",
    "with open(os.path.join(RESULTS_PATH, \"regression_metrics.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== Regression Metrics ===\\n\\n\")\n",
    "    print(\"=== Regression Metrics ===\\n\")\n",
    "    for metric in ['mse', 'mae', 'rmse', 'corr_coef', 'rse', 'rae', 'rrmse']:\n",
    "        mean_val, lower, upper = confidence_interval(cv_results[metric], confidence_level=0.95)\n",
    "        std_val = cv_results[metric].std()\n",
    "        f.write(f\"{metric}: {mean_val:.4f} ¬± {std_val:.4f} [{lower:.4f}, {upper:.4f}]\\n\")\n",
    "        print(f\"{metric}: {mean_val:.4f} ¬± {std_val:.4f} [{lower:.4f}, {upper:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087affd6",
   "metadata": {},
   "source": [
    "## ü§ñ Production BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e217ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_nepochs(all_log_histories):\n",
    "    \n",
    "    eval_costs_by_epoch = defaultdict(list)\n",
    "    for history in all_log_histories:\n",
    "        for log in history:\n",
    "            if 'eval_cost' in log and log.get('epoch') is not None:\n",
    "                epoch = int(round(log['epoch']))\n",
    "                eval_costs_by_epoch[epoch].append(log['eval_cost'])\n",
    "        \n",
    "    epochs = sorted(eval_costs_by_epoch.keys())\n",
    "    mean_eval_cost = [np.mean(eval_costs_by_epoch[e]) for e in epochs]\n",
    "    optimal_epochs = epochs[np.argmin(mean_eval_cost)]\n",
    "    \n",
    "    print(f\"Optimal number of epochs (min cost): {optimal_epochs}\")\n",
    "    print(f\"Mean cost at optimal epoch: {min(mean_eval_cost):.4f}\")\n",
    "    \n",
    "    return optimal_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ab22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Determine Optimal Number of Epochs\n",
    "optimal_epochs = choose_best_nepochs(all_log_histories)\n",
    "\n",
    "# 2. Prepare Full Dataset\n",
    "final_train_dataset = tokenized_dataset.remove_columns(['couple_ids', \"chat_ids\", \"user_ids\"])\n",
    "final_train_dataset.set_format(\"torch\")\n",
    "\n",
    "# 3. Configure Trainer for Final Run\n",
    "FINAL_MODEL_DIR = os.path.join(OUT_DIR, \"final_production_model\")\n",
    "os.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "if WITH_USER_TYPES:\n",
    "    data_collator = DataCollatorWithUserTypePadding(tokenizer=tokenizer)\n",
    "    config = BertConfig.from_pretrained(\n",
    "        MODEL_CHECKPOINT,\n",
    "        num_labels=1, # our task is regression (num_labels=1).\n",
    "    )\n",
    "    config.user_type_vocab_size = 2\n",
    "    final_model = BertWithUserTypeForSequenceClassification.from_pretrained(\n",
    "        MODEL_CHECKPOINT,\n",
    "        config=config\n",
    "    )\n",
    "else:\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    final_model = BertForSequenceClassification.from_pretrained(\n",
    "        MODEL_CHECKPOINT,\n",
    "        num_labels=1,\n",
    "    )\n",
    "\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=FINAL_MODEL_DIR,\n",
    "    num_train_epochs=optimal_epochs, # Train for the optimal number of epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=BODY_LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_PERCENTAGE,\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"no\", # We will save manually at the end\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=NUM_WORKERS,\n",
    "    seed=GLOBAL_SEED,\n",
    "    data_seed=GLOBAL_SEED\n",
    ")\n",
    "\n",
    "final_trainer = Trainer(\n",
    "    model=final_model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=final_train_dataset,\n",
    "    # No eval_dataset needed for the final run\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "final_trainer.train()\n",
    "\n",
    "final_trainer.save_model(FINAL_MODEL_DIR)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44561a76",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"/kaggle\"):\n",
    "    shutil.make_archive(OUT_DIR, 'zip', OUT_DIR)\n",
    "    # shutil.rmtree(OUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8202448,
     "sourceId": 12960502,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
