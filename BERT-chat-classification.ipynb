{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59bae597",
   "metadata": {},
   "source": [
    "# üì¶ Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8a1bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "from scipy import stats\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7dc38",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b7ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/kaggle\"):\n",
    "    # Clean up the entire /kaggle/working directory\n",
    "    shutil.rmtree(\"/kaggle/working\", ignore_errors=True)\n",
    "    os.makedirs(\"/kaggle/working\", exist_ok=True)\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a6514",
   "metadata": {},
   "source": [
    "## Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1481b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_BERT_MODEL = 'dbmdz/bert-base-italian-cased' #110M\n",
    "# SELECTED_BERT_MODEL = 'FacebookAI/xlm-roberta-base' # 278M\n",
    "# SELECTED_BERT_MODEL = 'Musixmatch/umberto-commoncrawl-cased-v1' #110M\n",
    "DATASET_TYPE = 'toxicity'\n",
    "\n",
    "TASK = 'multiclass'  # 'binary' or 'multiclass'\n",
    "\n",
    "if TASK == 'binary':\n",
    "    POLARITY_BINS = [-1.01, -0.35, 1.01]\n",
    "    POLARITY_LABELS = [0, 1]  # 0: Toxic, 1: Healthy\n",
    "    TARGET_NAMES = ['Toxic', 'Healthy']\n",
    "    NUM_LABELS = 2\n",
    "else:\n",
    "    POLARITY_BINS = [-1.01, -0.35, 0.35, 1.01]\n",
    "    POLARITY_LABELS = [0, 1, 2]\n",
    "    TARGET_NAMES = ['Toxic', 'Neutral', 'Healthy']\n",
    "    NUM_LABELS = 3\n",
    "    cost_mat = np.array([\n",
    "        [0, 8, 16],\n",
    "        [8, 0, 1],\n",
    "        [16, 4, 0]\n",
    "    ])\n",
    "\n",
    "WITH_SEP_TOKENS = False\n",
    "WITH_TOKEN_TYPE_IDS = True\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WARMUP_PERCENTAGE = 0.1\n",
    "NUM_WORKERS = 0\n",
    "SAVE_TOTAL_LIMIT = 2\n",
    "EARLY_STOPPING_PATIENCE = 4\n",
    "# EARLY_STOPPING_THRESHOLD = 0.0005\n",
    "\n",
    "LR_SCHEDULER_KWARGS = {\n",
    "    \"factor\": 0.5,        # Riduce il learning rate del 50% quando non migliora\n",
    "    \"patience\": 2,\n",
    "    # \"threshold\": EARLY_STOPPING_THRESHOLD,\n",
    "    \"mode\": \"max\"\n",
    "}\n",
    "\n",
    "LEARNING_RATE = 3e-5\n",
    "WEIGHT_DECAY = 0.001\n",
    "MAX_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc8ffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # The following two lines are for deterministic results on CUDA.\n",
    "        # They can have a performance impact.\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dfdf19",
   "metadata": {},
   "source": [
    "## Paths Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb95e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "MODEL_NAME = SELECTED_BERT_MODEL.replace('/', '-')\n",
    "\n",
    "if os.path.exists(\"/kaggle\"):\n",
    "    # ==== KAGGLE SETTINGS ====\n",
    "    PATH = os.path.join(os.sep, \"kaggle\", \"input\", f\"cipv-chats-{DATASET_TYPE}\", f\"cipv-chats-{TASK}-{DATASET_TYPE}.parquet\")\n",
    "    OUT_DIR = os.path.join(os.sep, \"kaggle\", \"working\", f\"{timestamp}-{MODEL_NAME}-Sep_{WITH_SEP_TOKENS}-Type_{WITH_TOKEN_TYPE_IDS}\")\n",
    "    NESTED_CV_RESULTS_PATH = os.path.join(OUT_DIR, \"nested-cv-results\", timestamp)\n",
    "else:\n",
    "    # ==== LOCAL SETTINGS ====\n",
    "    PATH = os.path.join(\".\", \"out\", \"datasets\", f\"cipv-chats-{TASK}-{DATASET_TYPE}.parquet\")\n",
    "    OUT_DIR = os.path.join(\".\", 'out', 'models', DATASET_TYPE, f'entire-chat-{TASK}-classification', f'{timestamp}-{MODEL_NAME}-Sep_{WITH_SEP_TOKENS}-Type_{WITH_TOKEN_TYPE_IDS}')\n",
    "    NESTED_CV_RESULTS_PATH = os.path.join(\".\", \"out\", \"models\", DATASET_TYPE, f\"entire-chat-{TASK}-classification\", \"nested-cv-results\", timestamp)\n",
    "\n",
    "RESULTS_PATH = os.path.join(OUT_DIR, \"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28835025",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c96800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aggregated_curves(log_histories, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    train_losses_by_epoch = defaultdict(list)\n",
    "    eval_losses_by_epoch = defaultdict(list)\n",
    "    eval_costs_by_epoch = defaultdict(list)\n",
    "\n",
    "    for history in log_histories:\n",
    "        for log in history:\n",
    "            epoch = log.get('epoch')\n",
    "            if epoch is None:\n",
    "                continue\n",
    "\n",
    "            # Round epoch to handle potential float values like 1.0, 2.0\n",
    "            epoch = int(round(epoch))\n",
    "            \n",
    "            if 'loss' in log:\n",
    "                train_losses_by_epoch[epoch].append(log['loss'])\n",
    "            if 'eval_loss' in log:\n",
    "                eval_losses_by_epoch[epoch].append(log['eval_loss'])\n",
    "            if 'eval_cost' in log:\n",
    "                eval_costs_by_epoch[epoch].append(log['eval_cost'])\n",
    "\n",
    "    # --- Plotting Aggregated Loss Curve ---\n",
    "    epochs = sorted(eval_losses_by_epoch.keys())\n",
    "    \n",
    "    mean_train_loss = [np.mean(train_losses_by_epoch[e]) for e in epochs if e in train_losses_by_epoch]\n",
    "    std_train_loss = [np.std(train_losses_by_epoch[e]) for e in epochs if e in train_losses_by_epoch]\n",
    "    \n",
    "    mean_eval_loss = [np.mean(eval_losses_by_epoch[e]) for e in epochs]\n",
    "    std_eval_loss = [np.std(eval_losses_by_epoch[e]) for e in epochs]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, mean_eval_loss, 'c-o', label='Mean Eval Loss')\n",
    "    plt.fill_between(\n",
    "        epochs,\n",
    "        np.array(mean_eval_loss) - np.array(std_eval_loss),\n",
    "        np.array(mean_eval_loss) + np.array(std_eval_loss),\n",
    "        color='c', alpha=0.2\n",
    "    )\n",
    "\n",
    "    # Ensure train epochs align with eval epochs for plotting\n",
    "    train_epochs_for_plot = [e for e in epochs if e in train_losses_by_epoch]\n",
    "    plt.plot(train_epochs_for_plot, mean_train_loss, 'g-o', label='Mean Train Loss')\n",
    "    plt.fill_between(\n",
    "        train_epochs_for_plot,\n",
    "        np.array(mean_train_loss) - np.array(std_train_loss),\n",
    "        np.array(mean_train_loss) + np.array(std_train_loss),\n",
    "        color='g', alpha=0.2\n",
    "    )\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Aggregated Learning Curve (Mean ¬± Std)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"aggregated_learning_curve.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # --- Plotting Aggregated Cost Curve ---\n",
    "    if TASK == 'multiclass':\n",
    "        epochs = sorted(eval_costs_by_epoch.keys())\n",
    "        mean_eval_cost = [np.mean(eval_costs_by_epoch[e]) for e in epochs]\n",
    "        std_eval_cost = [np.std(eval_costs_by_epoch[e]) for e in epochs]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, mean_eval_cost, 'r-o', label='Mean Eval Cost')\n",
    "        plt.fill_between(\n",
    "            epochs,\n",
    "            np.array(mean_eval_cost) - np.array(std_eval_cost),\n",
    "            np.array(mean_eval_cost) + np.array(std_eval_cost),\n",
    "            color='r', alpha=0.2\n",
    "        )\n",
    "\n",
    "        min_cost_epoch_idx = np.argmin(mean_eval_cost)\n",
    "        min_cost_epoch = epochs[min_cost_epoch_idx]\n",
    "        min_cost_value = mean_eval_cost[min_cost_epoch_idx]\n",
    "        \n",
    "        plt.axvline(\n",
    "            x=min_cost_epoch, color='green', linestyle='--', alpha=0.7,\n",
    "            label=f'Min Mean Cost at Epoch {min_cost_epoch}'\n",
    "        )\n",
    "\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.title('Aggregated Evaluation Cost (Mean ¬± Std)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(out_dir, \"aggregated_cost_curve.png\"))\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b6021d",
   "metadata": {},
   "source": [
    "# üìÇ Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68731692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formatted_msg(msg):\n",
    "    return emoji.demojize(f\"{msg['user']}:\\n{msg['content']}\", language='it')\n",
    "\n",
    "def preprocess(messages):\n",
    "    all_users = [msg['user'] for msg in messages]\n",
    "    if WITH_TOKEN_TYPE_IDS:\n",
    "        user_ids = list(set(all_users))\n",
    "        all_messages = [ \n",
    "            f\"[{user_ids.index(all_users[i])}]\"\n",
    "            + get_formatted_msg(msg)\n",
    "            for i, msg in enumerate(messages)\n",
    "        ]\n",
    "    else:\n",
    "        all_messages = [get_formatted_msg(msg) for msg in messages]\n",
    "\n",
    "    if WITH_SEP_TOKENS:\n",
    "        input_chat = '[CLS]' + (\"[SEP]\" + \"\\n\").join(all_messages)\n",
    "    else:\n",
    "        input_chat = \"[CLS]\" + (\"\\n\").join(all_messages)\n",
    "    return input_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29457cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(PATH)\n",
    "print(df.info())\n",
    "df['messages'] = df['messages'].apply(lambda x: preprocess(x))\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da14f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_info(dataset):\n",
    "    print(dataset)\n",
    "    # For each field, print the first entry\n",
    "    for field in dataset.features:\n",
    "        print(f\"{field}: {dataset[0][field]}\\n\")\n",
    "\n",
    "print_dataset_info(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bb8493",
   "metadata": {},
   "source": [
    "# ü™Ñ Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3255da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(SELECTED_BERT_MODEL)\n",
    "if WITH_TOKEN_TYPE_IDS:\n",
    "    tokenizer.add_special_tokens({\n",
    "        \"additional_special_tokens\": [\"[0]\", \"[1]\"]\n",
    "    })\n",
    "    id_0 = tokenizer.convert_tokens_to_ids(\"[0]\")\n",
    "    id_1 = tokenizer.convert_tokens_to_ids(\"[1]\")\n",
    "    print(f\"Special tokens added: {id_0} for [0], {id_1} for [1]\")\n",
    "\n",
    "def preprocess(examples):\n",
    "    tokenized_chats = tokenizer(\n",
    "        examples['messages'],\n",
    "        add_special_tokens=False # Skip special tokens in the target text\n",
    "    )\n",
    "    tokenized_chats[\"labels\"] = examples[\"labels\"]\n",
    "    tokenized_chats[\"couple_ids\"] = examples[\"couple_ids\"]\n",
    "    # tokenized_chats[\"msgs_lengths\"] = examples[\"msgs_lengths\"]\n",
    "\n",
    "    if WITH_TOKEN_TYPE_IDS:\n",
    "        for i, sample in enumerate(tokenized_chats['input_ids']):\n",
    "            new_input_ids = []\n",
    "            new_attention_mask = []\n",
    "            new_token_type_ids = []\n",
    "            tkn_type = 0  # Default token type\n",
    "            \n",
    "            for j, tkn in enumerate(sample):\n",
    "                if tkn == id_0:\n",
    "                    tkn_type = 0\n",
    "                    # Skip adding this special token to new lists\n",
    "                    continue\n",
    "                elif tkn == id_1:\n",
    "                    tkn_type = 1\n",
    "                    # Skip adding this special token to new lists\n",
    "                    continue\n",
    "                else:\n",
    "                    # Add token with current token type\n",
    "                    new_input_ids.append(tkn)\n",
    "                    new_attention_mask.append(tokenized_chats['attention_mask'][i][j])\n",
    "                    new_token_type_ids.append(tkn_type)\n",
    "            \n",
    "            # Replace the original lists with the new ones\n",
    "            tokenized_chats['input_ids'][i] = new_input_ids\n",
    "            tokenized_chats['attention_mask'][i] = new_attention_mask\n",
    "            tokenized_chats['token_type_ids'][i] = new_token_type_ids\n",
    "\n",
    "            if len(new_input_ids) > MAX_LENGTH:\n",
    "                raise ValueError(\n",
    "                    f\"Chat length exceeds MAX_LENGTH = {MAX_LENGTH}.\\n\"\n",
    "                    f\"Chat content:\\n{new_input_ids}\"\n",
    "                )\n",
    "             \n",
    "    return tokenized_chats\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "print_dataset_info(tokenized_dataset)\n",
    "# print(tokenized_dataset)\n",
    "\n",
    "# remove the special tokens from the tokenizer\n",
    "tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": []\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14078ddf",
   "metadata": {},
   "source": [
    "# üìÑ‚Äã Methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b3e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(scores, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Computes the confidence interval for a given performance metric.\n",
    "\n",
    "    This function is useful for understanding the reliability of a single model's \n",
    "    mean performance score from cross-validation.\n",
    "\n",
    "    Args:\n",
    "        scores (list or np.ndarray): A list of scores from cross-validation folds.\n",
    "        confidence_level (float): The desired confidence level (e.g., 0.95 for 95%).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the mean score, and the lower and upper bounds \n",
    "               of the confidence interval (mean, lower_bound, upper_bound).\n",
    "    \"\"\"\n",
    "    n = len(scores)\n",
    "    if n <= 1:\n",
    "        # Cannot compute CI for 1 or 0 scores, return mean and NaN for bounds\n",
    "        return (np.mean(scores), np.nan, np.nan)\n",
    "        \n",
    "    mean_score = np.mean(scores)\n",
    "    # Standard Error of the Mean (SEM) = Sn / sqrt(n)\n",
    "    # where Sn is the standard deviation of the scores\n",
    "    std_err = stats.sem(scores)\n",
    "    \n",
    "    # Degrees of freedom\n",
    "    dof = n - 1\n",
    "    \n",
    "    # Get the critical value from the t-distribution\n",
    "    t_critical = stats.t.ppf((1 + confidence_level) / 2., dof)\n",
    "    \n",
    "    margin_of_error = t_critical * std_err\n",
    "    \n",
    "    lower_bound = mean_score - margin_of_error\n",
    "    upper_bound = mean_score + margin_of_error\n",
    "    \n",
    "    return (mean_score, lower_bound, upper_bound)\n",
    "\n",
    "def print_and_save_classification_report_conf_intervals(cv_results, save_path, confidence=0.95):\n",
    "    with open(os.path.join(save_path, \"classification_report_with_cv.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"=== Cross-Validation Results (Mean ¬± Std [{confidence * 100:.0f}% CI]) ===\\n\\n\")\n",
    "        print(f\"=== Cross-Validation Results (Mean ¬± Std [{confidence * 100:.0f}% CI]) ===\\n\")\n",
    "\n",
    "        # Create the classification report format\n",
    "        report_lines = []\n",
    "        \n",
    "        # Header\n",
    "        header = f\"{'':>14} {'precision':>27} {'recall':>27} {'f1-score':>27}\"\n",
    "        report_lines.append(header)\n",
    "        report_lines.append(\"\")\n",
    "        \n",
    "        # Per-class metrics\n",
    "        for i, name in enumerate(TARGET_NAMES):\n",
    "            name_lower = name.lower()\n",
    "            \n",
    "            # Calculate confidence intervals for each metric\n",
    "            precision_scores = cv_results[f'test_precision_{name_lower}']\n",
    "            precision_mean, precision_lower, precision_upper = confidence_interval(precision_scores, confidence)\n",
    "            precision_std = np.std(precision_scores)\n",
    "\n",
    "            recall_scores = cv_results[f'test_recall_{name_lower}']\n",
    "            recall_mean, recall_lower, recall_upper = confidence_interval(recall_scores, confidence)\n",
    "            recall_std = np.std(recall_scores)\n",
    "\n",
    "            f1_scores = cv_results[f'test_f1_{name_lower}']\n",
    "            f1_mean, f1_lower, f1_upper = confidence_interval(f1_scores, confidence)\n",
    "            f1_std = np.std(f1_scores)\n",
    "\n",
    "            # Format with confidence intervals\n",
    "            precision_ci = f\"{precision_mean:.2f} ¬± {precision_std:.2f} [{precision_lower:.2f}, {precision_upper:.2f}]\"\n",
    "            recall_ci = f\"{recall_mean:.2f} ¬± {recall_std:.2f} [{recall_lower:.2f}, {recall_upper:.2f}]\"\n",
    "            f1_ci = f\"{f1_mean:.2f} ¬± {f1_std:.2f} [{f1_lower:.2f}, {f1_upper:.2f}]\"\n",
    "\n",
    "            line = f\"{name:>14} {precision_ci:>27} {recall_ci:>27} {f1_ci:>27}\"\n",
    "            report_lines.append(line)\n",
    "        \n",
    "        report_lines.append(\"\")\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy_scores = cv_results['test_accuracy']\n",
    "        accuracy_mean, accuracy_lower, accuracy_upper = confidence_interval(accuracy_scores, confidence)\n",
    "        accuracy_std = np.std(accuracy_scores)\n",
    "        accuracy_ci = f\"{accuracy_mean:.2f} ¬± {accuracy_std:.2f} [{accuracy_lower:.2f}, {accuracy_upper:.2f}]\"\n",
    "        line = f\"{'accuracy':>14} {'':>27} {'':>27} {accuracy_ci:>27}\"\n",
    "        report_lines.append(line)\n",
    "        \n",
    "        # Macro and weighted averages\n",
    "        for avg_type in ['macro', 'weighted']:\n",
    "            precision_scores = cv_results[f'test_precision_{avg_type}']\n",
    "            precision_mean, precision_lower, precision_upper = confidence_interval(precision_scores, confidence)\n",
    "            precision_std = np.std(precision_scores)\n",
    "\n",
    "            recall_scores = cv_results[f'test_recall_{avg_type}']\n",
    "            recall_mean, recall_lower, recall_upper = confidence_interval(recall_scores, confidence)\n",
    "            recall_std = np.std(recall_scores)\n",
    "\n",
    "            f1_scores = cv_results[f'test_f1_{avg_type}']\n",
    "            f1_mean, f1_lower, f1_upper = confidence_interval(f1_scores, confidence)\n",
    "            f1_std = np.std(f1_scores)\n",
    "\n",
    "            # Format with confidence intervals\n",
    "            precision_ci = f\"{precision_mean:.2f} ¬± {precision_std:.2f} [{precision_lower:.2f}, {precision_upper:.2f}]\"\n",
    "            recall_ci = f\"{recall_mean:.2f} ¬± {recall_std:.2f} [{recall_lower:.2f}, {recall_upper:.2f}]\"\n",
    "            f1_ci = f\"{f1_mean:.2f} ¬± {f1_std:.2f} [{f1_lower:.2f}, {f1_upper:.2f}]\"\n",
    "\n",
    "            line = f\"{avg_type + ' avg':>14} {precision_ci:>27} {recall_ci:>27} {f1_ci:>27}\"\n",
    "            report_lines.append(line)\n",
    "        \n",
    "        # Cost (if multiclass)\n",
    "        if TASK == 'multiclass':\n",
    "            cost_scores = cv_results['test_cost']\n",
    "            cost_mean, cost_lower, cost_upper = confidence_interval(cost_scores, confidence)\n",
    "            cost_std = np.std(cost_scores)\n",
    "            cost_ci = f\"{cost_mean:.2f} ¬± {cost_std:.2f} [{cost_lower:.2f}, {cost_upper:.2f}]\"\n",
    "            report_lines.append(\"\")\n",
    "            report_lines.append(f\"Total Cost: {cost_ci}\")\n",
    "        \n",
    "        # Write to file and print\n",
    "        report_text = \"\\n\".join(report_lines)\n",
    "        f.write(report_text)\n",
    "        print(report_text)\n",
    "\n",
    "def plot_confusion_matrices(cms, classes, path=None):\n",
    "    \"\"\"\n",
    "    Plots a single confusion matrix showing mean ¬± standard deviation for each cell.\n",
    "    \n",
    "    Args:\n",
    "        cv_results: Cross-validation results containing confusion_matrix scores\n",
    "        classes: List of class names\n",
    "        path: Path to save the plot\n",
    "    \"\"\"\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    # Calculate mean and std for each cell\n",
    "    cm_mean = np.mean(cms, axis=0)\n",
    "    cm_std = np.std(cms, axis=0)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    \n",
    "    # Create annotations with mean ¬± std format\n",
    "    annotations = np.empty_like(cm_mean, dtype=object)\n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_classes):\n",
    "            annotations[i, j] = f'{cm_mean[i, j]:.1f} ¬± {cm_std[i, j]:.2f}'\n",
    "    \n",
    "    sns.heatmap(\n",
    "        cm_mean, \n",
    "        annot=annotations, \n",
    "        fmt='', \n",
    "        cmap=sns.color_palette(\"ch:s=-.2,r=.6\", as_cmap=True),\n",
    "        xticklabels=classes, \n",
    "        yticklabels=classes,\n",
    "        cbar_kws={'label': 'Mean Count'}\n",
    "    )\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion Matrix (Mean ¬± Std)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if path:\n",
    "        plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eacaade",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cdf6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_cost(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the total cost of predictions using a cost matrix.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels  \n",
    "        cost_mat: Cost matrix where cost_mat[i,j] is the cost of \n",
    "                 predicting class j when true class is i\n",
    "    \n",
    "    Returns:\n",
    "        Total cost (scalar)\n",
    "    \"\"\"\n",
    "    # Generate labels that match cost matrix dimensions\n",
    "    # Assumes labels are 0, 1, 2, ..., num_classes-1\n",
    "    num_classes = cost_mat.shape[0]\n",
    "    labels = np.arange(num_classes)\n",
    "    \n",
    "    # Get confusion matrix with all possible labels\n",
    "    conf_mat = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    # Calculate total cost\n",
    "    return np.sum(conf_mat * cost_mat)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    precision = precision_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    result = {\n",
    "        'accuracy': torch.tensor([accuracy]),\n",
    "        'precision': torch.tensor([precision]),\n",
    "        'recall': torch.tensor([recall]),\n",
    "        'f1_weighted': torch.tensor([f1_weighted]),\n",
    "    }\n",
    "    \n",
    "    if TASK == 'multiclass':\n",
    "        cost = calculate_total_cost(labels, predictions)\n",
    "        result['cost'] = torch.tensor([cost])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97836e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainer(tokenized_train_set, tokenized_eval_set, out_dir):\n",
    "\n",
    "    # free all unused occupied RAM and GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        warmup_ratio=WARMUP_PERCENTAGE,\n",
    "        \n",
    "        # lr_scheduler_type=\"linear\",\n",
    "        lr_scheduler_type=\"reduce_lr_on_plateau\",\n",
    "        lr_scheduler_kwargs=LR_SCHEDULER_KWARGS,\n",
    "\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        \n",
    "        report_to=\"none\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        \n",
    "        load_best_model_at_end=True,\n",
    "\n",
    "        metric_for_best_model=\"cost\" if TASK == \"multiclass\" else \"f1_weighted\",\n",
    "        greater_is_better=False if TASK == \"multiclass\" else True,\n",
    "        save_total_limit=SAVE_TOTAL_LIMIT,\n",
    "\n",
    "        dataloader_num_workers=NUM_WORKERS,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        group_by_length=True,\n",
    "        remove_unused_columns=True,\n",
    "\n",
    "        seed=GLOBAL_SEED,\n",
    "        data_seed=GLOBAL_SEED\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        SELECTED_BERT_MODEL,\n",
    "        num_labels=NUM_LABELS,\n",
    "        problem_type=\"single_label_classification\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_set,\n",
    "        eval_dataset=tokenized_eval_set,\n",
    "        # tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(\n",
    "            early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "            # early_stopping_threshold=EARLY_STOPPING_THRESHOLD\n",
    "        )]\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d34e1f",
   "metadata": {},
   "source": [
    "# ü§ñ Fine-Tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fcd249",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "os.makedirs(NESTED_CV_RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "outer_cv = GroupKFold(n_splits=5) # , shuffle=True, random_state=GLOBAL_SEED\n",
    "confusion_matrices = []\n",
    "reports = []\n",
    "all_log_histories = []\n",
    "df = tokenized_dataset.to_pandas()\n",
    "for fold, (train_idx, test_idx) in enumerate(outer_cv.split(\n",
    "    X=df[['input_ids', 'token_type_ids', 'attention_mask']],\n",
    "    y=df['labels'],\n",
    "    groups=df['couple_ids']\n",
    ")):\n",
    "    print(f\"Starting fold {fold + 1}/{outer_cv.get_n_splits()}\")\n",
    "\n",
    "    # Subset the datasets for the current fold\n",
    "    fold_train_dataset = tokenized_dataset.select(train_idx)\n",
    "    fold_test_dataset = tokenized_dataset.select(test_idx)\n",
    "\n",
    "    # Further split fold_train_dataset into training and validation sets\n",
    "    gss_val = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=GLOBAL_SEED)\n",
    "    train_idx, eval_idx = next(gss_val.split(\n",
    "        X=fold_train_dataset,\n",
    "        y=fold_train_dataset['labels'],\n",
    "        groups=fold_train_dataset['couple_ids']\n",
    "    ))\n",
    "    fold_eval_dataset = fold_train_dataset.select(eval_idx)\n",
    "    fold_train_dataset = fold_train_dataset.select(train_idx)\n",
    "\n",
    "    # remove all couple_ids columns\n",
    "    fold_train_dataset = fold_train_dataset.remove_columns(['couple_ids'])\n",
    "    fold_eval_dataset = fold_eval_dataset.remove_columns(['couple_ids'])\n",
    "    fold_test_dataset = fold_test_dataset.remove_columns(['couple_ids'])\n",
    "\n",
    "    # Set the format to PyTorch tensors\n",
    "    fold_train_dataset.set_format(\"torch\")\n",
    "    fold_eval_dataset.set_format(\"torch\")\n",
    "    fold_test_dataset.set_format(\"torch\")\n",
    "\n",
    "    # Define a unique output directory for this fold\n",
    "    fold_output_dir = os.path.join(OUT_DIR, f\"fold_{fold+1}\")\n",
    "    trainer = get_trainer(fold_train_dataset, fold_eval_dataset, fold_output_dir)\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    predictions_output = trainer.predict(fold_test_dataset)\n",
    "    preds = np.argmax(predictions_output.predictions, axis=1)\n",
    "    labels = predictions_output.label_ids\n",
    "    report = classification_report(\n",
    "        labels, preds, zero_division=0,\n",
    "        target_names=TARGET_NAMES,\n",
    "        output_dict=True\n",
    "    )\n",
    "    if TASK == 'multiclass':\n",
    "        report['cost'] = calculate_total_cost(labels, preds)\n",
    "\n",
    "    confusion_matrices.append(confusion_matrix(labels, preds))\n",
    "    reports.append(report)\n",
    "    all_log_histories.append(trainer.state.log_history)\n",
    "\n",
    "    print(f\"Cleaning up checkpoint directory: {fold_output_dir}\")\n",
    "    shutil.rmtree(fold_output_dir, ignore_errors=True)\n",
    "\n",
    "# create cv_results to be passed to print_and_save_classification_report_conf_intervals from reports\n",
    "cv_results = defaultdict(list)\n",
    "for report in reports:\n",
    "    for key, value in report.items():\n",
    "        if key in TARGET_NAMES:\n",
    "            key_lower = key.lower()\n",
    "            cv_results[f'test_precision_{key_lower}'].append(value['precision'])\n",
    "            cv_results[f'test_recall_{key_lower}'].append(value['recall'])\n",
    "            cv_results[f'test_f1_{key_lower}'].append(value['f1-score'])\n",
    "        elif key in ['macro avg', 'weighted avg']:\n",
    "            avg_type = key.split()[0]  # 'macro' or 'weighted'\n",
    "            avg_type_lower = avg_type.lower()\n",
    "            cv_results[f'test_precision_{avg_type_lower}'].append(value['precision'])\n",
    "            cv_results[f'test_recall_{avg_type_lower}'].append(value['recall'])\n",
    "            cv_results[f'test_f1_{avg_type_lower}'].append(value['f1-score'])\n",
    "        elif key in ['accuracy', 'cost']:\n",
    "            cv_results['test_' + key].append(value)\n",
    "            \n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results.to_csv(os.path.join(NESTED_CV_RESULTS_PATH, \"BERT.csv\"), index=False)\n",
    "print_and_save_classification_report_conf_intervals(cv_results, RESULTS_PATH, confidence=0.95)\n",
    "plot_confusion_matrices(confusion_matrices, TARGET_NAMES, path=os.path.join(RESULTS_PATH, \"confusion_matrix_cv.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c944709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_aggregated_curves(all_log_histories, RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best_nepochs(all_log_histories):\n",
    "    \"\"\"\n",
    "    Determines the optimal number of epochs based on the task type.\n",
    "    \n",
    "    For binary classification: chooses epoch with maximum mean f1_weighted\n",
    "    For multiclass classification: chooses epoch with minimum mean cost\n",
    "    \n",
    "    Args:\n",
    "        all_log_histories: List of training log histories from all CV folds\n",
    "        \n",
    "    Returns:\n",
    "        int: Optimal number of epochs\n",
    "    \"\"\"\n",
    "    if TASK == 'multiclass':\n",
    "        # For multiclass, minimize cost\n",
    "        eval_costs_by_epoch = defaultdict(list)\n",
    "        for history in all_log_histories:\n",
    "            for log in history:\n",
    "                if 'eval_cost' in log and log.get('epoch') is not None:\n",
    "                    epoch = int(round(log['epoch']))\n",
    "                    eval_costs_by_epoch[epoch].append(log['eval_cost'])\n",
    "            \n",
    "        epochs = sorted(eval_costs_by_epoch.keys())\n",
    "        mean_eval_cost = [np.mean(eval_costs_by_epoch[e]) for e in epochs]\n",
    "        optimal_epochs = epochs[np.argmin(mean_eval_cost)]\n",
    "        \n",
    "        print(f\"Optimal number of epochs (min cost): {optimal_epochs}\")\n",
    "        print(f\"Mean cost at optimal epoch: {min(mean_eval_cost):.4f}\")\n",
    "        \n",
    "    else:  # binary classification\n",
    "        # For binary, maximize f1_weighted\n",
    "        eval_f1_by_epoch = defaultdict(list)\n",
    "        for history in all_log_histories:\n",
    "            for log in history:\n",
    "                if 'eval_f1_weighted' in log and log.get('epoch') is not None:\n",
    "                    epoch = int(round(log['epoch']))\n",
    "                    eval_f1_by_epoch[epoch].append(log['eval_f1_weighted'])\n",
    "            \n",
    "        epochs = sorted(eval_f1_by_epoch.keys())\n",
    "        mean_eval_f1 = [np.mean(eval_f1_by_epoch[e]) for e in epochs]\n",
    "        optimal_epochs = epochs[np.argmax(mean_eval_f1)]\n",
    "        \n",
    "        print(f\"Optimal number of epochs (max f1_weighted): {optimal_epochs}\")\n",
    "        print(f\"Mean f1_weighted at optimal epoch: {max(mean_eval_f1):.4f}\")\n",
    "    \n",
    "    return optimal_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c1d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Determine Optimal Number of Epochs\n",
    "optimal_epochs = choose_best_nepochs(all_log_histories)\n",
    "\n",
    "# 2. Prepare Full Dataset\n",
    "final_train_dataset = tokenized_dataset.remove_columns(['couple_ids'])\n",
    "final_train_dataset.set_format(\"torch\")\n",
    "\n",
    "# 3. Configure Trainer for Final Run\n",
    "FINAL_MODEL_DIR = os.path.join(OUT_DIR, \"final_production_model\")\n",
    "os.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "final_training_args = TrainingArguments(\n",
    "    output_dir=FINAL_MODEL_DIR,\n",
    "    num_train_epochs=optimal_epochs, # Train for the optimal number of epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_PERCENTAGE,\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"no\", # We will save manually at the end\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=NUM_WORKERS,\n",
    "    seed=GLOBAL_SEED,\n",
    "    data_seed=GLOBAL_SEED\n",
    ")\n",
    "\n",
    "final_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    SELECTED_BERT_MODEL,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "final_trainer = Trainer(\n",
    "    model=final_model,\n",
    "    args=final_training_args,\n",
    "    train_dataset=final_train_dataset,\n",
    "    # No eval_dataset needed for the final run\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "final_trainer.train()\n",
    "\n",
    "final_trainer.save_model(FINAL_MODEL_DIR)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d378cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/kaggle\"):\n",
    "    shutil.make_archive(OUT_DIR, 'zip', OUT_DIR)\n",
    "    # shutil.rmtree(OUT_DIR)\n",
    "    # os.remove(OUT_DIR + '.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
