{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab09196",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55f0c9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicol\\Desktop\\cipv-assistant\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dfb8478",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_regex = re.compile(r\"(?P<stage>Stage: .+)\\nChat Polarity Mean: (?:-?|\\+?)\\d\\.?\\d?\\d?\\nChat Polarity Variance: \\d\\.?\\d?\\d?\\n(?P<event>Event: .+)\\n\\n(?P<chat>(?:.+|\\n+)+)\")\n",
    "msgs_regex = re.compile(r\"(?P<message>(?P<timestamp>\\d\\d\\d\\d-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d) \\| (?P<name>.+):\\n(?P<content>.+)\\nPolarity: (?P<polarity>(?:-?|\\+?)\\d\\.?\\d?\\d?)\\n\\[(?P<tag_explanation>(?P<tag>Tag: .+)\\n?(?P<explanation>Spiegazione: .+))\\])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e747c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer_bart = AutoTokenizer.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0032e196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chats: 100%|██████████| 7/7 [00:00<00:00,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART Max Token Count: 1006, Min Token Count: 45\n",
      "BART Mean Token Count: 580.39\n",
      "BART Variance Token Count: 183.09\n",
      "BART Over 1024 Tokens: 0 / 244\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"./rsc/gemini-2.5-flash-dataset_2025-07-07-10-45-16/chats\"\n",
    "dirs = os.listdir(path)\n",
    "bert_lengths = []\n",
    "bart_lengths = []\n",
    "polarities = []\n",
    "raw_chats = []\n",
    "for directory in tqdm.tqdm(dirs, desc=\"Processing chats\"):\n",
    "    files = os.listdir(os.path.join(path, directory))\n",
    "    for file in files:\n",
    "        bert_count = 0\n",
    "        bart_count = 0\n",
    "        polarity_sum = 0\n",
    "        with open(os.path.join(path, directory, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            match = top_regex.match(content)\n",
    "            if match:\n",
    "                chat = match.group(\"chat\")\n",
    "                messages = list(msgs_regex.finditer(chat))\n",
    "                # checks if there are matched messages\n",
    "                if len(messages) > 0:\n",
    "                    total = 0\n",
    "                    example = {\n",
    "                        \"messages\": [],\n",
    "                        \"polarities\": [],\n",
    "                        \"explanations\": []\n",
    "                    }\n",
    "                    for message in messages:\n",
    "                        content = message.group(\"content\")\n",
    "                        if content:\n",
    "                            example[\"messages\"] += [content]\n",
    "                            example[\"polarities\"] += [float(message.group(\"polarity\"))]\n",
    "                            example[\"explanations\"] += [\n",
    "                                message.group(\"tag\") + \"\\n\" +\n",
    "                                message.group(\"explanation\")\n",
    "                            ]\n",
    "                            bert_tokens = tokenizer_bert.encode(content, add_special_tokens=True)\n",
    "                            bart_tokens = tokenizer_bart.encode(content, add_special_tokens=True)\n",
    "                            bert_count += len(bert_tokens)\n",
    "                            bart_count += len(bart_tokens)\n",
    "                        polarity_sum += float(message.group(\"polarity\"))\n",
    "                        total += 1\n",
    "                    # try:\n",
    "                    #     polarities.append(polarity_sum / total)\n",
    "                    # except ZeroDivisionError:\n",
    "                    #     print(f\"ZeroDivisionError in file: {file}\")\n",
    "                    if bart_count <= 1024:\n",
    "                        raw_chats.append(example)\n",
    "                        bert_lengths.append(bert_count)\n",
    "                        bart_lengths.append(bart_count)\n",
    "                else:\n",
    "                    print(f\"No messages found in file: {os.path.join(path, directory, file)}\")\n",
    "            else:\n",
    "                print(f\"No match found in file: {os.path.join(path, directory, file)}\")\n",
    "\n",
    "bert_v = np.array(bert_lengths)\n",
    "bart_v = np.array(bart_lengths)\n",
    "polarities_v = np.array(polarities)\n",
    "\n",
    "# print(f\"BERT Max Token Count: {np.max(bert_v)}, Min Token Count: {np.min(bert_v)}\")\n",
    "# print(f\"BERT Mean Token Count: {np.mean(bert_v):.2f}\")\n",
    "# print(f\"BERT Variance Token Count: {np.std(bert_v):.2f}\")\n",
    "# print(f\"BERT Over 512 Tokens: {len(bert_v[bert_v > 512])} / {bert_v.shape[0]}\\n\")\n",
    "\n",
    "print(f\"BART Max Token Count: {np.max(bart_v)}, Min Token Count: {np.min(bart_v)}\")\n",
    "print(f\"BART Mean Token Count: {np.mean(bart_v):.2f}\")\n",
    "print(f\"BART Variance Token Count: {np.std(bart_v):.2f}\")\n",
    "print(f\"BART Over 1024 Tokens: {len(bart_v[bart_v > 1024])} / {bart_v.shape[0]}\\n\")\n",
    "\n",
    "# print(f\"Polarities Mean: {np.mean(polarities_v):.2f}\")\n",
    "# print(f\"Polarities Variance: {np.std(polarities_v):.2f}\")\n",
    "# print(f\"Polarities Min: {np.min(polarities_v):.2f}\")\n",
    "# print(f\"Polarities Max: {np.max(polarities_v):.2f}\")\n",
    "# print(f\"Polarities Over 0: {len(polarities_v[polarities_v > 0])} / {polarities_v.shape[0]}\")\n",
    "# print(f\"Polarities Under 0: {len(polarities_v[polarities_v < 0])} / {polarities_v.shape[0]}\")\n",
    "# print(f\"Polarities Around 0: {len(polarities_v[np.logical_and(polarities_v < 0.5, polarities_v > -0.5)])} / {polarities_v.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3fb4f6",
   "metadata": {},
   "source": [
    "# BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b4d1b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "import evaluate # Hugging Face's library for evaluation\n",
    "from datetime import datetime\n",
    "import os\n",
    "from src.model import (\n",
    "    BartForChatRegressionAndGeneration,\n",
    "    ChatMultiTaskOutput,\n",
    "    CustomChatTrainer,\n",
    "    DataCollatorForChat\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1317be87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Checks if torch can detect a GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "189c0b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 244/244 [00:00<00:00, 21475.40 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['messages', 'polarities', 'explanations'],\n",
      "    num_rows: 244\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# raw_chats = [\n",
    "#     {\n",
    "#         \"messages\": [\"Sei un incompetente!\", \"Davvero, non capisci niente.\"],\n",
    "#         \"polarities\": [-0.9, -1.0],\n",
    "#         \"explanations\": [\"Il messaggio contiene un insulto.\", \"Il messaggio rafforza l'attacco personale.\"]\n",
    "#     },\n",
    "#     {\n",
    "#         \"messages\": [\"Grazie mille per l'aiuto.\", \"Sei stato gentilissimo.\", \"Apprezzo molto il tuo tempo.\"],\n",
    "#         \"polarities\": [1.0, 0.9, 0.9],\n",
    "#         \"explanations\": [\"Esprime gratitudine esplicita.\", \"Contiene un complimento diretto.\", \"Mostra apprezzamento per lo sforzo altrui.\"]\n",
    "#     },\n",
    "#     {\n",
    "#         \"messages\": [\"Non sono d'accordo con questa decisione.\"],\n",
    "#         \"polarities\": [0.1],\n",
    "#         \"explanations\": [\"Esprime disaccordo in modo neutrale e rispettoso.\"]\n",
    "#     },\n",
    "#     {\n",
    "#         \"messages\": [\"Fai schifo.\", \"Spero che ti licenzino.\"],\n",
    "#         \"polarities\": [-1.0, -1.0],\n",
    "#         \"explanations\": [\"Contiene un insulto grave.\", \"Contiene un augurio negativo e minaccioso.\"]\n",
    "#     },\n",
    "#     {\n",
    "#         \"messages\": [\"Il prodotto è arrivato rotto.\", \"Il servizio clienti non risponde.\", \"Sono molto insoddisfatto.\"],\n",
    "#         \"polarities\": [-0.8, -0.7, -0.9],\n",
    "#         \"explanations\": [\"Descrive un problema con il prodotto.\", \"Lamenta una mancanza di supporto.\", \"Esprime insoddisfazione generale.\"]\n",
    "#     }\n",
    "# ]\n",
    "# Convert to a Hugging Face Dataset\n",
    "dataset = Dataset.from_list(raw_chats)\n",
    "#remove empty lists from dataset['messages']\n",
    "dataset = dataset.filter(lambda x: len(x['messages']) > 0)\n",
    "print(dataset)\n",
    "# print(dataset[0]['messages'])\n",
    "# print(dataset[0]['polarities'])\n",
    "# print(dataset[0]['explanations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0205906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special token ID for '<sep>': 52000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 244/244 [00:00<00:00, 1746.28 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels', 'regression_labels'],\n",
      "    num_rows: 244\n",
      "})\n",
      "[0, 6664, 2209, 16, 9644, 312, 473, 3174, 2440, 368, 2565, 2467, 322, 77, 1005, 1268, 18, 4641, 458, 781, 225, 69, 329, 19613, 1777, 266, 4448, 458, 225, 77, 82, 10267, 18, 1474, 8776, 4441, 266, 17149, 18, 1519, 8368, 467, 8671, 18, 52000, 7354, 4266, 18, 12711, 16, 493, 301, 1798, 2467, 345, 7319, 1050, 16, 939, 6480, 329, 11, 38863, 11022, 18, 2554, 329, 357, 11, 22553, 16, 346, 6759, 300, 311, 1005, 18, 1474, 41669, 1366, 710, 515, 322, 77, 4240, 368, 4567, 11227, 16, 4058, 15013, 329, 1900, 1773, 16262, 35, 632, 2340, 1039, 944, 3793, 35, 52000, 4076, 1039, 329, 5239, 1777, 16, 20826, 18, 1474, 7182, 225, 71, 83, 82, 225, 77, 3803, 8997, 7832, 300, 368, 7067, 322, 77, 1005, 16, 384, 1049, 968, 10527, 18, 1875, 329, 11, 7683, 322, 77, 35628, 18, 1282, 345, 329, 2092, 16, 13236, 3905, 322, 77, 880, 946, 312, 4275, 18, 1519, 8368, 5512, 18, 52000, 15561, 1366, 17855, 16, 4266, 18, 10962, 16, 14795, 18, 640, 1005, 345, 1777, 18, 351, 718, 377, 722, 1366, 524, 750, 335, 18, 32859, 356, 671, 491, 7067, 2722, 18, 13446, 4567, 18, 52000, 13996, 18, 1474, 2823, 1021, 18, 2279, 6006, 511, 1263, 18, 1519, 8368, 5512, 18, 52000, 11831, 16, 225, 69, 5512, 18, 52000, 2]\n",
      "[0, 25349, 30, 5389, 379, 1160, 408, 16, 27300, 322, 77, 16616, 49785, 16, 21781, 399, 322, 77, 18490, 829, 203, 55, 1688, 2810, 30, 4266, 4821, 1702, 1541, 1830, 1528, 10827, 16, 12755, 329, 12082, 26767, 266, 9114, 18, 581, 799, 3305, 345, 1698, 7619, 934, 16, 26849, 339, 3937, 1263, 266, 5748, 322, 77, 329, 2107, 7431, 322, 77, 12601, 20826, 332, 322, 77, 9932, 301, 14648, 18, 1875, 329, 4789, 312, 12520, 301, 3660, 16, 10522, 371, 2829, 322, 77, 385, 377, 607, 691, 18, 52000, 25349, 30, 9892, 7667, 5389, 45298, 16, 30127, 88, 34010, 322, 77, 18490, 829, 16, 628, 359, 751, 16, 12626, 322, 77, 2054, 430, 598, 203, 55, 1688, 2810, 30, 20826, 6452, 27063, 301, 799, 1528, 12257, 16, 346, 3759, 493, 322, 77, 9042, 329, 11, 3044, 322, 77, 5029, 16, 4677, 322, 77, 35588, 368, 6527, 18, 581, 799, 2977, 349, 679, 18437, 51, 2340, 1039, 944, 3793, 16231, 13, 14291, 301, 799, 6883, 371, 3660, 508, 761, 266, 301, 799, 4665, 322, 77, 329, 12889, 16, 346, 12520, 329, 12082, 1358, 26123, 18, 52000, 25349, 30, 488, 3762, 420, 16, 984, 2478, 1105, 322, 77, 2436, 5072, 49785, 16, 3424, 8457, 322, 77, 11346, 829, 16, 21781, 399, 322, 77, 1436, 1584, 410, 203, 55, 1688, 2810, 30, 4266, 17512, 301, 4098, 322, 77, 20826, 16, 6780, 9658, 225, 69, 329, 5239, 6491, 18, 581, 799, 28794, 345, 9114, 266, 301, 799, 26993, 678, 1290, 829, 6, 18437, 1548, 490, 3905, 322, 77, 880, 946, 312, 4275, 17993, 345, 329, 927, 300, 23523, 301, 3660, 266, 264, 11, 10273, 322, 77, 11823, 10892, 297, 16, 12014, 338, 11, 37, 377, 607, 691, 18, 581, 20636, 598, 345, 50064, 16, 346, 22113, 920, 1685, 311, 3596, 225, 69, 2042, 287, 12889, 18, 52000, 25349, 30, 461, 18152, 16, 5238, 4192, 16, 9170, 4618, 658, 21458, 16, 488, 3762, 420, 287, 9495, 267, 363, 203, 55, 1688, 2810, 30, 20826, 32344, 225, 71, 83, 82, 45309, 266, 20599, 18, 9567, 301, 799, 5856, 322, 77, 6527, 16, 9397, 4090, 2348, 311, 12889, 322, 77, 4266, 16, 12027, 311, 9173, 18, 640, 670, 4789, 345, 2930, 266, 13075, 322, 77, 19057, 16, 19797, 368, 5612, 266, 368, 3660, 19987, 18, 52000, 25349, 30, 5672, 883, 16, 1016, 2646, 16, 21781, 399, 322, 77, 18121, 8499, 203, 55, 1688, 2810, 30, 4266, 6452, 225, 71, 83, 82, 10757, 2315, 883, 266, 368, 2948, 5588, 322, 77, 32842, 16, 384, 32604, 9569, 301, 20599, 322, 77, 20826, 18, 640, 670, 4789, 345, 20562, 9114, 16, 45561, 301, 3660, 266, 301, 38844, 371, 761, 20552, 18, 581, 20636, 598, 345, 765, 6143, 16, 346, 384, 24980, 6467, 7878, 18, 52000, 25349, 30, 5672, 883, 50649, 283, 16, 1016, 323, 993, 35353, 1234, 16, 461, 18152, 203, 55, 1688, 2810, 30, 581, 3259, 5705, 268, 380, 16743, 322, 77, 20826, 14291, 301, 3107, 45309, 266, 311, 17127, 17472, 18, 1282, 225, 71, 11, 335, 4187, 7431, 322, 77, 6795, 20552, 16, 3948, 329, 6410, 9114, 16, 35719, 368, 3660, 266, 368, 437, 8933, 610, 3305, 18, 52000, 2]\n",
      "[0.5, 0.6, 0.3, 0.2, 0.1, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_CHECKPOINT = \"morenolq/bart-it\"\n",
    "SEP_TOKEN = \"<sep>\"\n",
    "\n",
    "# --- Tokenizer Setup ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "tokenizer.add_special_tokens({'sep_token': SEP_TOKEN})\n",
    "print(f\"Special token ID for '{SEP_TOKEN}': {tokenizer.sep_token_id}\")\n",
    "\n",
    "# --- Preprocessing ---\n",
    "def preprocess(examples):\n",
    "    '''\n",
    "    Preprocess the dataset by joining messages and explanations with the special separator token.\n",
    "    The inputs are the concatenated messages, and the targets are the concatenated explanations.\n",
    "    '''\n",
    "    inputs = [SEP_TOKEN.join(chat) + SEP_TOKEN for chat in examples['messages']]\n",
    "    targets = [SEP_TOKEN.join(chat) + SEP_TOKEN for chat in examples['explanations']]\n",
    "    tokenized_examples = tokenizer(\n",
    "        inputs,\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        text_target=targets\n",
    "    )\n",
    "    tokenized_examples[\"regression_labels\"] = examples[\"polarities\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "def print_dataset(dataset): # debug only\n",
    "    print(dataset)\n",
    "    print(dataset[0]['input_ids'])\n",
    "    print(dataset[0]['labels'])\n",
    "    print(dataset[0]['regression_labels'])\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "print_dataset(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a622a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForChat(tokenizer=tokenizer)\n",
    "new_dataset = data_collator(tokenized_dataset.to_list())\n",
    "new_dataset = Dataset.from_dict(new_dataset)\n",
    "print_dataset(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6decec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Metrics Computation ---\n",
    "rouge_metric = evaluate.load('rouge')\n",
    "bleu_metric = evaluate.load('bleu')\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    print(f\"Predictions: {preds}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # Decode generated summaries, replacing -100 padding with pad_token\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Decode reference summaries, replacing -100 padding with pad_token\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Simple text cleaning\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds_rouge = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in decoded_preds]\n",
    "    decoded_labels_rouge = [\"\\n\".join(nltk.sent_tokenize(label)) for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    rouge_result = rouge_metric.compute(predictions=decoded_preds_rouge, references=decoded_labels_rouge)\n",
    "    \n",
    "    # Compute BLEU scores\n",
    "    decoded_labels_bleu = [[label] for label in decoded_labels] # BLEU expects a list of references\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels_bleu)\n",
    "\n",
    "    result = {\n",
    "        \"rouge1\": rouge_result[\"rouge1\"],\n",
    "        \"rouge2\": rouge_result[\"rouge2\"],\n",
    "        \"rougeL\": rouge_result[\"rougeL\"],\n",
    "        \"bleu\": bleu_result[\"bleu\"]\n",
    "    }\n",
    "\n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152cc017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForChatRegressionAndGeneration were not initialized from the model checkpoint at morenolq/bart-it and are newly initialized: ['regression_head.1.bias', 'regression_head.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='195' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [195/195 04:09, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.825300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.679800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.367300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./out/2025-07-11_20-10-02/bart-it-chat-multitask-final\\\\tokenizer_config.json',\n",
       " './out/2025-07-11_20-10-02/bart-it-chat-multitask-final\\\\special_tokens_map.json',\n",
       " './out/2025-07-11_20-10-02/bart-it-chat-multitask-final\\\\vocab.json',\n",
       " './out/2025-07-11_20-10-02/bart-it-chat-multitask-final\\\\merges.txt',\n",
       " './out/2025-07-11_20-10-02/bart-it-chat-multitask-final\\\\added_tokens.json',\n",
       " './out/2025-07-11_20-10-02/bart-it-chat-multitask-final\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "OUT_DIR = \"./out/\" + timestamp\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "tokenized_train_dataset = train_test_split['train']\n",
    "tokenized_test_dataset = train_test_split['test']\n",
    "\n",
    "model = BartForChatRegressionAndGeneration.from_pretrained(MODEL_CHECKPOINT)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# Ensure the model's config has the correct sep_token_id\n",
    "model.config.sep_token_id = tokenizer.sep_token_id\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUT_DIR + \"/bart-it-chat-multitask\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=5,\n",
    "    per_device_eval_batch_size=5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True, # Crucial for generation metrics\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForChat(tokenizer=tokenizer)\n",
    "\n",
    "trainer = CustomChatTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics, # Add metrics computation\n",
    ")\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "trainer.save_model(OUT_DIR + \"/bart-it-chat-multitask-final\")\n",
    "tokenizer.save_pretrained(OUT_DIR + \"/bart-it-chat-multitask-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c20b3866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating on Test Set ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'eval_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_test_dataset)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate and add perplexity\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(\u001b[43meval_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      7\u001b[0m eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperplexity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(perplexity, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Final Evaluation Metrics ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'eval_loss'"
     ]
    }
   ],
   "source": [
    "# --- Final Evaluation on Test Set ---\n",
    "print(\"\\n--- Evaluating on Test Set ---\")\n",
    "eval_results = trainer.evaluate(eval_dataset=tokenized_test_dataset)\n",
    "\n",
    "# Calculate and add perplexity\n",
    "perplexity = math.exp(eval_results['eval_loss'])\n",
    "eval_results['perplexity'] = round(perplexity, 4)\n",
    "\n",
    "print(\"\\n--- Final Evaluation Metrics ---\")\n",
    "for key, value in sorted(eval_results.items()):\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cedd18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Use the correct local path for the model and tokenizer\n",
    "MODEL_LOCAL_PATH = OUT_DIR + \"/bart-it-chat-multitask-final\"\n",
    "\n",
    "model = BartForChatRegressionAndGeneration.from_pretrained(\n",
    "    MODEL_LOCAL_PATH\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_LOCAL_PATH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95d50fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Inference ---\n",
      "Generated IDs: tensor([[    2,     0, 25349,    30,   866,   297,   340,  1359,   203,  1818,\n",
      "          4114,    17,  7100,    17,  6371,  2158,    30,   516,    30,  2258,\n",
      "             2]], device='cuda:0')\n",
      "Input Messages:\n",
      "\n",
      "2024-03-29 17:00:10 | Lucia:\n",
      "Ciao amore sei bellissimo e fichissimo!<sep>\n",
      "2024-03-29 17:05:10 | Pippo:\n",
      "Non ho parole per descrivere quanto sei stupida. Troia<sep>\n",
      "2024-03-29 17:08:10 | Lucia:\n",
      "Come sei cattivo<sep>\n",
      "\n",
      "Predicted Polarities: [ 0.33225065 -0.07724506  0.09848431]\n",
      "Generated Explanations: \n",
      "Tag: Reciprocità\n",
      "2024-03-29 17:00:10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Inference ---\n",
    "print(\"\\n--- Running Inference ---\")\n",
    "\n",
    "def predict(text):\n",
    "    \"\"\"\n",
    "    Function to perform inference on a single piece of text.\n",
    "    \"\"\"\n",
    "    # Prepare the input\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        # max_length=1024,\n",
    "        truncation=True,\n",
    "        # padding=\"max_length\"\n",
    "    ).to(device)\n",
    "    # print(inputs)\n",
    "    \n",
    "    # Get the regression prediction\n",
    "    # We don't need gradients for inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        polarity_scores = outputs.regression_logits\n",
    "\n",
    "    # Generate the explanation text\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        # max_length=1024,\n",
    "        num_beams=7,\n",
    "        # early_stopping=True\n",
    "    )\n",
    "    print(f\"Generated IDs: {generated_ids}\")\n",
    "    # Decode the generated explanation skipping also the '<sep>' token\n",
    "    explanations = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return polarity_scores, explanations\n",
    "\n",
    "# Test with some examples\n",
    "chat = '''\n",
    "2024-03-29 17:00:10 | Lucia:\n",
    "Ciao amore sei bellissimo e fichissimo!<sep>\n",
    "2024-03-29 17:05:10 | Pippo:\n",
    "Non ho parole per descrivere quanto sei stupida. Troia<sep>\n",
    "2024-03-29 17:08:10 | Lucia:\n",
    "Come sei cattivo<sep>\n",
    "'''\n",
    "\n",
    "\n",
    "polarities, explanations = predict(chat)\n",
    "print(f\"Input Messages:\\n{chat}\")\n",
    "print(f\"Predicted Polarities: {polarities.cpu().numpy()}\")\n",
    "print(f\"Generated Explanations: \\n{explanations}\\n\")\n",
    "# input('Press Enter to continue...')  # Pause for user input before next iteration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
