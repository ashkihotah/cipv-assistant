{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe897e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ../input/requirements/requirements.txt\n",
    "# !pip install evaluate\n",
    "# !pip install rouge_score\n",
    "# !pip install bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02653c6f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41f6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import shutil\n",
    "import re\n",
    "import os\n",
    "\n",
    "import evaluate as ev\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import (\n",
    "    Seq2SeqTrainingArguments,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4792a16",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396b55ba",
   "metadata": {},
   "source": [
    "## Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60769c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"Using device: {torch.cuda.get_device_name(0)}\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "TRAIN_MODE = \"train_no_optuna\"\n",
    "# TRAIN_MODE = \"train_with_optuna\"\n",
    "TEST_MODE = \"test_no_optuna\"\n",
    "# TEST_MODE = \"test_with_optuna\"\n",
    "\n",
    "DEFAULT_CHECKPOINT = \"morenolq/bart-it\"\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "SAVE_TOTAL_LIMIT = 3\n",
    "TEST_SIZE = 0.2\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "WARMUP_PERCENTAGE = 0.1\n",
    "WEIGHT_DECAY = 0.01 # int the 0 to 0.1 range\n",
    "BODY_LR = 3e-5\n",
    "HEAD_LR = 1.5e-4\n",
    "\n",
    "OPTUNA_TRAIN_TRIALS=2\n",
    "OPTUNA_TEST_TRIALS=2\n",
    "PRUNER_WARMUP_STEPS=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1150a317",
   "metadata": {},
   "source": [
    "## Paths Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a97a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# ==== LOCAL SETTINGS ====\n",
    "PATH = os.path.join(\".\", \"out\", \"datasets\", \"cipv-chats-toxicity\", \"chats\") # , \"gen2\", \"chats\"\n",
    "OUT_DIR = os.path.join(\".\", \"out\", \"models\", \"BART\", timestamp)\n",
    "\n",
    "# ==== KAGGLE SETTINGS ====\n",
    "# PATH = os.path.join(os.sep, \"kaggle\", \"input\", \"cipv-chats-sentiment\")\n",
    "# OUT_DIR = os.path.join(os.sep, \"kaggle\", \"working\", \"out\")\n",
    "\n",
    "RESULTS_PATH = os.path.join(OUT_DIR, \"results\")\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde9cd7",
   "metadata": {},
   "source": [
    "## Kaggle Specific Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir(os.path.join(os.sep, \"kaggle\", \"working\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea886d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip_file_path = \"/kaggle/working/out-EuclideanLoss-MSELoss-single_sep_False\"\n",
    "# shutil.make_archive(zip_file_path, 'zip', zip_file_path)\n",
    "# shutil.rmtree(zip_file_path)\n",
    "# os.remove(zip_file_path + '.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a1767",
   "metadata": {},
   "source": [
    "## Plots Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(log_history, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Extract logs with 'epoch', 'loss', and 'eval_loss'\n",
    "    train_logs = [log for log in log_history if 'epoch' in log and 'loss' in log]\n",
    "    eval_logs = [log for log in log_history if 'epoch' in log and 'eval_loss' in log]\n",
    "\n",
    "    # Convert to DataFrames for easy grouping\n",
    "    train_df = DataFrame(train_logs)\n",
    "    eval_df = DataFrame(eval_logs)\n",
    "\n",
    "    # Group by epoch and compute mean loss per epoch\n",
    "    train_epoch_loss = train_df.groupby('epoch')['loss'].mean()\n",
    "    eval_epoch_loss = eval_df.groupby('epoch')['eval_loss'].mean()\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(train_epoch_loss.index, train_epoch_loss.values, 'g-o', label='Train Loss')\n",
    "    plt.plot(eval_epoch_loss.index, eval_epoch_loss.values, 'c-o', label='Eval Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Losses Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"learning_curve.png\"))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62adebe2",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c237e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_add(dataset, messages, couple_dir):\n",
    "    all_messages = [msg.group(\"name_content\") for msg in messages]\n",
    "\n",
    "    input_chat = \"\\n\".join(all_messages)\n",
    "\n",
    "    dataset['chats'].append(input_chat)\n",
    "    # use the directory as user_id\n",
    "    dataset['user_ids'].append(couple_dir)\n",
    "    dataset['msgs_lengths'].append(len(messages))\n",
    "\n",
    "def load_dataset(path):\n",
    "    # msgs_regex = re.compile(r\"(?P<message>(?P<timestamp>\\d\\d\\d\\d-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d) \\|? ?(?P<name_content>(?P<name>.+):\\n(?P<content>.+))\\n+Polarity: (?P<polarity>[-+]?\\d\\.?\\d?\\d?)\\n\\[(?P<tag_explanation>(?P<tag>Tag: .+)\\n?Spiegazione: (?P<explanation>.+))\\])\")\n",
    "    msgs_regex = re.compile(r\"(?P<message>\\(?(?P<timestamp>\\d\\d\\d\\d-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d)\\)? ?\\|? ?(?P<name_content>(?P<name>.+):\\n?\\s?(?P<content>.+))\\n?\\s?Polarity: (?P<polarity>(?:-?|\\+?)\\d\\.?\\d?\\d?))\")\n",
    "    explanation_regex = re.compile(r\"Spiegazione:\\n(?P<explanation>(?:.|\\n)+)\")\n",
    "    dataset = {\n",
    "        \"chats\": [],\n",
    "        \"explanations\": [],\n",
    "        \"user_ids\": [],\n",
    "        \"msgs_lengths\": []\n",
    "    }\n",
    "    skipped = 0\n",
    "    model_dirs = os.listdir(path)\n",
    "    for model_dir in tqdm(model_dirs, desc=\"ðŸ“‚ Loading Dataset\"):\n",
    "        model_dir_path = os.path.join(path, model_dir)\n",
    "        couple_dirs = os.listdir(model_dir_path)\n",
    "        for couple_dir in couple_dirs: # tqdm(couple_dirs, desc=f\"ðŸ“‚ Loading Directory: {model_dir_path}\")\n",
    "            couple_dir_path = os.path.join(model_dir_path, couple_dir)\n",
    "            files = os.listdir(couple_dir_path)\n",
    "            for file in files:\n",
    "                with open(os.path.join(couple_dir_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                    chat = f.read()\n",
    "                    messages = list(msgs_regex.finditer(chat))\n",
    "                    if len(messages) > 0: # checks if there are matched messages\n",
    "                        match = explanation_regex.search(chat)\n",
    "                        if match:\n",
    "                            dataset['explanations'].append(match.group(\"explanation\"))\n",
    "                        else:\n",
    "                            print(f\"No explanation found in file: {os.path.join(couple_dir_path, file)}\")\n",
    "                            skipped += 1\n",
    "                            continue\n",
    "                        simple_add(dataset, messages, couple_dir)\n",
    "                    else:\n",
    "                        skipped += 1\n",
    "                        print(f\"No messages found in file: {os.path.join(couple_dir_path, file)}\")\n",
    "                        \n",
    "    return dataset, skipped\n",
    "\n",
    "dataset, skipped = load_dataset(PATH)\n",
    "dataset = Dataset.from_dict(dataset)\n",
    "print(f\"Skipped: {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_info(dataset):\n",
    "    print(dataset)\n",
    "    # For each field, print the first entry\n",
    "    for field in dataset.features:\n",
    "        print(f\"{field}: {dataset[0][field]}\\n\")\n",
    "\n",
    "print_dataset_info(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3505aa33",
   "metadata": {},
   "source": [
    "# Pre-Processing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02488f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_CHECKPOINT)\n",
    "\n",
    "def preprocess(examples):\n",
    "    tokenized_chats = tokenizer(\n",
    "        examples['chats'],\n",
    "        # padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        # return_tensors='pt',\n",
    "        text_target=examples['explanations']\n",
    "    )\n",
    "    tokenized_chats[\"user_ids\"] = examples[\"user_ids\"]\n",
    "    tokenized_chats[\"msgs_lengths\"] = examples[\"msgs_lengths\"]\n",
    "    return tokenized_chats\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    # batch_size=1000,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "# print_dataset_info(tokenized_dataset)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490277ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(tokenized_dataset)\n",
    "\n",
    "more_than_1024_input_mask = df['input_ids'].apply(lambda x: len(x) > 1024)\n",
    "more_than_512_input_mask = df['input_ids'].apply(lambda x: len(x) > 512)\n",
    "more_than_1024_labels_mask = df['labels'].apply(lambda x: len(x) > 1024)\n",
    "more_than_512_labels_mask = df['labels'].apply(lambda x: len(x) > 512)\n",
    "\n",
    "# print(f\"Tokenized dataset:\\n{df.head()}\")\n",
    "print(\"input_ids token length statistics:\")\n",
    "print(f\"Number of samples with more than 1024 tokens: {len(df[more_than_1024_input_mask])}\")\n",
    "print(f\"Number of samples with more than 512 tokens: {len(df[more_than_512_input_mask])}\")\n",
    "\n",
    "print(\"\\nlabels token length statistics:\")\n",
    "print(f\"Number of samples with more than 1024 tokens: {len(df[more_than_1024_labels_mask])}\")\n",
    "print(f\"Number of samples with more than 512 tokens: {len(df[more_than_512_labels_mask])}\")\n",
    "\n",
    "# plots histograms for input_ids and labels with different colors in a single plot\n",
    "# with semi-transparent bars in order to visualize overlaps\n",
    "# with 1024 + 1 bins where the last bin is for samples with more than 1024 tokens\n",
    "\n",
    "input_ds_token_lengths = df['input_ids'].apply(lambda x: len(x))\n",
    "labels_ds_token_lengths = df['labels'].apply(lambda x: len(x))\n",
    "\n",
    "input_ds_token_lengths.hist(bins=input_ds_token_lengths.max(), edgecolor='blue', alpha=0.5, label='Input Chats')\n",
    "labels_ds_token_lengths.hist(bins=labels_ds_token_lengths.max(), edgecolor='orange', alpha=0.5, label='Output Explanations')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Token Length Distribution')\n",
    "plt.legend()\n",
    "# plt.savefig(os.path.join(OUT_DIR, \"token_and_message_length_distribution.png\"))\n",
    "plt.show()\n",
    "\n",
    "# min_msgs = df['msgs_lengths'].min()\n",
    "max_msgs = df['msgs_lengths'].max()\n",
    "df['msgs_lengths'].hist(\n",
    "    bins=range(max_msgs + 1),  # +2 so last bin includes max\n",
    "    edgecolor='black',\n",
    "    label='Messages Lengths',\n",
    "    align='left'\n",
    ")\n",
    "plt.xlabel('Number of Messages')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Messages Length Distribution')\n",
    "plt.legend()\n",
    "# plt.savefig(os.path.join(OUT_DIR, \"messages_length_distribution.png\"))\n",
    "plt.yscale('log')\n",
    "plt.xticks(range(max_msgs + 1))\n",
    "plt.show()\n",
    "\n",
    "# Remove all samples with more than 1024 tokens in input_ids and labels\n",
    "df = df[~more_than_1024_input_mask & ~more_than_1024_labels_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c0867",
   "metadata": {},
   "source": [
    "## Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d9c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Users in dataset: {df['user_ids'].nunique()}\")\n",
    "print(f\"Dataset size: {len(df)}\\n\")\n",
    "\n",
    "df.drop(columns=['msgs_lengths'], inplace=True)\n",
    "\n",
    "# Split the dataset into train, test, and eval sets using user_ids grouped examples\n",
    "grouped = df.groupby('user_ids')#.size().reset_index(name='counts')\n",
    "user_ids = list(grouped.groups.keys())#[:10]\n",
    "# df = df[df['user_ids'].isin(user_ids)]\n",
    "\n",
    "train_ids, test_ids = train_test_split(user_ids, test_size=TEST_SIZE, random_state=42)\n",
    "train_ids, eval_ids = train_test_split(train_ids, test_size=TEST_SIZE, random_state=42)\n",
    "tokenized_train_set = df[df['user_ids'].isin(train_ids)]\n",
    "tokenized_test_set = df[df['user_ids'].isin(test_ids)]\n",
    "tokenized_eval_set = df[df['user_ids'].isin(eval_ids)]\n",
    "\n",
    "# Prints how many users are in each set\n",
    "print(f\"Users in train set: {tokenized_train_set['user_ids'].nunique()}\")\n",
    "print(f\"Users in test set: {tokenized_test_set['user_ids'].nunique()}\")\n",
    "print(f\"Users in eval set: {tokenized_eval_set['user_ids'].nunique()}\\n\")\n",
    "\n",
    "# Remove the 'user_ids' column from the train, test, and eval sets\n",
    "tokenized_train_set = tokenized_train_set.drop(columns=['user_ids'])\n",
    "tokenized_test_set = tokenized_test_set.drop(columns=['user_ids'])\n",
    "tokenized_eval_set = tokenized_eval_set.drop(columns=['user_ids'])\n",
    "\n",
    "tokenized_train_set = tokenized_train_set.reset_index(drop=True)\n",
    "tokenized_test_set = tokenized_test_set.reset_index(drop=True)\n",
    "tokenized_eval_set = tokenized_eval_set.reset_index(drop=True)\n",
    "\n",
    "tokenized_train_set = Dataset.from_pandas(tokenized_train_set)\n",
    "tokenized_test_set = Dataset.from_pandas(tokenized_test_set)\n",
    "tokenized_eval_set = Dataset.from_pandas(tokenized_eval_set)\n",
    "\n",
    "print(f\"Train set size: {len(tokenized_train_set)}\")\n",
    "print(f\"Test set size: {len(tokenized_test_set)}\")\n",
    "print(f\"Eval set size: {len(tokenized_eval_set)}\\n\")\n",
    "\n",
    "# Set the format to PyTorch tensors\n",
    "tokenized_train_set.set_format(\"torch\")\n",
    "tokenized_test_set.set_format(\"torch\")\n",
    "tokenized_eval_set.set_format(\"torch\")\n",
    "\n",
    "print_dataset_info(tokenized_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c86159",
   "metadata": {},
   "source": [
    "# Fine-Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0433cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compute_metrics_fn(tokenizer, sts_model):    \n",
    "    # Initialize metrics outside the function to avoid reloading\n",
    "    rouge_metric = ev.load('rouge')\n",
    "    bleu_metric = ev.load('bleu')\n",
    "    bertscore_metric = ev.load('bertscore')\n",
    "    \n",
    "    sts_model = SentenceTransformer(sts_model)\n",
    "    def compute_metrics(eval_pred):\n",
    "        \"\"\"\n",
    "        Compute metrics function for Seq2SeqTrainer.\n",
    "        \n",
    "        Args:\n",
    "            eval_pred: EvalPrediction object containing predictions and label_ids\n",
    "            tokenizer: The tokenizer used for decoding\n",
    "            sts_model: Sentence transformer model for computing SBERT similarity\n",
    "            device: Device to run computations on\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing computed metrics\n",
    "        \"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        \n",
    "        # Replace -100 with pad_token_id for decoding\n",
    "        labels = torch.where(\n",
    "            torch.tensor(labels) == -100,\n",
    "            torch.tensor(tokenizer.pad_token_id),\n",
    "            torch.tensor(labels)\n",
    "        )\n",
    "        \n",
    "        predicted_explanations = tokenizer.batch_decode(\n",
    "            predictions, \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        true_explanations = tokenizer.batch_decode(\n",
    "            labels, \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # Compute ROUGE scores\n",
    "        rouge_results = rouge_metric.compute(\n",
    "            predictions=predicted_explanations,\n",
    "            references=true_explanations\n",
    "        )\n",
    "        \n",
    "        # Compute BLEU score\n",
    "        bleu_results = bleu_metric.compute(\n",
    "            predictions=predicted_explanations,\n",
    "            references=[[ref] for ref in true_explanations]\n",
    "        )\n",
    "        \n",
    "        # Compute SBERT similarity\n",
    "        reference_embeddings = sts_model.encode(true_explanations, convert_to_tensor=True)\n",
    "        generated_embeddings = sts_model.encode(predicted_explanations, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(generated_embeddings, reference_embeddings)\n",
    "        sbert_similarity = torch.diag(cosine_scores).mean().item()\n",
    "        \n",
    "        # Compute BERTScore\n",
    "        bertscore_results = bertscore_metric.compute(\n",
    "            predictions=predicted_explanations,\n",
    "            references=true_explanations,\n",
    "            lang=\"it\",\n",
    "        )\n",
    "        bertscore_f1 = sum(bertscore_results['f1']) / len(bertscore_results['f1'])\n",
    "        \n",
    "        return {\n",
    "            'rouge1': rouge_results['rouge1'],\n",
    "            'rouge2': rouge_results['rouge2'],\n",
    "            'rougeL': rouge_results['rougeL'],\n",
    "            'bleu': bleu_results['bleu'],\n",
    "            'sbert_similarity': sbert_similarity,\n",
    "            'bertscore_f1': bertscore_f1\n",
    "        }\n",
    "    \n",
    "    return compute_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1375cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    DEFAULT_CHECKPOINT\n",
    ").to(DEVICE)\n",
    "\n",
    "# checkpoint_path = os.path.abspath(os.path.join(\n",
    "#     \".\", \"out\", \"models\", \"toxicity\", \"BART\", \"2025-08-03_17-21-32\", \"checkpoint-420\"\n",
    "# ))\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "#     checkpoint_path\n",
    "# ).to(DEVICE)\n",
    "\n",
    "gen_config = GenerationConfig(\n",
    "    max_length=1024,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    top_k=25,\n",
    "    temperature=0.6,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    "    bos_token_id=model.config.bos_token_id,\n",
    ")\n",
    "\n",
    "# model.tie_weights()\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=BODY_LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_PERCENTAGE,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    # load_best_model_at_end=True,\n",
    "    save_total_limit=SAVE_TOTAL_LIMIT,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    predict_with_generate=True,\n",
    "    generation_config=gen_config,\n",
    "    dataloader_num_workers=NUM_WORKERS,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    # model=model\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_set,\n",
    "    eval_dataset=tokenized_eval_set,\n",
    "    # tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=create_compute_metrics_fn(\n",
    "    #     tokenizer=tokenizer,\n",
    "    #     sts_model='sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "    # )\n",
    ")\n",
    "\n",
    "# train_result = trainer.train()\n",
    "# log_history = trainer.state.log_history\n",
    "# plot_losses(log_history, RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b321b5f7",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e26b7",
   "metadata": {},
   "source": [
    "## Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ac2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inference(input_text):\n",
    "#     inputs = tokenizer(\n",
    "#         input_text,\n",
    "#         max_length=1024,\n",
    "#         truncation=True,\n",
    "#         return_tensors=\"pt\"\n",
    "#     ).to(DEVICE)\n",
    "#     # inputs = {k: v.to(DEVICE) for k, v in inputs.items()}  # Move tensors to device\n",
    "#     outputs = model.generate(**inputs, generation_config=gen_config)\n",
    "#     decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return decoded_output\n",
    "\n",
    "# chat = '''\n",
    "# Topolino:\n",
    "# Ei, come va?\n",
    "# Topolina:\n",
    "# Ciao Mauro! Tutto bene, grazie! E tu?\n",
    "# Topolino:\n",
    "# Tutto ok, grazie! Che fai di bello oggi?\n",
    "# Topolina:\n",
    "# Sinceramente non lo so, ho un po' di cose da fare ma non so da dove cominciare. Tu che fai?\n",
    "# '''\n",
    "\n",
    "# print(inference(chat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efeb97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example in tokenized_train_set.select(range(1, 2)):\n",
    "#     decoded_chat = tokenizer.decode(example['input_ids'], skip_special_tokens=True)\n",
    "#     decoded_true_explanation = tokenizer.decode(example['labels'], skip_special_tokens=True)\n",
    "\n",
    "#     output = inference(decoded_chat)\n",
    "\n",
    "#     print(f\"Message:\\n{decoded_chat}\")\n",
    "#     print(f\"True Explanation:\\n{decoded_true_explanation}\\n\")\n",
    "#     print(f\"Generated Explanation:\\n{output}\\n\")\n",
    "#     print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06923925",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa7d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_set,\n",
    "    eval_dataset=tokenized_eval_set,\n",
    "    # tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=create_compute_metrics_fn(\n",
    "        tokenizer=tokenizer,\n",
    "        sts_model='sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "    )\n",
    ")\n",
    "\n",
    "test_metrics = trainer.evaluate(\n",
    "    eval_dataset=tokenized_test_set,\n",
    "    metric_key_prefix=\"test\"\n",
    ")\n",
    "for key, value in test_metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "with open(os.path.join(RESULTS_PATH, \"test_metrics.txt\"), \"w\") as f:\n",
    "    for key, value in test_metrics.items():\n",
    "        f.write(f\"{key}: {value}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
